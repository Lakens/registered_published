@misc{akker_effectiveness_2023,
  title = {The Effectiveness of Preregistration in Psychology: {{Assessing}} Preregistration Strictness and Preregistration-Study Consistency},
  shorttitle = {The Effectiveness of Preregistration in Psychology},
  author = {van den Akker, Olmo and Bakker, Marjan and van Assen, Marcel A. L. M. and Pennington, Charlotte Rebecca and Verweij, Leone and Elsherif, Mahmoud and Claesen, Aline and Gaillard, Stefan Daniel Michel and Yeung, Siu Kit and Frankenberger, Jan-Luca and Krautter, Kai and Cockcroft, Jamie P. and Kreuer, Katharina Sybille and Evans, Thomas Rhys and Heppel, Fr{\'e}d{\'e}rique and Schoch, Sarah Fiona and Korbmacher, Max and Yamada, Yuki and {Albayrak-Aydemir}, Nihan and Alzahawi, Shilaan and Sarafoglou, Alexandra and Sitnikov, Maksim and Dechterenko, Filip and Wingen, Sophia and Grinschgl, Sandra and Hartmann, Helena and Stewart, Suzanne and Oliveira, Catia Margarida and {Ashcroft-Jones}, Sarah and Baker, Bradley James and Wicherts, Jelte},
  year = {2023},
  month = may,
  publisher = {{MetaArXiv}},
  doi = {10.31222/osf.io/h8xjw},
  urldate = {2023-05-13},
  abstract = {Study preregistration has become increasingly popular in psychology, but its effectiveness in restricting potentially biasing researcher degrees of freedom remains unclear. We used an extensive protocol to assess the strictness of preregistrations and the consistency between preregistration and publications of 300 preregistered psychology studies. We found that preregistrations often lack methodological details and that undisclosed deviations from preregistered plans are frequent. Combining the strictness and consistency results highlights that biases due to researcher degrees of freedom are prevalent and likely in many preregistered studies. More comprehensive registration templates typically yielded stricter and hence better preregistrations. We did not find that effectiveness of preregistrations differed over time or between original and replication studies. Furthermore, we found that operationalizations of variables were generally more effectively preregistered than other study parts. Inconsistencies between preregistrations and published studies were mainly encountered for data collection procedures, statistical models, and exclusion criteria. Our results indicate that, to unlock the full potential of preregistration, researchers in psychology should aim to write stricter preregistrations, adhere to these preregistrations more faithfully, and more transparently report any deviations from the preregistrations. This could be facilitated by training and education to improve preregistration skills, as well as the development of more comprehensive templates.},
  langid = {american},
  keywords = {preregistration,Psychology,Social and Behavioral Sciences}
}

@article{allen_open_2019,
  title = {Open Science Challenges, Benefits and Tips in Early Career and Beyond},
  author = {Allen, Christopher and Mehler, David M. A.},
  year = {2019},
  month = may,
  journal = {PLOS Biology},
  volume = {17},
  number = {5},
  pages = {e3000246},
  publisher = {{Public Library of Science}},
  issn = {1545-7885},
  doi = {10.1371/journal.pbio.3000246},
  urldate = {2023-05-17},
  abstract = {The movement towards open science is a consequence of seemingly pervasive failures to replicate previous research. This transition comes with great benefits but also significant challenges that are likely to affect those who carry out the research, usually early career researchers (ECRs). Here, we describe key benefits, including reputational gains, increased chances of publication, and a broader increase in the reliability of research. The increased chances of publication are supported by exploratory analyses indicating null findings are substantially more likely to be published via open registered reports in comparison to more conventional methods. These benefits are balanced by challenges that we have encountered and that involve increased costs in terms of flexibility, time, and issues with the current incentive structure, all of which seem to affect ECRs acutely. Although there are major obstacles to the early adoption of open science, overall open science practices should benefit both the ECR and improve the quality of research. We review 3 benefits and 3 challenges and provide suggestions from the perspective of ECRs for moving towards open science practices, which we believe scientists and institutions at all levels would do well to consider.},
  langid = {english},
  keywords = {Careers,Experimental design,Neuroimaging,Open data,Open science,Peer review,Reproducibility,Statistical data}
}

@book{bernal_social_1939,
  title = {The {{Social Function Of Science}}},
  author = {Bernal, J. D.},
  year = {1939},
  publisher = {{Routledge}},
  address = {{London}},
  urldate = {2023-04-23},
  langid = {english},
  keywords = {IIIT}
}

@article{bottesini_what_2022,
  title = {What Do Participants Think of Our Research Practices? {{An}} Examination of Behavioural Psychology Participants' Preferences},
  shorttitle = {What Do Participants Think of Our Research Practices?},
  author = {Bottesini, Julia G. and Rhemtulla, Mijke and Vazire, Simine},
  year = {2022},
  journal = {Royal Society Open Science},
  volume = {9},
  number = {4},
  pages = {200048},
  publisher = {{Royal Society}},
  doi = {10.1098/rsos.200048},
  urldate = {2022-06-21},
  abstract = {What research practices should be considered acceptable? Historically, scientists have set the standards for what constitutes acceptable research practices. However, there is value in considering non-scientists' perspectives, including research participants'. 1873 participants from MTurk and university subject pools were surveyed after their participation in one of eight minimal-risk studies. We asked participants how they would feel if (mostly) common research practices were applied to their data: p-hacking/cherry-picking results, selective reporting of studies, Hypothesizing After Results are Known (HARKing), committing fraud, conducting direct replications, sharing data, sharing methods, and open access publishing. An overwhelming majority of psychology research participants think questionable research practices (e.g. p-hacking, HARKing) are unacceptable (68.3\textendash 81.3\%), and were supportive of practices to increase transparency and replicability (71.4\textendash 80.1\%). A surprising number of participants expressed positive or neutral views toward scientific fraud (18.7\%), raising concerns about data quality. We grapple with this concern and interpret our results in light of the limitations of our study. Despite the ambiguity in our results, we argue that there is evidence (from our study and others') that researchers may be violating participants' expectations and should be transparent with participants about how their data will be used.},
  keywords = {informed consent,open science,research practices,scientific integrity}
}

@article{buxton_avoiding_2021,
  title = {Avoiding Wasted Research Resources in Conservation Science},
  author = {Buxton, Rachel T. and Nyboer, Elizabeth A. and Pigeon, Karine E. and Raby, Graham D. and Rytwinski, Trina and Gallagher, Austin J. and Schuster, Richard and Lin, Hsien-Yung and Fahrig, Lenore and Bennett, Joseph R. and Cooke, Steven J. and Roche, Dominique G.},
  year = {2021},
  journal = {Conservation Science and Practice},
  volume = {3},
  number = {2},
  pages = {e329},
  issn = {2578-4854},
  doi = {10.1111/csp2.329},
  urldate = {2021-04-03},
  abstract = {Scientific evidence is fundamental for guiding effective conservation action to curb biodiversity loss. Yet, research resources in conservation are often wasted due to biased allocation of research effort, irrelevant or low-priority questions, flawed studies, inaccessible research outputs, and biased or poor-quality reporting. We outline a striking example of wasted research resources, highlight a powerful case of data rescue/reuse, and discuss an exemplary model of evidence-informed conservation. We suggest that funding agencies, research institutions, NGOs, publishers, and researchers are part of the problem and solutions, and outline recommendations to curb the waste of research resources, including knowledge co-creation and open science practices.},
  copyright = {\textcopyright{} 2021 The Authors. Conservation Science and Practice published by Wiley Periodicals LLC. on behalf of Society for Conservation Biology},
  langid = {english},
  keywords = {co-production,data rescue and reuse,evidence-informed decision making,FAIR data,open science,research data management}
}

@article{chalmers_avoidable_2009,
  title = {Avoidable Waste in the Production and Reporting of Research Evidence},
  author = {Chalmers, Iain and Glasziou, Paul},
  year = {2009},
  journal = {The Lancet},
  volume = {374},
  number = {9683},
  pages = {86--89}
}

@article{chambers_past_2022,
  title = {The Past, Present and Future of {{Registered Reports}}},
  author = {Chambers, Christopher D. and Tzavella, Loukia},
  year = {2022},
  month = jan,
  journal = {Nature Human Behaviour},
  volume = {6},
  number = {1},
  pages = {29--42},
  publisher = {{Nature Publishing Group}},
  issn = {2397-3374},
  doi = {10.1038/s41562-021-01193-7},
  urldate = {2022-04-06},
  abstract = {Registered Reports are a form of empirical publication in which study proposals are peer reviewed and pre-accepted before research is undertaken. By deciding which articles are published based on the question, theory and methods, Registered Reports offer a remedy for a range of reporting and publication biases. Here, we reflect on the history, progress and future prospects of the Registered Reports initiative and offer practical guidance for authors, reviewers and editors. We review early evidence that Registered Reports are working as intended, while at the same time acknowledging that they are not a universal solution for irreproducibility. We also consider how the policies and practices surrounding Registered Reports are changing, or must change in the future, to address limitations and adapt to new challenges. We conclude that Registered Reports are promoting reproducibility, transparency and self-correction across disciplines and may help reshape how society evaluates research and researchers.},
  copyright = {2021 Springer Nature Limited},
  langid = {english},
  keywords = {Culture,Publishing}
}

@article{dickersin_evolution_2012,
  title = {The {{Evolution}} of {{Trial Registries}} and {{Their Use}} to {{Assess}} the {{Clinical Trial Enterprise}}},
  author = {Dickersin, K. and Rennie, D.},
  year = {2012},
  month = may,
  journal = {JAMA},
  volume = {307},
  number = {17},
  pages = {1861--1864},
  issn = {0098-7484},
  doi = {10.1001/jama.2012.4230},
  urldate = {2023-02-04},
  abstract = {The original purpose of registries of clinical trials was to reveal the existence of all trials, published or not, to investigators and systematic reviewers. Trials left unpublished because results were unfavorable to their sponsors, or simply because investigators never submitted them to journals for publication, could then be discovered, the trial investigators contacted, and the available trial evidence involving medical interventions could then be assessed. This would help eliminate publication bias, shown originally in the 1980s, demonstrated by Simes to affect the treatment of patients, and later revealed to be widespread by the expanding efforts of the Cochrane Collaboration. A seemingly arcane statistical point became a pressing clinical problem.}
}

@article{dickersin_factors_1992,
  title = {Factors Influencing Publication of Research Results. {{Follow-up}} of Applications Submitted to Two Institutional Review Boards},
  author = {Dickersin, K. and Min, Y. I. and Meinert, C. L.},
  year = {1992},
  month = jan,
  journal = {JAMA},
  volume = {267},
  number = {3},
  pages = {374--378},
  issn = {0098-7484},
  abstract = {OBJECTIVE: --To investigate factors associated with the publication of research findings, in particular, the association between "significant" results and publication. DESIGN: --Follow-up study. SETTING: --Studies approved in 1980 or prior to 1980 by the two institutional review boards that serve The Johns Hopkins Health Institutions--one that serves the School of Medicine and Hospital and the other that serves the School of Hygiene and Public Health. POPULATION: --A total of 737 studies were followed up. RESULTS: --Of the studies for which analyses had been reported as having been performed at the time of interview, 81\% from the School of Medicine and Hospital and 66\% from the School of Hygiene and Public Health had been published. Publication was not associated with sample size, presence of a comparison group, or type of study (eg, observational study vs clinical trial). External funding and multiple data collection sites were positively associated with publication. There was evidence of publication bias in that for both institutional review boards there was an association between results reported to be significant and publication (adjusted odds ratio, 2.54; 95\% confidence interval, 1.63 to 3.94). Contrary to popular opinion, publication bias originates primarily with investigators, not journal editors: only six of the 124 studies not published were reported to have been rejected for publication. CONCLUSION: --There is a statistically significant association between significant results and publication.},
  langid = {english},
  pmid = {1727960},
  keywords = {Academic Medical Centers,Baltimore,Bias,Biomedical and Behavioral Research,Biomedical Research,Editorial Policies,{Ethics Committees, Research},Female,Follow-Up Studies,Humans,Johns Hopkins University,Male,Odds Ratio,Peer Review,Publishing,Research Design,Research Personnel,Research Support as Topic,Sampling Studies}
}

@article{dickersin_nih_1993,
  title = {{{NIH}} Clinical Trials and Publication Bias},
  author = {Dickersin, K. and Min, Y. I.},
  year = {1993},
  month = apr,
  journal = {The Online Journal of Current Clinical Trials},
  volume = {Doc No 50},
  pages = {[4967 words; 53 paragraphs]},
  issn = {1059-2725},
  abstract = {OBJECTIVE: To investigate the association between trial characteristics, findings, and publication. The major factor hypothesized to be associated with publication was "significant" results, which included both statistically significant results and results assessed by the investigators to be qualitatively significant, when statistical testing was not done. Other factors hypothesized to have a possible association with publication were funding institute, funding mechanism (grant versus contract versus intramural), multicenter status, use of comparison groups, large sample size, type of control (parallel versus nonparallel), use of randomization and masking, type of analysis (by treatment received versus by treatment assigned), and investigator sex and rank. DESIGN: Follow-up, by 1988 interview with the principal investigator or surrogate, of all clinical trials funded by the National Institutes of Health (NIH) in 1979, to learn of trial results and publication status. POPULATION: Two hundred ninety-three NIH trials, funded in 1979. MAIN OUTCOME MEASURE: Publication of clinical trial results. RESULTS: Of the 198 clinical trials completed by 1988, 93\% had been published. Trials with "significant" results were more likely to be published than those showing "nonsignificant" results (adjusted odds ratio [OR] = 12.30; 95\% confidence interval [CI], 2.54 to 60.00). No other factor was positively associated with publication. Most unpublished trials remained so because investigators thought the results were "not interesting" or they "did not have enough time" (42.8\%). Metaanalysis using data from this and 3 similar studies provided a combined unadjusted OR of 2.88 (95\% CI, 2.13 to 3.89) for the association between significant results and publication. CONCLUSIONS: Even when the overall publication rate is high, such as for trials funded by the NIH, publication bias remains a significant problem. Given the importance of trials and their utility in evaluating medical treatments, especially within the context of metaanalysis, it is clear that we need more reliable systems for maintaining information about initiated studies. Trial registers represent such a system but must receive increased financial support to succeed.},
  langid = {english},
  pmid = {8306005},
  keywords = {Clinical Trials as Topic,Female,Humans,Logistic Models,Male,National Institutes of Health (U.S.),Odds Ratio,Publication Bias,Publishing,Randomized Controlled Trials as Topic,Research Design,United States}
}

@article{easterbrook_publication_1991,
  title = {Publication Bias in Clinical Research},
  author = {Easterbrook, P. J. and Berlin, J. A. and Gopalan, R. and Matthews, D. R.},
  year = {1991},
  month = apr,
  journal = {Lancet (London, England)},
  volume = {337},
  number = {8746},
  pages = {867--872},
  issn = {0140-6736},
  doi = {10.1016/0140-6736(91)90201-y},
  abstract = {In a retrospective survey, 487 research projects approved by the Central Oxford Research Ethics Committee between 1984 and 1987, were studied for evidence of publication bias. As of May, 1990, 285 of the studies had been analysed by the investigators, and 52\% of these had been published. Studies with statistically significant results were more likely to be published than those finding no difference between the study groups (adjusted odds ratio [OR] 2.32; 95\% confidence interval [Cl] 1.25-4.28). Studies with significant results were also more likely to lead to a greater number of publications and presentations and to be published in journals with a high citation impact factor. An increased likelihood of publication was also associated with a high rating by the investigator of the importance of the study results, and with increasing sample size. The tendency towards publication bias was greater with observational and laboratory-based experimental studies (OR = 3.79; 95\% Cl = 1.47-9.76) than with randomised clinical trials (OR = 0.84; 95\% Cl = 0.34-2.09). We have confirmed the presence of publication bias in a cohort of clinical research studies. These findings suggest that conclusions based only on a review of published data should be interpreted cautiously, especially for observational studies. Improved strategies are needed to identify the results of unpublished as well as published studies.},
  langid = {english},
  pmid = {1672966},
  keywords = {Bias,Biomedical and Behavioral Research,Biomedical Research,Central Oxford Research Ethics Committee,Clinical Trials as Topic,Cohort Studies,Confidence Intervals,Data Collection,Editorial Policies,Empirical Approach,Ethical Review,{Ethics Committees, Research},Information Dissemination,Odds Ratio,Periodicals as Topic,Publishing,Randomized Controlled Trials as Topic,Research,Research Design,Retrospective Studies}
}

@article{egger_bias_1997,
  title = {Bias in Meta-Analysis Detected by a Simple, Graphical Test},
  author = {Egger, M. and Davey Smith, G. and Schneider, M. and Minder, C.},
  year = {1997},
  month = sep,
  journal = {BMJ (Clinical research ed.)},
  volume = {315},
  number = {7109},
  pages = {629--634},
  issn = {0959-8138},
  doi = {10.1136/bmj.315.7109.629},
  abstract = {OBJECTIVE: Funnel plots (plots of effect estimates against sample size) may be useful to detect bias in meta-analyses that were later contradicted by large trials. We examined whether a simple test of asymmetry of funnel plots predicts discordance of results when meta-analyses are compared to large trials, and we assessed the prevalence of bias in published meta-analyses. DESIGN: Medline search to identify pairs consisting of a meta-analysis and a single large trial (concordance of results was assumed if effects were in the same direction and the meta-analytic estimate was within 30\% of the trial); analysis of funnel plots from 37 meta-analyses identified from a hand search of four leading general medicine journals 1993-6 and 38 meta-analyses from the second 1996 issue of the Cochrane Database of Systematic Reviews. MAIN OUTCOME MEASURE: Degree of funnel plot asymmetry as measured by the intercept from regression of standard normal deviates against precision. RESULTS: In the eight pairs of meta-analysis and large trial that were identified (five from cardiovascular medicine, one from diabetic medicine, one from geriatric medicine, one from perinatal medicine) there were four concordant and four discordant pairs. In all cases discordance was due to meta-analyses showing larger effects. Funnel plot asymmetry was present in three out of four discordant pairs but in none of concordant pairs. In 14 (38\%) journal meta-analyses and 5 (13\%) Cochrane reviews, funnel plot asymmetry indicated that there was bias. CONCLUSIONS: A simple analysis of funnel plots provides a useful test for the likely presence of bias in meta-analyses, but as the capacity to detect bias will be limited when meta-analyses are based on a limited number of small trials the results from such analyses should be treated with considerable caution.},
  langid = {english},
  pmcid = {PMC2127453},
  pmid = {9310563},
  keywords = {Bias,Meta-Analysis as Topic,Randomized Controlled Trials as Topic,Regression Analysis,Statistics as Topic,Treatment Outcome}
}

@article{fanelli_positive_2010,
  title = {``{{Positive}}'' {{Results Increase Down}} the {{Hierarchy}} of the {{Sciences}}},
  author = {Fanelli, Daniele},
  year = {2010},
  month = apr,
  journal = {PLoS ONE},
  volume = {5},
  number = {4},
  issn = {1932-6203},
  doi = {10.1371/journal.pone.0010068},
  urldate = {2015-12-11},
  abstract = {The hypothesis of a Hierarchy of the Sciences with physical sciences at the top, social sciences at the bottom, and biological sciences in-between is nearly 200 years old. This order is intuitive and reflected in many features of academic life, but whether it reflects the ``hardness'' of scientific research\textemdash i.e., the extent to which research questions and results are determined by data and theories as opposed to non-cognitive factors\textemdash is controversial. This study analysed 2434 papers published in all disciplines and that declared to have tested a hypothesis. It was determined how many papers reported a ``positive'' (full or partial) or ``negative'' support for the tested hypothesis. If the hierarchy hypothesis is correct, then researchers in ``softer'' sciences should have fewer constraints to their conscious and unconscious biases, and therefore report more positive outcomes. Results confirmed the predictions at all levels considered: discipline, domain and methodology broadly defined. Controlling for observed differences between pure and applied disciplines, and between papers testing one or several hypotheses, the odds of reporting a positive result were around 5 times higher among papers in the disciplines of Psychology and Psychiatry and Economics and Business compared to Space Science, 2.3 times higher in the domain of social sciences compared to the physical sciences, and 3.4 times higher in studies applying behavioural and social methodologies on people compared to physical and chemical studies on non-biological material. In all comparisons, biological studies had intermediate values. These results suggest that the nature of hypotheses tested and the logical and methodological rigour employed to test them vary systematically across disciplines and fields, depending on the complexity of the subject matter and possibly other factors (e.g., a field's level of historical and/or intellectual development). On the other hand, these results support the scientific status of the social sciences against claims that they are completely subjective, by showing that, when they adopt a scientific approach to discovery, they differ from the natural sciences only by a matter of degree.},
  pmcid = {PMC2850928},
  pmid = {20383332}
}

@article{franco_publication_2014,
  title = {Publication Bias in the Social Sciences: {{Unlocking}} the File Drawer},
  shorttitle = {Publication Bias in the Social Sciences},
  author = {Franco, Annie and Malhotra, Neil and Simonovits, Gabor},
  year = {2014},
  journal = {Science},
  volume = {345},
  number = {6203},
  pages = {1502--1505},
  doi = {10.1126/SCIENCE.1255484},
  annotation = {00377}
}

@article{gerber_publication_2010,
  title = {Publication {{Bias}} in {{Two Political Behavior Literatures}}},
  author = {Gerber, Alan S. and Malhotra, Neil and Dowling, Conor M. and Doherty, David},
  year = {2010},
  month = jul,
  journal = {American Politics Research},
  volume = {38},
  number = {4},
  pages = {591--613},
  issn = {1532-673X, 1552-3373},
  doi = {10.1177/1532673X09350979},
  urldate = {2021-07-09},
  abstract = {Publication bias occurs when the probability that a paper enters the scholarly literature is a function of the magnitude or significance levels of the coefficient estimates.We investigate publication bias in two large literatures in political behavior: economic voting and the effects of negative advertising. We find that the pattern of published estimates is consistent with the presence of publication bias and that bias is more prevalent in the most influential and highly cited outlets.We consider the possible causes and find some evidence that papers systematically employ one-sided hypothesis tests in response to failure to meet the more demanding critical values associated with twotailed tests, a practice that leads to misleading reports of the probability of Type I errors.},
  langid = {english}
}

@article{greenwald_consequences_1975,
  title = {Consequences of Prejudice against the Null Hypothesis.},
  author = {Greenwald, Anthony G.},
  year = {1975},
  journal = {Psychological Bulletin},
  volume = {82},
  number = {1},
  pages = {1--20},
  doi = {10.1037/h0076157},
  urldate = {2015-12-23}
}

@article{hanel_beyond_nodate,
  title = {Beyond {{Reporting Statistical Significance}}: {{Identifying Informative Effect Sizes}} to {{Improve Scientific Communication}}},
  shorttitle = {Beyond {{Reporting Statistical Significance}}},
  author = {Hanel, Paul H. P. and Mehler, David Marc Anton},
  doi = {10/gfwp3x},
  urldate = {2019-03-10},
  abstract = {Transparent communication of research is key to foster understanding within and beyond the scientific community.  Increased focus on reporting effect sizes in addition of p-value based significance statements may improve scientific communication with the general public.  Across two studies (N = 446), we compared informativeness ratings for five effect sizes, Bayes Factor and commonly used significance statements.  Results showed that Cohen's U3 was rated as most informative.  For example, 77\% of participants found it more informative than Cohen's d.  We therefore suggest that Cohen's U3 is used when scientific findings are communicated.},
  annotation = {00000}
}

@article{lakens_improving_2021,
  title = {Improving {{Transparency}}, {{Falsifiability}}, and {{Rigor}} by {{Making Hypothesis Tests Machine-Readable}}},
  author = {Lakens, Dani{\"e}l and DeBruine, Lisa M.},
  year = {2021},
  month = apr,
  journal = {Advances in Methods and Practices in Psychological Science},
  volume = {4},
  number = {2},
  pages = {2515245920970949},
  publisher = {{SAGE Publications Inc}},
  issn = {2515-2459},
  doi = {10.1177/2515245920970949},
  urldate = {2021-07-13},
  abstract = {Making scientific information machine-readable greatly facilitates its reuse. Many scientific articles have the goal to test a hypothesis, so making the tests of statistical predictions easier to find and access could be very beneficial. We propose an approach that can be used to make hypothesis tests machine-readable. We believe there are two benefits to specifying a hypothesis test in such a way that a computer can evaluate whether the statistical prediction is corroborated or not. First, hypothesis tests become more transparent, falsifiable, and rigorous. Second, scientists benefit if information related to hypothesis tests in scientific articles is easily findable and reusable, for example, to perform meta-analyses, conduct peer review, and examine metascientific research questions. We examine what a machine-readable hypothesis test should look like and demonstrate the feasibility of machine-readable hypothesis tests in a real-life example using the fully operational prototype R package scienceverse.},
  langid = {english},
  keywords = {hypothesis testing,machine readability,metadata,scholarly communication}
}

@article{lishner_sorting_2021,
  title = {Sorting the {{File Drawer}}: {{A Typology}} for {{Describing Unpublished Studies}}},
  shorttitle = {Sorting the {{File Drawer}}},
  author = {Lishner, David A.},
  year = {2021},
  month = mar,
  journal = {Perspectives on Psychological Science},
  pages = {1745691620979831},
  publisher = {{SAGE Publications Inc}},
  issn = {1745-6916},
  doi = {10.1177/1745691620979831},
  urldate = {2021-04-13},
  abstract = {A typology of unpublished studies is presented to describe various types of unpublished studies and the reasons for their nonpublication. Reasons for nonpublication are classified by whether they stem from an awareness of the study results (result-dependent reasons) or not (result-independent reasons) and whether the reasons affect the publication decisions of individual researchers or reviewers/editors. I argue that result-independent reasons for nonpublication are less likely to introduce motivated reasoning into the publication decision process than are result-dependent reasons. I also argue that some reasons for nonpublication would produce beneficial as opposed to problematic publication bias. The typology of unpublished studies provides a descriptive scheme that can facilitate understanding of the population of study results across the field of psychology, within subdisciplines of psychology, or within specific psychology research domains. The typology also offers insight into different publication biases and research-dissemination practices and can guide individual researchers in organizing their own file drawers of unpublished studies.},
  langid = {english},
  keywords = {file drawer,meta-analysis,methodology,publication bias,unpublished studies}
}

@article{nosek_registered_2014,
  title = {Registered Reports: {{A}} Method to Increase the Credibility of Published Results},
  shorttitle = {Registered Reports},
  author = {Nosek, Brian A. and Lakens, Dani{\"e}l},
  year = {2014},
  journal = {Social Psychology},
  volume = {45},
  number = {3},
  pages = {137--141},
  issn = {2151-2590(Electronic);1864-9335(Print)},
  doi = {10.1027/1864-9335/a000192},
  abstract = {The published journal article is the primary means of communicating scientific ideas, methods, and empirical data. Not all ideas and data get published. In the present scientific culture, novel and positive results are considered more publishable than replications and negative results. This creates incentives to avoid or ignore replications and negative results, even at the expense of accuracy (Giner-Sorolla, 2012; Nosek, Spies, \& Motyl, 2012). As a consequence, replications (Makel, Plucker, \& Hegarty, 2012) and negative results (Fanelli, 2010; Sterling, 1959) are rare in the published literature. This insight is not new, but the culture is resistant to change. This article introduces the first known journal issue in any discipline consisting exclusively of preregistered replication studies. It demonstrates that replications have substantial value, and that incentives can be changed.},
  copyright = {(c) 2014 APA, all rights reserved},
  keywords = {*Credibility,*Experimental Replication,*Scientific Communication,Sciences}
}

@article{pickett_questionable_2017,
  title = {Questionable, {{Objectionable}} or {{Criminal}}? {{Public Opinion}} on {{Data Fraud}} and {{Selective Reporting}} in {{Science}}},
  shorttitle = {Questionable, {{Objectionable}} or {{Criminal}}?},
  author = {Pickett, Justin T. and Roche, Sean Patrick},
  year = {2017},
  month = mar,
  journal = {Science and Engineering Ethics},
  pages = {1--21},
  issn = {1353-3452, 1471-5546},
  doi = {10.1007/s11948-017-9886-2},
  urldate = {2018-01-11},
  abstract = {Data fraud and selective reporting both present serious threats to the credibility of science. However, there remains considerable disagreement among scientists about how best to sanction data fraud, and about the ethicality of selective reporting. The public is arguably the largest stakeholder in the reproducibility of science; research is primarily paid for with public funds, and flawed science threatens the public's welfare. Members of the public are able to make meaningful judgments about the morality of different behaviors using moral intuitions. Legal scholars emphasize that to maintain legitimacy, social control policies must be developed with some consideration given to the public's moral intuitions. Although there is a large literature on popular attitudes toward science, there is no existing evidence about public opinion on data fraud or selective reporting. We conducted two studies\textemdash a survey experiment with a nationwide convenience sample (N = 821), and a follow-up survey with a representative sample of US adults (N = 964)\textemdash to explore community members' judgments about the morality of data fraud and selective reporting in science. The findings show that community members make a moral distinction between data fraud and selective reporting, but overwhelmingly judge both behaviors to be immoral and deserving of punishment. Community members believe that scientists who commit data fraud or selective reporting should be fired and banned from receiving funding. For data fraud, most Americans support criminal penalties. Results from an ordered logistic regression analysis reveal few demographic and no significant partisan differences in punitiveness toward data fraud.},
  langid = {english}
}

@article{rosenthal_file_1979,
  title = {The File Drawer Problem and Tolerance for Null Results.},
  author = {Rosenthal, Robert},
  year = {1979},
  journal = {Psychological bulletin},
  volume = {86},
  number = {3},
  pages = {638},
  urldate = {2016-09-07},
  annotation = {05335}
}

@article{scheel_excess_2021,
  title = {An {{Excess}} of {{Positive Results}}: {{Comparing}} the {{Standard Psychology Literature With Registered Reports}}},
  shorttitle = {An {{Excess}} of {{Positive Results}}},
  author = {Scheel, Anne M. and Schijen, Mitchell R. M. J. and Lakens, Dani{\"e}l},
  year = {2021},
  month = apr,
  journal = {Advances in Methods and Practices in Psychological Science},
  volume = {4},
  number = {2},
  pages = {25152459211007467},
  publisher = {{SAGE Publications Inc}},
  issn = {2515-2459},
  doi = {10.1177/25152459211007467},
  urldate = {2023-04-17},
  abstract = {Selectively publishing results that support the tested hypotheses (?positive? results) distorts the available evidence for scientific claims. For the past decade, psychological scientists have been increasingly concerned about the degree of such distortion in their literature. A new publication format has been developed to prevent selective reporting: In Registered Reports (RRs), peer review and the decision to publish take place before results are known. We compared the results in published RRs (N = 71 as of November 2018) with a random sample of hypothesis-testing studies from the standard literature (N = 152) in psychology. Analyzing the first hypothesis of each article, we found 96\% positive results in standard reports but only 44\% positive results in RRs. We discuss possible explanations for this large difference and suggest that a plausible factor is the reduction of publication bias and/or Type I error inflation in the RR literature.},
  langid = {english}
}

@article{schmidt_shall_2009,
  title = {Shall We Really Do It Again? {{The}} Powerful Concept of Replication Is Neglected in the Social Sciences.},
  shorttitle = {Shall We Really Do It Again?},
  author = {Schmidt, Stefan},
  year = {2009},
  journal = {Review of General Psychology},
  volume = {13},
  number = {2},
  pages = {90--100},
  issn = {1939-1552, 1089-2680},
  doi = {10.1037/a0015108},
  urldate = {2015-11-30},
  langid = {english},
  annotation = {00000}
}

@article{song_dissemination_2010,
  title = {Dissemination and Publication of Research Findings: An Updated Review of Related Biases},
  shorttitle = {Dissemination and Publication of Research Findings},
  author = {Song, F. and Parekh, S. and Hooper, L. and Loke, Y. K. and Ryder, J. and Sutton, A. J. and Hing, C. and Kwok, C. S. and Pang, C. and Harvey, I.},
  year = {2010},
  month = feb,
  journal = {Health Technology Assessment (Winchester, England)},
  volume = {14},
  number = {8},
  pages = {iii, ix-xi, 1--193},
  issn = {2046-4924},
  doi = {10.3310/hta14080},
  abstract = {OBJECTIVES: To identify and appraise empirical studies on publication and related biases published since 1998; to assess methods to deal with publication and related biases; and to examine, in a random sample of published systematic reviews, measures taken to prevent, reduce and detect dissemination bias. DATA SOURCES: The main literature search, in August 2008, covered the Cochrane Methodology Register Database, MEDLINE, EMBASE, AMED and CINAHL. In May 2009, PubMed, PsycINFO and OpenSIGLE were also searched. Reference lists of retrieved studies were also examined. REVIEW METHODS: In Part I, studies were classified as evidence or method studies and data were extracted according to types of dissemination bias or methods for dealing with it. Evidence from empirical studies was summarised narratively. In Part II, 300 systematic reviews were randomly selected from MEDLINE and the methods used to deal with publication and related biases were assessed. RESULTS: Studies with significant or positive results were more likely to be published than those with non-significant or negative results, thereby confirming findings from a previous HTA report. There was convincing evidence that outcome reporting bias exists and has an impact on the pooled summary in systematic reviews. Studies with significant results tended to be published earlier than studies with non-significant results, and empirical evidence suggests that published studies tended to report a greater treatment effect than those from the grey literature. Exclusion of non-English-language studies appeared to result in a high risk of bias in some areas of research such as complementary and alternative medicine. In a few cases, publication and related biases had a potentially detrimental impact on patients or resource use. Publication bias can be prevented before a literature review (e.g. by prospective registration of trials), or detected during a literature review (e.g. by locating unpublished studies, funnel plot and related tests, sensitivity analysis modelling), or its impact can be minimised after a literature review (e.g. by confirmatory large-scale trials, updating the systematic review). The interpretation of funnel plot and related statistical tests, often used to assess publication bias, was often too simplistic and likely misleading. More sophisticated modelling methods have not been widely used. Compared with systematic reviews published in 1996, recent reviews of health-care interventions were more likely to locate and include non-English-language studies and grey literature or unpublished studies, and to test for publication bias. CONCLUSIONS: Dissemination of research findings is likely to be a biased process, although the actual impact of such bias depends on specific circumstances. The prospective registration of clinical trials and the endorsement of reporting guidelines may reduce research dissemination bias in clinical research. In systematic reviews, measures can be taken to minimise the impact of dissemination bias by systematically searching for and including relevant studies that are difficult to access. Statistical methods can be useful for sensitivity analyses. Further research is needed to develop methods for qualitatively assessing the risk of publication bias in systematic reviews, and to evaluate the effect of prospective registration of studies, open access policy and improved publication guidelines.},
  langid = {english},
  pmid = {20181324},
  keywords = {Bias,Biomedical Research,Evidence-Based Medicine,Humans,Information Dissemination,Publication Bias,Review Literature as Topic}
}

@book{spies_open_2013,
  title = {The Open Science Framework: {{Improving}} Science by Making It Open and Accessible},
  shorttitle = {The Open Science Framework},
  author = {Spies, Jeffrey Robert},
  year = {2013},
  publisher = {{University of Virginia}}
}

@article{sterling_publication_1959,
  title = {Publication {{Decisions}} and {{Their Possible Effects}} on {{Inferences Drawn}} from {{Tests}} of {{Significance--Or Vice Versa}}},
  author = {Sterling, Theodore D.},
  year = {1959},
  journal = {Journal of the American Statistical Association},
  volume = {54},
  number = {285},
  pages = {30--34},
  issn = {0162-1459},
  doi = {10.2307/2282137},
  urldate = {2019-12-12},
  abstract = {There is some evidence that in fields where statistical tests of significance are commonly used, research which yields nonsignificant results is not published. Such research being unknown to other investigators may be repeated independently until eventually by chance a significant result occurs-an "error of the first kind"-and is published. Significant results published in these fields are seldom verified by independent replication. The possibility thus arises that the literature of such a field consists in substantial part of false conclusions resulting from errors of the first kind in statistical tests of significance.}
}

@article{van_elk_whats_2021,
  title = {What's Hidden in My Filedrawer and What's in Yours? {{Disclosing}} Non-Published Findings in the Cognitive Science of Religion},
  shorttitle = {What's Hidden in My Filedrawer and What's in Yours?},
  author = {{van Elk}, Michiel},
  year = {2021},
  month = jan,
  journal = {Religion, Brain \& Behavior},
  volume = {11},
  number = {1},
  pages = {5--16},
  publisher = {{Routledge}},
  issn = {2153-599X},
  doi = {10.1080/2153599X.2020.1729233},
  urldate = {2023-05-20},
  abstract = {Despite recent developments to improve the transparency of scientific research, the field is in need of a new and effective way to communicate non-significant or unpublished findings to a broader audience. In this short report, I present an overview of different unpublished studies that we conducted in my lab over the past years. Across the different studies we observed consistent effects of our experimental manipulations or variables of interest on self-report measures, but less so on behavioral and neurocognitive measures. For instance, religious people said they were more prosocial but did not donate more money (Study 1 and 2); participants experienced awe but this did not affect their body and self perception (Study 6 and 7); participants had mystical-like experiences but this did not affect the perception of their peripersonal space (Study 8 and 9); and self-reported magical thinking was unrelated to superstitious behavior (Study 11). In other studies, the hypothesized effects did not bear out as expected or were even in an unexpected direction. Participants perceived more agency in threatening pictures and scenarios, but this was not related to their supernatural beliefs (Study 3\textendash 5) and a death priming manipulation reduced rather than increased participants' religiosity (Study 10). Thus, opening the filedrawer through the publication of short reports will hopefully further increase transparency and will help other researchers to learn from our own trials and errors.},
  keywords = {experimental study of religion,Filedrawer,open science,replication}
}
