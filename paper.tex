% Options for packages loaded elsewhere
\PassOptionsToPackage{unicode}{hyperref}
\PassOptionsToPackage{hyphens}{url}
%
\documentclass[
  ,jou, a4paper,floatsintext]{apa6}
\usepackage{amsmath,amssymb}
\usepackage{lmodern}
\usepackage{iftex}
\ifPDFTeX
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provide euro and other symbols
\else % if luatex or xetex
  \usepackage{unicode-math}
  \defaultfontfeatures{Scale=MatchLowercase}
  \defaultfontfeatures[\rmfamily]{Ligatures=TeX,Scale=1}
\fi
% Use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{% use microtype if available
  \usepackage[]{microtype}
  \UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\makeatletter
\@ifundefined{KOMAClassName}{% if non-KOMA class
  \IfFileExists{parskip.sty}{%
    \usepackage{parskip}
  }{% else
    \setlength{\parindent}{0pt}
    \setlength{\parskip}{6pt plus 2pt minus 1pt}}
}{% if KOMA class
  \KOMAoptions{parskip=half}}
\makeatother
\usepackage{xcolor}
\usepackage{graphicx}
\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}
\makeatother
% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}
% Set default figure placement to htbp
\makeatletter
\def\fps@figure{htbp}
\makeatother
\setlength{\emergencystretch}{3em} % prevent overfull lines
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
\setcounter{secnumdepth}{-\maxdimen} % remove section numbering
% Make \paragraph and \subparagraph free-standing
\ifx\paragraph\undefined\else
  \let\oldparagraph\paragraph
  \renewcommand{\paragraph}[1]{\oldparagraph{#1}\mbox{}}
\fi
\ifx\subparagraph\undefined\else
  \let\oldsubparagraph\subparagraph
  \renewcommand{\subparagraph}[1]{\oldsubparagraph{#1}\mbox{}}
\fi
\newlength{\cslhangindent}
\setlength{\cslhangindent}{1.5em}
\newlength{\csllabelwidth}
\setlength{\csllabelwidth}{3em}
\newlength{\cslentryspacingunit} % times entry-spacing
\setlength{\cslentryspacingunit}{\parskip}
\newenvironment{CSLReferences}[2] % #1 hanging-ident, #2 entry spacing
 {% don't indent paragraphs
  \setlength{\parindent}{0pt}
  % turn on hanging indent if param 1 is 1
  \ifodd #1
  \let\oldpar\par
  \def\par{\hangindent=\cslhangindent\oldpar}
  \fi
  % set entry spacing
  \setlength{\parskip}{#2\cslentryspacingunit}
 }%
 {}
\usepackage{calc}
\newcommand{\CSLBlock}[1]{#1\hfill\break}
\newcommand{\CSLLeftMargin}[1]{\parbox[t]{\csllabelwidth}{#1}}
\newcommand{\CSLRightInline}[1]{\parbox[t]{\linewidth - \csllabelwidth}{#1}\break}
\newcommand{\CSLIndent}[1]{\hspace{\cslhangindent}#1}
\ifLuaTeX
\usepackage[bidi=basic]{babel}
\else
\usepackage[bidi=default]{babel}
\fi
\babelprovide[main,import]{english}
% get rid of language-specific shorthands (see #6817):
\let\LanguageShortHands\languageshorthands
\def\languageshorthands#1{}
% Manuscript styling
\usepackage{upgreek}
\captionsetup{font=singlespacing,justification=justified}

% Table formatting
\usepackage{longtable}
\usepackage{lscape}
% \usepackage[counterclockwise]{rotating}   % Landscape page setup for large tables
\usepackage{multirow}		% Table styling
\usepackage{tabularx}		% Control Column width
\usepackage[flushleft]{threeparttable}	% Allows for three part tables with a specified notes section
\usepackage{threeparttablex}            % Lets threeparttable work with longtable

% Create new environments so endfloat can handle them
% \newenvironment{ltable}
%   {\begin{landscape}\centering\begin{threeparttable}}
%   {\end{threeparttable}\end{landscape}}
\newenvironment{lltable}{\begin{landscape}\centering\begin{ThreePartTable}}{\end{ThreePartTable}\end{landscape}}

% Enables adjusting longtable caption width to table width
% Solution found at http://golatex.de/longtable-mit-caption-so-breit-wie-die-tabelle-t15767.html
\makeatletter
\newcommand\LastLTentrywidth{1em}
\newlength\longtablewidth
\setlength{\longtablewidth}{1in}
\newcommand{\getlongtablewidth}{\begingroup \ifcsname LT@\roman{LT@tables}\endcsname \global\longtablewidth=0pt \renewcommand{\LT@entry}[2]{\global\advance\longtablewidth by ##2\relax\gdef\LastLTentrywidth{##2}}\@nameuse{LT@\roman{LT@tables}} \fi \endgroup}

% \setlength{\parindent}{0.5in}
% \setlength{\parskip}{0pt plus 0pt minus 0pt}

% Overwrite redefinition of paragraph and subparagraph by the default LaTeX template
% See https://github.com/crsh/papaja/issues/292
\makeatletter
\renewcommand{\paragraph}{\@startsection{paragraph}{4}{\parindent}%
  {0\baselineskip \@plus 0.2ex \@minus 0.2ex}%
  {-1em}%
  {\normalfont\normalsize\bfseries\itshape\typesectitle}}

\renewcommand{\subparagraph}[1]{\@startsection{subparagraph}{5}{1em}%
  {0\baselineskip \@plus 0.2ex \@minus 0.2ex}%
  {-\z@\relax}%
  {\normalfont\normalsize\itshape\hspace{\parindent}{#1}\textit{\addperi}}{\relax}}
\makeatother

% \usepackage{etoolbox}
\makeatletter
\patchcmd{\HyOrg@maketitle}
  {\section{\normalfont\normalsize\abstractname}}
  {\section*{\normalfont\normalsize\abstractname}}
  {}{\typeout{Failed to patch abstract.}}
\patchcmd{\HyOrg@maketitle}
  {\section{\protect\normalfont{\@title}}}
  {\section*{\protect\normalfont{\@title}}}
  {}{\typeout{Failed to patch title.}}
\makeatother

\usepackage{xpatch}
\makeatletter
\xapptocmd\appendix
  {\xapptocmd\section
    {\addcontentsline{toc}{section}{\appendixname\ifoneappendix\else~\theappendix\fi\\: #1}}
    {}{\InnerPatchFailed}%
  }
{}{\PatchFailed}
\keywords{Publication Bias; File-Drawer; Registry; Research Waste\newline\indent Word count: 1111}
\usepackage{dblfloatfix}


\usepackage{csquotes}
\ifLuaTeX
  \usepackage{selnolig}  % disable illegal ligatures
\fi
\IfFileExists{bookmark.sty}{\usepackage{bookmark}}{\usepackage{hyperref}}
\IfFileExists{xurl.sty}{\usepackage{xurl}}{} % add URL line breaks if available
\urlstyle{same} % disable monospaced font for URLs
\hypersetup{
  pdftitle={An Inception Cohort Study Quantifying How Many Registered Studies are Published.},
  pdfauthor={Eline Ensinck1 \& Daniël Lakens1},
  pdflang={en-EN},
  pdfkeywords={Publication Bias; File-Drawer; Registry; Research Waste},
  hidelinks,
  pdfcreator={LaTeX via pandoc}}

\title{An Inception Cohort Study Quantifying How Many Registered Studies are Published.}
\author{Eline Ensinck\textsuperscript{1} \& Daniël Lakens\textsuperscript{1}}
\date{}


\shorttitle{QUANTIFYING PUBLISHED REGISTERED STUDIES}

\authornote{

This work was funded by VIDI Grant 452-17-013 from the Netherlands Organisation for Scientific Research. The computationally reproducible version of this manuscript, are available at \url{https://github.com/Lakens/xxx}.

Correspondence concerning this article should be addressed to Daniël Lakens, Den Dolech 1, 5600MB Eindhoven, The Netherlands. E-mail: \href{mailto:D.Lakens@tue.nl}{\nolinkurl{D.Lakens@tue.nl}}

}

\affiliation{\vspace{0.5cm}\textsuperscript{1} Eindhoven University of Technology}

\abstract{%
We quantified how many research studies registered on the Open Science Framework (OSF) are performed but never publicly shared. Examining 315 registrations revealed that 169 were research studies, of which 104 (62\%) were published. For the whole population of registrations on the OSF before November 2017 we estimate 58\% is published. Researchers use registries to make unpublished studies public, and the OSF policy to automatically make registrations public substantially increases the number of performed studies that become known to the scientific community. It is often challenging to identify whether public registrations are published due to a lack of information in registrations. In responses to emails asking researchers why studies remained unpublished, logistics (e.g., lack of time, researchers changing jobs) was the most common reason, followed by null results and rejections during peer review. Our project shows a substantial amount of data that researchers collect is never publicly shared. Selectively publishing studies is problematic because of the possibility that the resources used to collect the data are wasted, and if the publication of studies depend on the results their non-publication can cause bias in the scientific literature. Although it is common knowledge that researchers have studies in their filedrawer, it is extremely difficult to quantify how many studies in psychology are performed, but never published. The OSF has a policy to make all registrations public after four years. This allows us to perform the first inception cohort study on the OSF where we attempt to examine for registered studies if they end up in the scientific literature after at least 4 years. A better understanding of how many studies remain unpublished, and the reasons why researchers did not publish studies, enables researchers to seriously engage with the question how internal inefficiency in science can be reduced.
}



\begin{document}
\maketitle

Scientists selectively publish studies they perform (\protect\hyperlink{ref-franco_publication_2014}{Franco, Malhotra, \& Simonovits, 2014}; \protect\hyperlink{ref-greenwald_consequences_1975}{Greenwald, 1975}; \protect\hyperlink{ref-sterling_publication_1959}{Sterling, 1959}). Studies remain unpublished for a variety of reasons, such as a lack of resources to analyse the results, a loss of interest, perceived methodological weaknesses, or because the results do not support predictions (\protect\hyperlink{ref-easterbrook_publication_1991}{Easterbrook, Berlin, Gopalan, \& Matthews, 1991}; \protect\hyperlink{ref-lishner_sorting_2021}{Lishner, 2021}). Research resources are wasted when studies that have the potential to contribute to scientific knowledge go unpublished, (\protect\hyperlink{ref-buxton_avoiding_2021}{Buxton et al., 2021}; \protect\hyperlink{ref-chalmers_avoidable_2009}{Chalmers \& Glasziou, 2009}).

The practice of selective reporting has been demonstrated in various disciplines, ranging from psychology and sociology to economics (\protect\hyperlink{ref-fanelli_positive_2010}{Fanelli, 2010}; \protect\hyperlink{ref-franco_publication_2014}{Franco et al., 2014}; \protect\hyperlink{ref-gerber_publication_2010}{Gerber, Malhotra, Dowling, \& Doherty, 2010}). It is challenging to quantify how many studies remain unpublished. The most accurate estimates come from inception cohort studies where studies are -- typically retroactively - followed from the moment they are started (\protect\hyperlink{ref-song_dissemination_2010}{Song et al., 2010}). Dickersin and colleagues observed that from all 737 studies submitted to John Hopkins institutional review boards of the school of medicine and health 81\% and 66\% were published, respectively (\protect\hyperlink{ref-dickersin_factors_1992}{Dickersin, Min, \& Meinert, 1992}). Publication bias was mainly caused by researchers not submitting non-significant results, as only 6 out of 124 non-significant results submitted for publication were rejected by a journal. Of 293 clinical trials funded by NIH in 1979, 93\% was published after a decade (\protect\hyperlink{ref-dickersin_nih_1993}{Dickersin \& Min, 1993}). In the only inception cohort study in the social sciences we know of, Franco et al.~(2014) found that out of 249 experiments 106 were published in a scientific journal or as a book chapter (45\%), 29\% of studies were written up but remained unpublished at the time of the study, and 20\% (primarily non-significant results) were unpublished and not written up.

One solution to prevent the selective reporting of studies is to require researchers to register their study in a publicly accessible database. An example of such a study registry is ClinicalTrials.gov (\protect\hyperlink{ref-dickersin_evolution_2012}{Dickersin \& Rennie, 2012}). The requirement to register certain types of studies does not exist in psychology. However, since 2013 the Open Science Framework (OSF) has provided a free study registry for any scientific domain (\protect\hyperlink{ref-spies_open_2013}{Spies, 2013}). The OSF offers researchers the possibility to voluntarily preregister their study by specifying their hypotheses, methods, and statistical analysis plan.

One interesting aspect of preregistrations on the OSF is the fact that every registration after June 8 2015 (if not withdrawn) will be made public after an embargo period of four years\footnote{see \url{https://web.archive.org/web/20230519110220/https://groups.google.com/g/openscienceframework/c/3ZaZYKV8WD4/m/JCN4OB31skQJ}}. This policy means that the OSF provides a public record of all studies that were registered on the platform up to four years ago. It is therefore possible to use the database of preregistered studies for an inception cohort study that quantifies how many studies are performed, but never publicly shared.

\hypertarget{the-current-study}{%
\section{The current study}\label{the-current-study}}

In this project we aimed to answer four main research questions. First, we were interested in estimating the number of registered studies on the OSF that were performed but remained unpublished after four years. Although this population of studies is not representative of all studies researchers perform, it is more representative of the studies researchers perform than the study by Franco and colleagues (2014) as any researcher can register a study on the OSF.

Second, we were interested in whether a registry with the policy to make registration public after an embargo facilitates identifying unpublished studies. By examining the publication status of registrations manually made public by researchers with registrations automatically opened by the OSF after four years we can estimate how researchers voluntarily share that they have collected data, even when the study remains unpublished, and how often a policy to automatically make a registration public makes performed but unpublished studies findable. This comparison can provide insights into the benefits of a platform that makes registrations public after a certain time, compared to platforms that permanently keep registrations private (e.g., AsPredicted.org).

Our third question concerns the possibility to correctly identify whether the study related to a registration is publicly shared, or not. For a registry to notify the scientific community about the presence of unpublished studies, the information in the registry needs to be complete enough to identify relevant studies and contact the researchers who performed the study. As far as we know, this is the first investigation that retrieves registrations from the OSF to attempt to identify whether the associated study has been publicly shared or not.
Finally, while it is useful to quantify how many preregistered studies remain unpublished after four years, it is also important to understand why studies remain unpublished. The fourth aim of our study was to contacted the authors of registrations of studies that were not publicly shared and ask them whether the study indeed remained unpublished, and if so, why their study remained unpublished.

\hypertarget{method}{%
\section{Method}\label{method}}

\emph{Data sources}: As registrations become public after four years, we downloaded information about 17.729 registrations created on the OSF before November 18, 2017 (four years before the start of this study) through its application programming interface (API). Registrations are part of at least one associated project page on the OSF. Researchers might still work on projects four years after a registration. The final classification was based on whether the projects were public in April 2022. For 8008 registrations (45.17\%) the associated project was public, while for the remaining 9721 registrations (54.83\%) the associated project is private. We randomly sampled registrations from both these groups. During data analysis we made sure that we did not include the same study twice (even though a single study can have multiple registrations, for example because an analysis subcomponent and a hypothesis subcomponent each get a separate ID in the OSF database). Our sampling strategy means that a study with multiple registrations is more likely to be included in our final sample.

\emph{Classifying Registrations}: We aimed to examine at least 300 registrations (150 for which the preregistration was made public by the researchers, and 150 for which the preregistration was made public automatically after four years ). The OSF API does not provide information about how a project has been made public. To circumvent this limitation, we relied on a proxy indicator to classify projects as opened by the user, or opened automatically when the embargo was lifted. We classified OSF preregistrations as made public by the researcher when any of the associated OSF project pages was public, and classified the project as made public automatically by the OSF after four years when the associated project page was not made public. Our reasoning is that when only the registration is public, but the underlying project remains closed, the registration is most likely opened automatically, as there are not that many use-cases where a researcher would open the registration, but not the associated project. This is not a perfect proxy, as it is possible for users to make a registration public, but to keep the associated OSF project hidden (for example, because they mistakenly believe they made the entire project public when making the registration public). When the registration and underlying project are public, we assume the registration is opened by the researchers. This assumption is more likely to hold, as only users can make underlying project pages public.

\emph{Classifying Publication Status}: Studies can be accessible in different degrees of accessibility, ranging from never being analysed (for example because the researcher who collected the data no longer works in science) to presented at a conference, to being published in a peer reviewed journal (\protect\hyperlink{ref-lishner_sorting_2021}{Lishner, 2021}). We considered a study `published' if the study was communicated in any form that allows other researchers to learn about the results. In the remainder of this article the term `published' is used when we classified studies as publicly shared because they appeared in the peer reviewed literature, but also when they were shared as a preprint, a PhD thesis in a stable repository, or a poster or conference paper that described the study in sufficient detail to include it in a meta-analysis. We did not include bachelor or master theses because even though these can eventually be published (as this article demonstrates) they primarily have an educational goal and we do not consider the file drawer problem equally applicable to this category of studies. This use of the term `published' differs from other inception cohort studies (e.g., Franco et al., 2014) which limit publication to journal articles. We believe the rise of preprints and stable repositories warrants a broader definition of what it means to publish the results of a study, and especially preprints can provide an important outlet for studies that a researcher might decide not to submit to a scientific journal.

\hypertarget{results}{%
\section{Results}\label{results}}

\emph{Inclusion criteria}: After some projects were reclassified during the data analysis in total 315 randomly chosen registrations from both groups were examined to see whether they met the selection criteria of our study. Of these, the associated project was public for 164 registrations and the associated project was closed for 151 registrations. As we are interested in estimating how many studies that were performed with the goal to publish the findings remain unpublished, registrations needed to be actual research studies where the goal of the researchers was to publish a paper. We therefore aimed to remove all registrations that were performed for practice purposes (e.g.~student assignments or trial registrations), as well as all registrations that were created to archive materials after the research project was completed (e.g., some journals ask researchers to use the registration option on the OSF to create a DOI that is linked to all files associated with the project after the manuscript has been accepted for publication). Furthermore, we excluded Registered Reports, as this publication format is much less susceptible to publication bias (\protect\hyperlink{ref-allen_open_2019}{Allen \& Mehler, 2019}; \protect\hyperlink{ref-chambers_past_2022}{Chambers \& Tzavella, 2022}; \protect\hyperlink{ref-scheel_excess_2021}{Scheel, Schijen, \& Lakens, 2021}). For each registration we coded whether they were (1) withdrawn, (2) a research study, (3) part of a multi-study project (i.e., the Reproducibility Project: Psychology), (4) a preregistration that occurred at the inception of the research project (i.e., not upon publication to generate a stable DOI for supplementary materials), and (5) a registered report.

The number of included and excluded registrations in the sample is summarized in Table \ref{tab:table-included}. We originally included 176 registrations. After excluding 7 registrations based on email responses for being student projects, or being performed without the goal of publishing the data, our final sample size consists of 169 registrations, where for 88 registrations the main project was public and for 81 registrations the main project was not public. An overview of how often registrations were excluded for each of the 5 inclusion criteria is provided in Table \ref{tab:table-excluded}.

\begin{table*}[tbp]

\begin{center}
\begin{threeparttable}

\caption{\label{tab:table-included}Studies included in the analysis before and after e-mailing researchers.}

\begin{tabular}{lll}
\toprule
 & \multicolumn{1}{c}{Manually Opened (n = 164)} & \multicolumn{1}{c}{Automatically Opened (n = 151)}\\
\midrule
Included Before Emails & 90 & 86\\
Percentage Before Emails & 55 & 57\\
Included After Emails & 88 & 81\\
Percentage After Emails & 54 & 54\\
Total & 164 & 151\\
\bottomrule
\addlinespace
\end{tabular}

\begin{tablenotes}[para]
\normalsize{\textit{Note.} The sum is greater than the total number of excluded registrations because an individual registration can be excluded for multiple reasons.}
\end{tablenotes}

\end{threeparttable}
\end{center}

\end{table*}

\begin{table*}[tbp]

\begin{center}
\begin{threeparttable}

\caption{\label{tab:table-excluded}Reasons studies were excluded from the analysis.}

\begin{tabular}{lll}
\toprule
 & \multicolumn{1}{c}{Manually Opened} & \multicolumn{1}{c}{Automatically Opened}\\
\midrule
Withdrawn & 1 & 5\\
No Research Study & 21 & 50\\
No Unique Study & 13 & 5\\
No Registration & 27 & 20\\
Registered Report & 13 & 3\\
Replication Project & 12 & 1\\
\bottomrule
\end{tabular}

\end{threeparttable}
\end{center}

\end{table*}

\emph{Identifying Publications Corresponding to Registrations}: We attempted to find a publication (e.g., a journal article, preprint, thesis, or scientific poster) associated with each registration included in the analysis. Ideally, it is possible to find a publication associated with a registration without the help of the researchers who performed the study. In practice, due to the often limited information included in a registration on the OSF, information provided by the researchers improved our ability to classify registrations as publicly shared or not. If a publication could be found that provided a direct link to the OSF registration we could be certain that we found the publication corresponding to the registration. In many cases, especially when registrations on the OSF contained little information, and publications did not link to a registration on the OSF, there was some uncertainty about whether a paper corresponded to a registration. Furthermore, if no publication could be found, there always remained the possibility that we failed to find it. Based on an extensive search (for details, see the online supplement) we classified 68 out of the 169 registrations as published, 71 articles as unpublished, and for 30 articles we were uncertain about whether the registration was published, for example because we found a paper on the same topic, but this paper did not contain a link to the OSF registration, or there was very little information in the registration.

For all registrations where we were uncertain about whether a publication corresponded to the registration, we attempted to retrieve contact information of researchers involved, and contacted the authors. We also contacted researchers associated with all registrations classified after an extensive search as unpublished, to check if the registration was indeed unpublished, and if so, ask the researchers why the study remained unpublished. We contacted 109 researchers, 29 authors with registrations associated with public OSF projects and 80 researchers with registrations associated with closed OSF projects, of which 24 (26.61\%) and 53 (73.39\%) replied to our email with additional information to update our classification. We sent 1 email for a registration classified as published because we found a thesis in a repository to check if a paper had been published (and received 1 response), for one registration we did not have to email the researcher because we could retrieve the required information from a paper in which they explicitly discussed their file-drawer (\protect\hyperlink{ref-van_elk_whats_2021}{van Elk, 2021}), 77 emails for registrations we believed remained unpublished (and received 51 responses), and 31 emails because we were uncertain if they were published or not (and received 25 responses). Two additional authors replied to our email, but did not give consent to use their replies in our data analysis, and we therefore did not change the classification of these two registrations.

After updating our classification based on email responses, the publication rate in group 1 increased by 6\% (meaning that we failed to identify some publications corresponding to registrations) while the publication rate in group 2 decreased by 1\%. Sometimes a publication we found in the literature did not belong to the preregistration. If the study associated with the registration was published in another article than the one we identified, the classification remained the same, but we still counted this registration as a mismatch because we failed to identify the correct article without the help of the researchers. In total, 48 of the 76 responses match with publications we found. For the 50 registrations we classified as unpublished, responses to the follow-up emails indicated that 36 (72\%) were unpublished, while for 14 (28\%) we failed to identify the published paper based on the information in the registration.

We received 25 responses for registrations we were uncertain about, and these follow-up emails indicated that 11 (44\%) were unpublished, while for 14 (28\%) we failed to identify the published paper based on the information in the registration.

Most of the mismatches occurred when we were uncertain about whether a paper we found included the registered study because there was no link to the registration on the OSF and only some terminology in the registration and article overlapped. For the 25 registrations where we were uncertain if the study was published, 11 (44\%) of registrations matched a paper we found. Of the 14 cases in this group that were a mismatch, 2 were indeed published but we failed to find the correct paper, and the remaining 12 were actually not published. In one special case the paper we found was indeed a match, but we decided to change the final classification to unpublished as the registration belonged to two studies and only one of the registered studies was published. Altogether, these results indicate that without contacting the researchers it is difficult to categorize studies as published or unpublished based exclusively on the information researchers provide in their preregistration on the OSF.

Due to non-response to our emails there is remaining uncertainty about the number of registrations in our sample that were eventually published. The 26 non-responses related to registrations where we could not find a publication might contain some publications that we missed, and the 6 non-responses related to registrations where we remained uncertain similarly contain some published studies. Researchers might be more likely to reply when their registration is actually published, or non-response might be unaffected by the publication status of their registration. Under the latter assumption, we can use the observed percentages of published studies in both categories to predict that responses from researchers would have told us that 19.45 out of 26 of the registration in the unpublished classification and 3.03 out of 6 of the registration in the uncertain classification are actually published.

\begin{table*}[tbp]

\begin{center}
\begin{threeparttable}

\caption{\label{tab:table-estimated-nonpublication}Estimated number of published and unpublished studies registered on the OSF before November 2017.}

\begin{tabular}{lllllll}
\toprule
Group & \multicolumn{1}{c}{Registrations} & \multicolumn{1}{c}{\% Included} & \multicolumn{1}{c}{Included} & \multicolumn{1}{c}{\% Published} & \multicolumn{1}{c}{Published} & \multicolumn{1}{c}{Not Published}\\
\midrule
Manually Opened & 8008 & 0.55 & 4,395 & 0.86 & 3,775 & 619\\
Automatically Opened & 9042 & 0.57 & 5,150 & 0.34 & 1,775 & 3,375\\
Total & 17050 &  & 9,544 &  & 5,550 & 3,994\\
\bottomrule
\end{tabular}

\end{threeparttable}
\end{center}

\end{table*}

After incorporating responses from researchers, and extrapolating these responses to the registrations where we did not receive a response, we predict that out of a total of 169 included registrations, 103.52 registered studies are published (61.25\%, 95\% CI {[}53.74; 68.27{]}). This sample contained an almost equal number of manually opened and automatically opened registrations. As can be seen in Table \ref{tab:table-estimated-nonpublication} manually opened registrations were much more likely to be published than automatically opened registrations. Multiplying our publications estimates by the estimated number of research studies registered on the OSF before November 2017 we see 5550 registrations are published, and 3994 remain unpublished. Therefore, in the total population of all registrations on the OSF from before November 2017 we estimated that 58.15\% of registrations are published as an article, preprint, thesis, or poster after at least four years.

\hypertarget{evaluating-the-effect-of-automatically-making-registries-public}{%
\section{Evaluating the Effect of Automatically Making Registries Public}\label{evaluating-the-effect-of-automatically-making-registries-public}}

We see that 85.91\%, 95\% CI {[}77.13; 91.68{]} of all registrations that based on our proxy, were classified as manually opened by the researchers have been published. In the remaining 14.09\% of these registrations researchers have voluntarily chosen to make the registration public, even though the study was not published. This shows that a platform that enables researchers to make their registration public will make studies that would otherwise remain in the file-drawer known to the research community. We estimate that 34.47\%, 95\% CI {[}25.04; 45.31{]} of automatically opened registrations is published. Consequently, 65.53\% of these registrations would not be known to the research community were it not for the fact that the OSF automatically opens registrations after four years. In other words, automatically opening registrations increases our awareness of all studies have been performed, above and beyond enabling researchers to create public registrations that they can voluntarily make public.

\hypertarget{why-do-studies-remained-unpublished}{%
\section{Why do studies remained unpublished?}\label{why-do-studies-remained-unpublished}}

When registrations were classified as unpublished, or when we were uncertain about whether a registration was published, we reached out to the researchers to ask them whether their registration was published or not, and if not, why it remained unpublished. We also asked why they made the corresponding OSF project public or why they kept it private, and if the researchers were aware that the OSF would automatically make their registration public after four years. This study was approved by the ERB at Eindhoven University of Technology under proposal number 1674.

There can be many reasons why a researcher does not publish a performed study. We classified the responses given by researchers into 5 categories (see Table \ref{tab:table-reasons}, for a more detailed overview, see the Supplementary Material). Researchers indicated their motivation to not publish were based on (1) the results of the study, such as null or unclear results and failed replications (25\%), (2) the review process, such as that the paper was rejected, or that reviewers thought it would be better to leave this study out (27\%), (3) the goal of the study, which was not publishing the results because the project was educational or intended to be shared with stakeholders (4\%), (4) planning, such as a lack of time, or a new job on a different research topic or outside of academia (45\%), and (5) problems with the study design (5\%).

According to the responses we received, issues related to `planning' are the biggest cause of unpublished studies in our sample. Researchers leave academia and fail to complete a project before their contract ends, move on to a new position with a different research focus, or experience a lack of time to complete this specific project given other responsibilities. In some of these cases researchers did intend to come back to this project, and we observed continuing activity in projects registered more than four years ago, including publications that appeared in the literature.

\begin{table*}[tbp]

\begin{center}
\begin{threeparttable}

\caption{\label{tab:table-reasons}Summary of main reasons researchers self-reported to not publish registered studies.}

\begin{tabular}{llll}
\toprule
 & \multicolumn{1}{c}{Manually Opened} & \multicolumn{1}{c}{Automatically Opened} & \multicolumn{1}{c}{Total}\\
\midrule
Results & 3 & 11 & 14\\
Peer Review & 6 & 5 & 11\\
Not Goal to Publish & 0 & 2 & 2\\
Planning & 7 & 18 & 25\\
Study Design & 1 & 2 & 3\\
\bottomrule
\addlinespace
\end{tabular}

\begin{tablenotes}[para]
\normalsize{\textit{Note.} Only the first or main reason was coded whenever respondents gave multiple reasons.}
\end{tablenotes}

\end{threeparttable}
\end{center}

\end{table*}

When asked why researchers made their OSF project page open, most researchers (2 out of 23 mentioned a motivation to practice open science. Out of 43 responses more diverse reasons were given for keeping the OSF project closed, such as that there was no relevant information there (15 times), researchers forgot to open it (11 times), waiting until a project was published (6 times), and technical issues (4 times). Finally, we were interested in how many researchers who preregistered on the OSF before 2017 - many of which should be considered early adopters - were aware of the OSF policy to automatically open all registrations after four years. Although not all of the email respondents answered this question, 24 of the researchers indicated they were aware of this policy, and 14 were not aware of this policy.

\hypertarget{discussion}{%
\section{Discussion}\label{discussion}}

Our examination of 169 registration on the Open Science framework created before November 18, 2017, led to an estimated (61.25\%, 95\% CI {[}53.74; 68.27{]}) of registrations that were published. Extrapolating from our sample, we estimate that of the 9544 registered research studies on the OSF that met our inclusion criteria, 3994 remain unpublished. The main reason researchers give when asked why their studies remain unpublished are logistical issues related to a lack of time, or responsible researchers moving on to a new job. Our study provides an important datapoint to understand how many studies are performed, but never shared. The possibility that around 38.75\% of research studies registered on the OSF are not shared should make scientists reflect on the efficiency of scientific research. Although not all unreported studies necessarily reflect research waste, our results suggest scientists might need to seriously engage with the question how internal inefficiency can be reduced (\protect\hyperlink{ref-bernal_social_1939}{Bernal, 1939}; \protect\hyperlink{ref-chalmers_avoidable_2009}{Chalmers \& Glasziou, 2009}), wherever this waste is avoidable.

The fact that for 20\% of unpublished studies researchers indicated that the results (e.g., null results or failed replications) were the reason that the study remained unpublished are concerning in light of the possibility that the absence of these studies contributed to publication bias in the scientific literature. Researchers can prevent the results of a study from influencing the probability that their study will be published by submitting their registration as a Registered Report (\protect\hyperlink{ref-chambers_past_2022}{Chambers \& Tzavella, 2022}; \protect\hyperlink{ref-nosek_registered_2014}{Nosek \& Lakens, 2014}). Compared to the standard literature, where almost all studies support their hypotheses, Registered Reports are much more likely to report null results (\protect\hyperlink{ref-allen_open_2019}{Allen \& Mehler, 2019}; \protect\hyperlink{ref-scheel_excess_2021}{Scheel et al., 2021}). If researchers do not report failed replications it is impossible to correct false positive claims in the scientific literature, and if null-results remain unreported effect size estimates in meta-analyses will be inflated to an extent. With the rise of data repositories and preprints it has become feasible and easy to share the results of studies, although future work that makes it easier to find, evaluate, and integrate study results is important.

It is important to keep in mind that our estimate of the number of registrations on the OSF that are published after four years has 1) remaining uncertainty, and 2) is unlikely to generalize beyond the specific context of our study. First, even after contacting researchers, due to non-response some registrations will be classified incorrectly, and the true number of unpublished studies in our sample could differ slightly from the numbers we report. Second, the estimate is specific to the context of our study, which were all registrations on the Open Science Framework created before November 18, 2017. The practice to register studies is becoming more widespread, but up to 2017 was predominantly done by early adopters interested in open science practices. It is possible that this group of researchers differs in how likely they are to share studies they have performed compared to researchers who did not preregister in this time-window, for example because researchers who adopt open science practices early more actively aim to publish null results, or reduce the size of their file-drawer. We studied the probability that a registered study was published, which might differ from the probability that a non-registered study is published. Registering a study requires work, and it possible that researchers only register studies they feel have a higher probability of being submitted for publication. Because the practice of registering studies on the OSF up to 2017 was more widespread in experimental social psychology, it remains unknown how many studies remain unpublished in other disciplines. The probability that researchers have the time to share data from a registered study might change over time, and could for example be in recent years due to the COVID pandemic. In short, it is unclear whether our estimates generalize to different populations of scientists, different populations of studies, different disciplines, and whether this estimate remains stable over time.

However, our goal was not to establish a generalizable estimate. The variation in the percentage of unpublished studies across scientific contexts is undoubtedly large. At the same time, the factors that cause researchers to not publish results (e.g., a lack of time, responsible researchers leaving academia, null results, failed replications) play a role in many (if not all) scientific contexts. Our study demonstrates that the percentage of unpublished studies can be high, and deserves attention across scientific contexts where similar factors preventing the publication of studies play a role. In fields where most studies are submitted as a Registered Report, researchers work in larger teams where one team member takes over the work of a colleague who leaves academia, and all studies are designed to have a high probability to yield informative results, there might be few unpublished studies. In fields where Registered Reports are rare, team science is less common, and less informative studies are designed, estimates of published results might be similar to those observed in our sample. Researchers who take efficient knowledge generation in their own scientific context seriously might want to empirically examine the question how many studies go unpublished to determine if there is room for improvement.

\emph{Recommendations for Registries}. We found that registrations are sometimes used to create a stable repository after a study has been completed, and sometimes to register the existence (and sometimes methodological details) of a planned study. To be able to distinguish these to uses of registries, it might be useful to create separate categories of registrations (e.g., distinguishing `prospective registration' or `preregistration' from `post registration' or `data archiving'). A third registration option should be created for all registrations that are done for purely educational or practice purposes. The content of these registrations could remain the same, but some features (such as the automatically created DOI) could be omitted to save money. It would be useful for meta-scientific research if the API that allows researchers to access data from the registry could distinguish different types of registrations (e.g., pre- and post-registrations), and if the API made it possible to retrieve whether a registration was made public manually, or automatically after 4 years. Lastly, the OSF currently displays registrations of separate components of the same project as unique registrations. It would be clearer if separate components belonging to the same registration would not be displayed as the same registrations. Alternatively, an extra field could be added that indicates that a registration is a sub-component of a certain larger project.

\emph{Recommendations for Registry Users}. Researchers should be aware that the OSF only makes the registration public, but not the main project page. This means that, for example, in the ``OSF-Standard Pre-Data Collection Registration'' the only information that becomes visible is the answer to the question ``Has data collection begun for this project?'', the answer to the question ``Have you looked at the data?'', and the text in the field ``Other comments''. Some users wrote in this field ``See the preregistration document specifying the logic, hypotheses, and analyses'' but that files is not made accessible. If researchers want to use registrations to inform peers about a study they have performed, they will need to provide all relevant information in the ``Other comments'' field. It is recommended to follow a comprehensive preregistration template to increase the quality of the preregistration (\protect\hyperlink{ref-akker_effectiveness_2023}{Akker et al., 2023}) and to create machine-readable analysis code that describe when statistical hypotheses are corroborated or falsified (\protect\hyperlink{ref-lakens_improving_2021}{Lakens \& DeBruine, 2021}). Finally, it would be useful if researchers include contact information such that peers can reach out to them with inquiries about the study that was performed. Adding an ORCID ID (for example to your OSF profile) and an email address (while realizing that researchers do not stay at the same institution indefinitely) is good practice.

\hypertarget{conclusion}{%
\section{Conclusion}\label{conclusion}}

A substantial amount of data that researchers collect is never publicly shared. Every system will have some inefficiencies, and it is difficult to judge whether the percentage of unpublished data we observed in our sample is too high. What is perhaps most surprising is that researchers rarely talk about how many datasets they have collected that could have value for peers but that nevertheless linger in their filedrawer. Researchers might not feel like they can frankly discuss this topic due to the association of unpublished studies with publication bias. Although we found publication bias to be one reason to not share data, logistical issues were more commonly provided as a reason not to share data. More transparency about unpublished datasets would allow our field to learn and improve the way we work, and improve practices to increase the efficiency of psychological science.

\hypertarget{references}{%
\section{References}\label{references}}

\begingroup

\interlinepenalty = 10000

\hypertarget{refs}{}
\begin{CSLReferences}{1}{0}
\leavevmode\vadjust pre{\hypertarget{ref-akker_effectiveness_2023}{}}%
Akker, O. van den, Bakker, M., Assen, M. A. L. M. van, Pennington, C. R., Verweij, L., Elsherif, M., \ldots{} Wicherts, J. (2023). \emph{The effectiveness of preregistration in psychology: {Assessing} preregistration strictness and preregistration-study consistency}. {MetaArXiv}. \url{https://doi.org/10.31222/osf.io/h8xjw}

\leavevmode\vadjust pre{\hypertarget{ref-allen_open_2019}{}}%
Allen, C., \& Mehler, D. M. A. (2019). Open science challenges, benefits and tips in early career and beyond. \emph{PLOS Biology}, \emph{17}(5), e3000246. \url{https://doi.org/10.1371/journal.pbio.3000246}

\leavevmode\vadjust pre{\hypertarget{ref-bernal_social_1939}{}}%
Bernal, J. D. (1939). \emph{The {Social Function Of Science}}. {London}: {Routledge}.

\leavevmode\vadjust pre{\hypertarget{ref-buxton_avoiding_2021}{}}%
Buxton, R. T., Nyboer, E. A., Pigeon, K. E., Raby, G. D., Rytwinski, T., Gallagher, A. J., \ldots{} Roche, D. G. (2021). Avoiding wasted research resources in conservation science. \emph{Conservation Science and Practice}, \emph{3}(2), e329. \url{https://doi.org/10.1111/csp2.329}

\leavevmode\vadjust pre{\hypertarget{ref-chalmers_avoidable_2009}{}}%
Chalmers, I., \& Glasziou, P. (2009). Avoidable waste in the production and reporting of research evidence. \emph{The Lancet}, \emph{374}(9683), 86--89.

\leavevmode\vadjust pre{\hypertarget{ref-chambers_past_2022}{}}%
Chambers, C. D., \& Tzavella, L. (2022). The past, present and future of {Registered Reports}. \emph{Nature Human Behaviour}, \emph{6}(1), 29--42. \url{https://doi.org/10.1038/s41562-021-01193-7}

\leavevmode\vadjust pre{\hypertarget{ref-dickersin_nih_1993}{}}%
Dickersin, K., \& Min, Y. I. (1993). \href{https://www.ncbi.nlm.nih.gov/pubmed/8306005}{{NIH} clinical trials and publication bias}. \emph{The Online Journal of Current Clinical Trials}, \emph{Doc No 50}, {[}4967 words; 53 paragraphs{]}.

\leavevmode\vadjust pre{\hypertarget{ref-dickersin_factors_1992}{}}%
Dickersin, K., Min, Y. I., \& Meinert, C. L. (1992). \href{https://www.ncbi.nlm.nih.gov/pubmed/1727960}{Factors influencing publication of research results. {Follow-up} of applications submitted to two institutional review boards}. \emph{JAMA}, \emph{267}(3), 374--378.

\leavevmode\vadjust pre{\hypertarget{ref-dickersin_evolution_2012}{}}%
Dickersin, K., \& Rennie, D. (2012). The {Evolution} of {Trial Registries} and {Their Use} to {Assess} the {Clinical Trial Enterprise}. \emph{JAMA}, \emph{307}(17), 1861--1864. \url{https://doi.org/10.1001/jama.2012.4230}

\leavevmode\vadjust pre{\hypertarget{ref-easterbrook_publication_1991}{}}%
Easterbrook, P. J., Berlin, J. A., Gopalan, R., \& Matthews, D. R. (1991). Publication bias in clinical research. \emph{Lancet (London, England)}, \emph{337}(8746), 867--872. \url{https://doi.org/10.1016/0140-6736(91)90201-y}

\leavevmode\vadjust pre{\hypertarget{ref-fanelli_positive_2010}{}}%
Fanelli, D. (2010). {``{Positive}''} {Results Increase Down} the {Hierarchy} of the {Sciences}. \emph{PLoS ONE}, \emph{5}(4). \url{https://doi.org/10.1371/journal.pone.0010068}

\leavevmode\vadjust pre{\hypertarget{ref-franco_publication_2014}{}}%
Franco, A., Malhotra, N., \& Simonovits, G. (2014). Publication bias in the social sciences: {Unlocking} the file drawer. \emph{Science}, \emph{345}(6203), 1502--1505. \url{https://doi.org/10.1126/SCIENCE.1255484}

\leavevmode\vadjust pre{\hypertarget{ref-gerber_publication_2010}{}}%
Gerber, A. S., Malhotra, N., Dowling, C. M., \& Doherty, D. (2010). Publication {Bias} in {Two Political Behavior Literatures}. \emph{American Politics Research}, \emph{38}(4), 591--613. \url{https://doi.org/10.1177/1532673X09350979}

\leavevmode\vadjust pre{\hypertarget{ref-greenwald_consequences_1975}{}}%
Greenwald, A. G. (1975). Consequences of prejudice against the null hypothesis. \emph{Psychological Bulletin}, \emph{82}(1), 1--20. \url{https://doi.org/10.1037/h0076157}

\leavevmode\vadjust pre{\hypertarget{ref-lakens_improving_2021}{}}%
Lakens, D., \& DeBruine, L. M. (2021). Improving {Transparency}, {Falsifiability}, and {Rigor} by {Making Hypothesis Tests Machine-Readable}. \emph{Advances in Methods and Practices in Psychological Science}, \emph{4}(2), 2515245920970949. \url{https://doi.org/10.1177/2515245920970949}

\leavevmode\vadjust pre{\hypertarget{ref-lishner_sorting_2021}{}}%
Lishner, D. A. (2021). Sorting the {File Drawer}: {A Typology} for {Describing Unpublished Studies}. \emph{Perspectives on Psychological Science}, 1745691620979831. \url{https://doi.org/10.1177/1745691620979831}

\leavevmode\vadjust pre{\hypertarget{ref-nosek_registered_2014}{}}%
Nosek, B. A., \& Lakens, D. (2014). Registered reports: {A} method to increase the credibility of published results. \emph{Social Psychology}, \emph{45}(3), 137--141. \url{https://doi.org/10.1027/1864-9335/a000192}

\leavevmode\vadjust pre{\hypertarget{ref-scheel_excess_2021}{}}%
Scheel, A. M., Schijen, M. R. M. J., \& Lakens, D. (2021). An {Excess} of {Positive Results}: {Comparing} the {Standard Psychology Literature With Registered Reports}. \emph{Advances in Methods and Practices in Psychological Science}, \emph{4}(2), 25152459211007467. \url{https://doi.org/10.1177/25152459211007467}

\leavevmode\vadjust pre{\hypertarget{ref-song_dissemination_2010}{}}%
Song, F., Parekh, S., Hooper, L., Loke, Y. K., Ryder, J., Sutton, A. J., \ldots{} Harvey, I. (2010). Dissemination and publication of research findings: An updated review of related biases. \emph{Health Technology Assessment (Winchester, England)}, \emph{14}(8), iii, ix--xi, 1--193. \url{https://doi.org/10.3310/hta14080}

\leavevmode\vadjust pre{\hypertarget{ref-spies_open_2013}{}}%
Spies, J. R. (2013). \emph{The open science framework: {Improving} science by making it open and accessible}. {University of Virginia}.

\leavevmode\vadjust pre{\hypertarget{ref-sterling_publication_1959}{}}%
Sterling, T. D. (1959). Publication {Decisions} and {Their Possible Effects} on {Inferences Drawn} from {Tests} of {Significance--Or Vice Versa}. \emph{Journal of the American Statistical Association}, \emph{54}(285), 30--34. \url{https://doi.org/10.2307/2282137}

\leavevmode\vadjust pre{\hypertarget{ref-van_elk_whats_2021}{}}%
van Elk, M. (2021). What's hidden in my filedrawer and what's in yours? {Disclosing} non-published findings in the cognitive science of religion. \emph{Religion, Brain \& Behavior}, \emph{11}(1), 5--16. \url{https://doi.org/10.1080/2153599X.2020.1729233}

\end{CSLReferences}

\endgroup


\end{document}
