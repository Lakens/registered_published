---
title             : "An Inception Cohort Study Quantifying How Many Registered Studies are Published."
shorttitle        : "QUANTIFYING PUBLISHED REGISTERED STUDIES"
author: 
  - name          : "Eline Ensinck"
    affiliation   : "1"
    corresponding : no    # Define only one corresponding author
    email         : "e.n.f.ensinck@student.tue.nl"  
  - name          : "Daniël Lakens"
    affiliation   : "1"
    corresponding : yes    # Define only one corresponding author
    address       : "Den Dolech 1, 5600MB Eindhoven, The Netherlands"
    email         : "D.Lakens@tue.nl"
affiliation:
  - id            : "1"
    institution   : "Eindhoven University of Technology"
authornote: |
  This work was funded by VIDI Grant 452-17-013 from the Netherlands Organisation for Scientific Research. The computationally reproducible version of this manuscript is available at https://github.com/Lakens/registered_published and additional materials on https://osf.io/8uqfb/. 
abstract: |
  We quantified how many research studies registered on the Open Science Framework (OSF) are performed but never publicly shared. Examining 315 registrations revealed that 169 were research studies, of which 104 (62%) were published. For the whole population of registrations on the OSF before November 2017 we estimate 58% is published within at least 4 years. Researchers use registries to make unpublished studies public, and the OSF policy to automatically make registrations public substantially increases the number of performed studies that become known to the scientific community. It is often challenging to identify whether public registrations are published due to a lack of information in registrations. In responses to emails asking researchers why studies remained unpublished, logistics (e.g., lack of time, researchers changing jobs) was the most common reason, followed by null results and rejections during peer review. Our project shows a substantial amount of data that researchers collect is never publicly shared. Selectively publishing studies is problematic because of the possibility that the resources used to collect the data are wasted, and if the publication of studies depend on the results their non-publication can cause bias in the scientific literature. Although it is common knowledge that researchers have studies in their filedrawer, it is extremely difficult to quantify how many studies in psychology are performed, but never published. The OSF has a policy to make all registrations public after four years. This allows us to perform the first inception cohort study on the OSF where we attempt to examine for registered studies if they end up in the scientific literature after at least 4 years. A better understanding of how many studies remain unpublished, and the reasons why researchers did not publish studies, enables researchers to seriously engage with the question how internal inefficiency in science can be reduced.
  
keywords          : "Publication Bias; File-Drawer; Registry; Research Waste"
wordcount         : "1111"
bibliography      : ["RegistriesPublication.bib"]
floatsintext      : yes
figurelist        : no
tablelist         : no
footnotelist      : no
linenumbers       : no
mask              : no
draft             : no
figsintext        : yes
class             : "jou, a4paper"
output            : papaja::apa6_pdf
link-citations    : true
editor_options: 
  chunk_output_type: console
---

```{r load packages and data, include = FALSE}
# Load packages ----

library(readxl)
library(dplyr)
library(papaja)
library(binom)
library(xfun)

# Load datafile ----

registration_data <- read_excel("registration_data.xlsx")

```

Not all studies scientists perform end up in the scientific literature [@franco_publication_2014; @greenwald_consequences_1975; @sterling_publication_1959]. Studies remain unpublished for a variety of reasons, such as a lack of resources to analyse the results, a loss of interest, perceived methodological weaknesses, or because the results do not support predictions [@easterbrook_publication_1991; @lishner_sorting_2021]. When studies that have the potential to contribute to scientific knowledge go unpublished, research resources are wasted [@buxton_avoiding_2021; @chalmers_avoidable_2009]. The practice of selective publication has been demonstrated in various disciplines, such as psychology, sociology, economics, and medicine [@scheel_excess_2021; @franco_publication_2014; @gerber_publication_2010], but it is challenging to quantify how many studies remain unpublished, and are locked away in the proverbial file drawer [@rosenthal_file_1979].

The most accurate estimates of the percentage of performed studies in the file drawer come from inception cohort studies where studies are – typically retroactively - followed from the moment they are started [@song_dissemination_2010]. Dickersin and colleagues observed that from all 737 studies submitted to two John Hopkins institutional review boards of the school of medicine and the school of health respectively 81% and 66% were published [@dickersin_factors_1992]. Publication bias was mainly caused by researchers not submitting non-significant results, as only 6 out of 124 non-significant results submitted for publication were rejected by a journal. Of 293 clinical trials funded by NIH in 1979 93% was published after a decade [@dickersin_nih_1993]. In the only inception cohort study in the social sciences we know of, Franco et al. [-@franco_publication_2014] found that out of 249 experiments 106 were published in a scientific journal or as a book chapter (45%), 29% of studies were written up but remained unpublished at the time of the study, and 20% (primarily non-significant results) were unpublished and not written up.

One solution to prevent the selective reporting of studies is to require researchers to register their study in a publicly accessible database. An example of such a study registry is ClinicalTrials.gov [@dickersin_evolution_2012]. The requirement to register certain types of studies does not exist in psychology. However, since 2013 the Open Science Framework (OSF) has provided a free study registry for any scientific domain [@spies_open_2013]. The OSF offers researchers the possibility to voluntarily register their study by specifying their hypotheses, methods, and statistical analysis plan. 

One interesting aspect of registrations on the OSF is the fact that every registration after June 8 2015  (if not withdrawn) will be made public after an embargo period of four years^[see https://web.archive.org/web/20230519110220/https://groups.google.com/g/openscienceframework/c/3ZaZYKV8WD4/m/JCN4OB31skQJ]. This policy means that the OSF provides a public record of all studies that were registered on the platform up to four years ago. It is therefore possible to use the database of registered studies for an inception cohort study that quantifies how many studies are performed, but never publicly shared.

# The current study

In this project we aimed to answer four main research questions. First, we were interested in estimating the number of registered studies on the OSF that were performed but remained unpublished after four years. Although this population of studies is not representative of all studies researchers perform (see the discussion), it is more representative of the studies researchers perform than the study by Franco and colleagues [-@franco_publication_2014] as any researcher can register a study on the OSF.

Second, we were interested in whether a registry with the policy to make registration public after an embargo facilitates identifying unpublished studies. By comparing the publication status of registrations manually made public by researchers with registrations automatically opened by the OSF after four years we can estimate how researchers voluntarily share that they have collected data, even when the study remains unpublished, and how often a policy to automatically make a registration public makes performed but unpublished studies findable. This comparison can provide insights into the benefits of a platform that makes registrations public after a certain time, compared to platforms that permanently keep registrations private (e.g., AsPredicted.org).

Our third question concerns the possibility to correctly identify whether a registered study is publicly shared, or not. For a registry to notify the scientific community about the presence of unpublished studies, the information in the registry needs to be complete enough to match the registration to a published study, or to conclude the registered study remains unpublished. As far as we know, this is the first investigation that retrieves registrations from the OSF to attempt to identify whether the associated study has been publicly shared or not. 

Finally, while it is useful to quantify how many registered studies remain unpublished after four years, it is equally important to understand why studies remain unpublished. The fourth aim of our study was to contact researchers of registrations of studies that were not publicly shared and ask them whether the study indeed remained unpublished, and if so, ask why their study remains unpublished. 

```{r registry info, include = FALSE}
load("res_df_data.RData")
load("author_df.RData")

colnames(res_df) <-  c("reg_id", "reg_date_created", "reg_date_modified", "reg_date_registered", "reg_date_registered", "category", "description", "fork", "public", "reg_withdrawn", "reg_withdrawal_justification", "has_project", "parent_project", "project_id", "web_address", "project_created", "project_modified", "project_title", "project_public")

tot_res <- cbind(res_df, author_df)

# Number of parent projects
length(res_df$parent_project)
# 9721 projects closed and or withdrawn
sum(res_df$project_id == "NA")
# 8008 projects public
sum(res_df$project_public == TRUE)

# Total closed is total not public - total withdrawn = 9042
total_closed <- sum(res_df$project_id == "NA") - sum(res_df$reg_withdrawn == TRUE)
total_closed
# Total open 8008
total_open <- sum(res_df$project_public == TRUE)
total_open

```

# Method

*Data sources*: As registrations become public after four years, we downloaded information about 17.729 registrations created on the OSF before November 18, 2017 (four years before the start of this study) through its application programming interface (API). Registrations are part of at least one associated project page on the OSF. Sometimes researchers still work on projects four years after a registration, and we saw continued activity in some registrations in our sample. The final classification was based on whether the projects were public in April 2022. For `r sum(res_df$project_public == TRUE)` registrations (`r round(100 * sum(res_df$project_public == TRUE)/length(res_df$parent_project), 2)`%) the associated project was public, while for the remaining `r sum(res_df$project_id == "NA")`  registrations (`r round(100 * sum(res_df$project_id == "NA")/length(res_df$parent_project), 2)`%) the associated project is private. We sampled registrations from both these groups. During data analysis we made sure that we did not include the same study twice (even though a single study can have multiple registrations, for example because an analysis subcomponent and a hypothesis subcomponent each get a separate ID in the OSF database). Our sampling strategy means that a study with multiple registrations is more likely to be included in our final sample. 


```{r inclusion, include = FALSE}

# I need to fix the issue of number of included before and after emails.
# We included more first, and emailed people, and then removed some data. 

# Table 2 with reasons for exclusions ----

excl_table <- registration_data %>% group_by(classification) %>% summarise_at(c("withdrawn", "no_research_study", "no_unique_study", "no_preregistration", "registered_report", "replication_project"), sum)
# Note that the sum in this table is higher than the total number of excluded registrations because an individual registration can be excluded for multiple reasons.

# Select only included studies ----

# select only the 171 included studies
registration_data_included <- registration_data[which(registration_data$no_research_study == 0 & registration_data$withdrawn == 0 & registration_data$no_unique_study == 0 & registration_data$no_preregistration == 0 & registration_data$registered_report == 0 & registration_data$replication_project == 0), ]

# Total registrations included in the study
total_included <- nrow(registration_data_included)

# Percentage included before and after emails for Table 1 ----

included_before <- registration_data %>% group_by(classification) %>% summarise(sum_before = sum(included_before_emails), binom::binom.confint(x = sum(included_before_emails), n = length(classification), tol = 1e-8, method = "wilson"))

included_after <- registration_data %>% group_by(classification) %>% summarise(sum_after = sum(included_after_emails), binom::binom.confint(x = sum(included_after_emails), n = length(classification), tol = 1e-8, method = "wilson"))

included_table <- cbind(paste(included_before$sum_before, " (", sprintf("%1.2f%%", 100*included_before$mean), ")", sep = ""), paste(included_after$sum_after, " (",sprintf("%1.2f%%", 100*included_after$mean), ")", sep = ""))
  
rownames(included_table) <- c(paste("Manually Opened (n = ", included_after$n[1], ")", sep = ""), paste("Automatically Opened (n = ", included_after$n[2], ")", sep = ""))
colnames(included_table) <- c("Included Before Emails", "Included After Emails")
included_table


# How often did we conclude the registration was published, not published, or uncertain? ----

table(registration_data_included$published_before_email)
# Based on an extensive search (see supplement) we classified 68 out of the 169 registrations as published, and 71 articles are unpublished. For 30 articles, we were uncertain about whether the registration was published, for example because we found a paper on the same topic, but this paper did not contain a link to the OSF registration, and there was very little information in the registration.

# Table 2 with reasons for exclusions ----

excl_table <- as.data.frame(registration_data %>% group_by(classification) %>% summarise_at(c("withdrawn", "no_research_study", "no_unique_study", "no_preregistration", "registered_report", "replication_project"), sum))[,2:7]
rownames(excl_table) <- c("Manually Opened", "Automatically Opened")
colnames(excl_table) <- c("Withdrawn", "No Research Study", "No Unique Study", "No Registration", "Registered Report", "Replication Project")
excl_table
# Note that the sum in this table is higher than the total number of excluded registrations because an individual registration can be excluded for multiple reasons.

```

*Classifying Registrations*: We aimed to examine at least 300 registrations (150 for which the registration was made public by the researchers, and 150 for which the registration was made public automatically after four years ). The OSF API does not provide information about how a project has been made public. To circumvent this limitation, we relied on a proxy indicator to classify projects as opened by the user, or opened automatically when the embargo was lifted. We classified OSF registrations as made public by the researcher when any of the associated OSF project pages was public, and classified the project as made public automatically by the OSF after four years when the associated project page was not made public. Our reasoning is that when only the registration is public, but the underlying project remains closed, the registration is most likely opened automatically, as there are not that many use-cases where a researcher would open the registration, but not the associated project. This is not a perfect proxy, as it is possible for users to make a registration public, but to keep the associated OSF project hidden (for example, because they mistakenly believe they made the entire project public when making the registration public). When the registration and underlying project are public, we assume the registration is opened by the researchers. This assumption is more likely to hold, as only users can make underlying project pages public. 

*Classifying Publication Status*: Studies can be available in different degrees, ranging from never being analysed (for example because the researcher who collected the data no longer works in science) to presented at a conference, to being published in a peer reviewed journal [@lishner_sorting_2021]. We considered a study ‘published’ if the study was communicated in any form that allows other researchers to learn about the results. In the remainder of this article the term ‘published’ is used when we classified studies as publicly shared because they appeared in the peer reviewed literature, but also when they were shared as a preprint, a PhD thesis in a stable repository, or a poster or conference paper that described the study in sufficient detail to include it in a meta-analysis (see the online supplementary material for more detail). We did not include bachelor or master theses because even though these can eventually be published (as this article demonstrates) they primarily have an educational goal and we do not consider the file drawer problem equally applicable to this category of studies. This use of the term ‘published’ differs from other inception cohort studies (e.g., Franco et al., 2014) which limit publication to journal articles. We believe the rise of preprints and stable repositories warrants a broader definition of what it means to publish the results of a study, and especially preprints can provide an important outlet for studies that a researcher might decide not to submit to a scientific journal. 

# Results 

*Inclusion criteria*: After some projects were reclassified during the data analysis in total `r sum(included_after$n)` randomly chosen registrations from both groups were examined to see whether they met the selection criteria of our study. Of these, the associated project was public for `r included_after$n[1]` registrations and the associated project was closed for `r included_after$n[2]` registrations. Registrations needed to be actual research studies, as we are interested in estimating how many studies remain unpublished when researchers had the goal to publish. We therefore coded and excluded registrations that were (1) withdrawn, (2) performed for practice purposes (e.g. student assignments or trial registrations), (3) used to generate a stable DOI for supplementary materials at the end of a project, and studies that are much less susceptible to publication bias such as (4) Registered Reports, [@allen_open_2019; @scheel_excess_2021; @chambers_past_2022], and (5) multi-lab projects (i.e., the Reproducibility Project: Psychology).

The number of included and excluded registrations in the sample is summarized in Table \@ref(tab:table-included). We originally included `r sum(included_before$sum_before)` registrations. After excluding `r sum(included_before$sum_before) - sum(included_after$sum_after)` registrations based on email responses for being student projects, performed without the goal of publishing the data, or part of a multi-lab project, our final sample size consists of `r sum(included_after$sum_after)` registrations, where for `r included_after$sum_after[1]` registrations the main project was public and for `r included_after$sum_after[2]` registrations the main project was not public.  An overview of how often registrations were excluded for each of the 5 inclusion criteria is provided in Table \@ref(tab:table-excluded). 

```{r table-included, echo = FALSE}
apa_table(included_table, digits = 0, caption = "Studies included in the analysis before and after e-mailing researchers.")
```

```{r table-excluded, echo = FALSE}
apa_table(t(excl_table), digits = 0, caption = "Reasons studies were excluded from the analysis.", note = "The sum is greater than the total number of excluded registrations because an individual registration can be excluded for multiple reasons.")
```

*Identifying Publications Corresponding to Registrations*: We attempted to find a publication (e.g., a journal article, preprint, thesis, or scientific poster) associated with each registration included in the analysis. Ideally, it is possible to find a publication associated with a registration without the help of the researchers who performed the study. In practice, due to the often limited information included in a registration on the OSF, information provided by the researchers improved our ability to classify registrations as publicly shared or not. If a publication could be found that provided a direct link to the OSF registration we could be certain that we found the publication corresponding to the registration. In many cases, especially when registrations on the OSF contained little information, and publications did not link to a registration on the OSF, there was some uncertainty about whether a paper corresponded to a registration. Furthermore, if no publication could be found, there always remained the possibility that we failed to find it. Based on an extensive search (for details, see the online supplement) we classified `r table(registration_data_included$published_before_email)[2]` out of the `r sum(included_after$sum_after)` registrations as published, `r table(registration_data_included$published_before_email)[3]` registrations as unpublished, and for `r table(registration_data_included$published_before_email)[4]` registrations we were uncertain about whether the registration was published, for example because we found a paper on the same topic, but this paper did not contain a link to the OSF registration, or there was very little information in the registration. 

```{r email_responses, include = FALSE}
# We contacted 109 researchers

contacted_researchers <- sum(registration_data$email_response != "-")

# 29 authors with registrations associated with public OSF projects and 80 authors with registrations associated with closed OSF projects

contacted_researchers_per_group <- registration_data %>% group_by(classification) %>% summarise(sum = sum(email_response != "-"))

# of which 24 and 53 replied to our email with additional information to update our classification.

replies_per_group <- registration_data %>% group_by(classification) %>% summarise(sum = sum(email_response == 1))
percentage_responded <- contacted_researchers_per_group[,2] / contacted_researchers

# Responses by initial publication classification before emails
contacted_researchers_per_publication_status <- registration_data %>% group_by(published_before_email) %>% summarise(sum = sum(email_response != "-"))
responses_per_publication_status <- registration_data %>% group_by(published_before_email) %>% summarise(sum = sum(email_response == 1))

```

For all registrations where we were uncertain about whether a publication corresponded to the registration, we attempted to retrieve contact information of researchers involved, and contacted the researchers. We also contacted researchers associated with all registrations classified after an extensive search as unpublished, to check if the registration was indeed unpublished, and if so, ask the researchers why the study remained unpublished. We contacted `r contacted_researchers` researchers, `r contacted_researchers_per_group$sum[1]` researchers with registrations associated with public OSF projects and `r contacted_researchers_per_group$sum[2]` researchers with registrations associated with closed OSF projects, of which `r replies_per_group$sum[1]` (`r round(100*replies_per_group$sum[1]/contacted_researchers_per_group$sum[1],2)`%) and `r replies_per_group$sum[2]` (`r round(100*replies_per_group$sum[2]/contacted_researchers_per_group$sum[2],2)`%) replied to our email with additional information to update our classification. We sent `r numbers_to_words(contacted_researchers_per_publication_status$sum[2])` email for a registration classified as published because we found a thesis in a repository to check if a paper had been published (and received `r numbers_to_words(responses_per_publication_status$sum[2])` response), for one registration we did not have to email the researcher because we could retrieve the required information from a paper in which they explicitly discussed their file-drawer [@van_elk_whats_2021], `r contacted_researchers_per_publication_status$sum[3]` emails for registrations we believed remained unpublished (and received `r responses_per_publication_status$sum[3]` responses), and `r contacted_researchers_per_publication_status$sum[4]` emails because we were uncertain if they were published or not (and received `r responses_per_publication_status$sum[4]` responses). Two additional researchers replied to our email, but did not give consent to use their replies in our data analysis, and we therefore did not change the classification of these two registrations. 

```{r match, include = FALSE}

match_per_group <- registration_data %>% group_by(classification) %>% summarise(no_match = sum(match == 0), match = sum(match == 1))
match_per_group

sum(registration_data$match == 0)
sum(registration_data$match == 1)

sum(registration_data_included$match != "-")

# How many did we correctly classify when we thought they were published (1), unpublished (2), and when we were uncertain (3)?
match_per_classification <- registration_data %>% group_by(published_before_email) %>% summarise(no_match = sum(match == 0), match = sum(match == 1))

match_per_classification_by_category <- registration_data %>% group_by(published_before_email, classification) %>% summarise(no_match = sum(match == 0), match = sum(match == 1))

# add total column to compute percentages
match_per_classification_by_category$total <- match_per_classification_by_category$no_match + match_per_classification_by_category$match
match_per_classification_by_category$no_match_percentage <- match_per_classification_by_category$no_match/match_per_classification_by_category$total
match_per_classification_by_category$match_percentage <- match_per_classification_by_category$match/match_per_classification_by_category$total


```

After updating our classification based on email responses, the publication rate in group 1 increased by 6% (meaning that we failed to identify some publications corresponding to registrations) while the publication rate in group 2 decreased by 1%. Sometimes a publication we found in the literature did not belong to the registration. If the study associated with the registration was published in another source than the one we identified, the classification remained the same, but we still counted this registration as a mismatch because we failed to identify the correct article without the help of the researchers. In total, `r sum(registration_data$match == 1)` of the `r sum(registration_data$match != "-")` responses match with publications we found. For the `r sum(match_per_classification[3,2:3])` registrations we classified as unpublished, responses to the follow-up emails indicated that `r match_per_classification[3,3]` (`r round(100*match_per_classification[3,3]/sum(match_per_classification[3,2:3]),2)`%) were unpublished, while for `r match_per_classification[3,2]` (`r round(100*match_per_classification[3,2]/sum(match_per_classification[3,2:3]),2)`%) we failed to identify the published paper based on the information in the registration. Most of the mismatches occurred when we were uncertain about whether a paper we found included the registered study because there was no link to the registration on the OSF and only some terminology in the registration and article overlapped. For the `r sum(match_per_classification[4,2:3])` registrations where we were uncertain if the study was published, `r match_per_classification[4,3]` (`r round(100*(match_per_classification[4,3]/sum(match_per_classification[4,2:3])), 2)`%) of registrations matched a paper we found. Of the `r match_per_classification[4,2]` (`r round(100*match_per_classification[3,2]/sum(match_per_classification[3,2:3]),2)`%) cases in this group that were a mismatch, 2 were indeed published but we failed to find the correct paper, and the remaining 12 were actually not published. In one special case the paper we found was indeed a match, but we decided to change the final classification to unpublished as the registration belonged to two studies and only one of the registered studies was published. Altogether, these results indicate that without contacting the researchers it is often difficult to determine whether a registration is published based exclusively on the information researchers provide in their registration on the OSF. 

```{r published, include = FALSE}

# How many are published after email when we thought before emailing they were published (1), unpublished (2), and when we were uncertain (3)? Divide this per classification (manually opened and automatically opened) and whether we got an email response. 
published_after_email_per_classification <- registration_data_included %>% group_by(published_before_email, email_response, classification) %>% summarise(published = sum(published_after_email == 1), not_published = sum(published_after_email == 2), unsure = sum(published_after_email == 3))
published_after_email_per_classification

# How many did we think unpublished before emails?
total_assumed_unpublished <- sum(published_after_email_per_classification$published[5:8], published_after_email_per_classification$not_published[5:8])

# Of those, how many did we get a reply for?
total_assumed_unpublished_responses <- sum(published_after_email_per_classification$published[7:8], published_after_email_per_classification$not_published[7:8])

# Of those, how many did we not get a reply for?
total_assumed_unpublished_no_responses <- sum(published_after_email_per_classification$published[5:6], published_after_email_per_classification$not_published[5:6])

# For the 45 we got responses about, 14 were actually published, and 31 were not.
sum(published_after_email_per_classification$published[7:8])
sum(published_after_email_per_classification$not_published[7:8])

# Of the 14 published, 6 were from automatically opened registrations, out of 29 total closed projects, so 6/29 automatically opened projects we thought were unpublished were published. For the manually opened projects, 8/16 were published.
published_after_email_per_classification$published[7]
published_after_email_per_classification$published[7] + published_after_email_per_classification$not_published[7]
published_after_email_per_classification$published[8]
published_after_email_per_classification$published[8] + published_after_email_per_classification$not_published[8]

# We did not get responses for 26 projects, of which 22 were automatically opened, and 4 were manually opened, for which the following might hold: 
# 8/16 of the 4 manually opened registrations could be published.
# 6/29 of the 22 automatically opened registrations could be published
total_assumed_unpublished_no_responses
published_after_email_per_classification$not_published[5]
published_after_email_per_classification$not_published[6]

published_after_email_per_classification$published[7] / (published_after_email_per_classification$published[7] + published_after_email_per_classification$not_published[7]) * published_after_email_per_classification$not_published[5]

published_after_email_per_classification$published[8] / (published_after_email_per_classification$published[8] + published_after_email_per_classification$not_published[8]) * published_after_email_per_classification$not_published[6]

# So how many of the assumed unpublished are actually published?
published_non_response_unpublished <- ((published_after_email_per_classification$published[7] / (published_after_email_per_classification$published[7] + published_after_email_per_classification$not_published[7]) * published_after_email_per_classification$not_published[5]) + (published_after_email_per_classification$published[8] / (published_after_email_per_classification$published[8] + published_after_email_per_classification$not_published[8]) * published_after_email_per_classification$not_published[6]))

unpublished_non_response_unpublished <- total_assumed_unpublished_no_responses - ((published_after_email_per_classification$published[7] / (published_after_email_per_classification$published[7] + published_after_email_per_classification$not_published[7]) * published_after_email_per_classification$not_published[5]) + (published_after_email_per_classification$published[8] / (published_after_email_per_classification$published[8] + published_after_email_per_classification$not_published[8]) * published_after_email_per_classification$not_published[6]))

# Now for uncertain projects

# How many were we uncertain about before emails? 30
total_assumed_unpublished_uncertain <- sum(published_after_email_per_classification$published[9:12], published_after_email_per_classification$not_published[9:12], published_after_email_per_classification$unsure[9:12])

# Of those, how many did we get a reply for?
total_assumed_unpublished_uncertain_responses <-  sum(published_after_email_per_classification$published[11:12], published_after_email_per_classification$not_published[11:12], published_after_email_per_classification$unsure[11:12])

# Of those, how many did we not get a reply for?
total_assumed_unpublished_uncertain_no_responses <-  sum(published_after_email_per_classification$published[9:10], published_after_email_per_classification$not_published[9:10], published_after_email_per_classification$unsure[9:10])

# For the 24 we got responses about, 12 were actually published, and 12 were not.
sum(published_after_email_per_classification$published[11:12])
sum(published_after_email_per_classification$not_published[11:12])

#Of the 14 published, 6 were from automatically opened registrations, out of 29 total closed projects, so 6/29 automatically opened projects we thought were unpublished were published. For the manually opened projects, 8/16 were published.
published_after_email_per_classification$published[11]
published_after_email_per_classification$published[11] + published_after_email_per_classification$not_published[11]
published_after_email_per_classification$published[12]
published_after_email_per_classification$published[12] + published_after_email_per_classification$not_published[12]

# We did not get a response for 6 projects we were uncertain about, of which 1 was manually opened, and 5 were automatically opened. For the 24 registrations we were uncertain about that we did received a response for, 12 are published, and 12 are not. For the manually opened projects, 3 out of 5 were published, and for the automatically opened projects 9 out of 10 were published. We did not get responses for 6 projects, of which 5 were automatically opened, and 1 was manually opened, for which the following might hold: 

total_assumed_unpublished_uncertain_no_responses
published_after_email_per_classification$unsure[9]
published_after_email_per_classification$unsure[10]

published_after_email_per_classification$published[11] / (published_after_email_per_classification$published[11] + published_after_email_per_classification$not_published[11]) * published_after_email_per_classification$unsure[9]

published_after_email_per_classification$published[12] / (published_after_email_per_classification$published[12] + published_after_email_per_classification$not_published[12]) * published_after_email_per_classification$unsure[10]

# So how many of the uncertain registrations are actually published?
published_non_response_uncertain <- (published_after_email_per_classification$published[11] / (published_after_email_per_classification$published[11] + published_after_email_per_classification$not_published[11]) * published_after_email_per_classification$unsure[9]) + (published_after_email_per_classification$published[12] / (published_after_email_per_classification$published[12] + published_after_email_per_classification$not_published[12]) * published_after_email_per_classification$unsure[10])

unpublished_non_response_uncertain <- total_assumed_unpublished_uncertain_no_responses - ((published_after_email_per_classification$published[11] / (published_after_email_per_classification$published[11] + published_after_email_per_classification$not_published[11]) * published_after_email_per_classification$unsure[9]) + (published_after_email_per_classification$published[12] / (published_after_email_per_classification$published[12] + published_after_email_per_classification$not_published[12]) * published_after_email_per_classification$unsure[10]))

# For manually opened registrations, altogether the % of published studies is:

published_manual <- published_after_email_per_classification$published[2] + published_after_email_per_classification$published[4] + published_after_email_per_classification$published[7] + published_after_email_per_classification$published[11] + published_after_email_per_classification$published[7] / (published_after_email_per_classification$published[7] + published_after_email_per_classification$not_published[7]) * published_after_email_per_classification$not_published[5] + published_after_email_per_classification$published[11] / (published_after_email_per_classification$published[11] + published_after_email_per_classification$not_published[11]) * published_after_email_per_classification$unsure[9]

# For automatically opened registrations (classification 2), altogether the % of published studies is: 
# 6 we classified as published, 6 published we thought unpublished but confirmed in email response, +9 confirmed unpublished we were uncertain about, + 5 * 9/19 for the 5 unsure without response, and those we thought unpublished predicted to be published. 
published_auto <- published_after_email_per_classification$published[3] + published_after_email_per_classification$published[8] + published_after_email_per_classification$published[12] + published_after_email_per_classification$published[12] / (published_after_email_per_classification$published[12] + published_after_email_per_classification$not_published[12]) * published_after_email_per_classification$unsure[10] + published_after_email_per_classification$published[8] / (published_after_email_per_classification$published[8] + published_after_email_per_classification$not_published[8]) * published_after_email_per_classification$not_published[6]

# And the unpublished studies for manually opened registrations is:
unpublished_manual <- published_after_email_per_classification$not_published[7] + published_after_email_per_classification$not_published[11] + (1 - (published_after_email_per_classification$published[7] / (published_after_email_per_classification$published[7] + published_after_email_per_classification$not_published[7]))) * published_after_email_per_classification$not_published[5] + (1 - (published_after_email_per_classification$published[11] / (published_after_email_per_classification$published[11] + published_after_email_per_classification$not_published[11]))) * published_after_email_per_classification$unsure[9]

# And the unpublished automatically opened studies are:
unpublished_auto <- published_after_email_per_classification$not_published[8] + published_after_email_per_classification$not_published[12] + (1 - (published_after_email_per_classification$published[8] / (published_after_email_per_classification$published[8] + published_after_email_per_classification$not_published[8]))) * published_after_email_per_classification$not_published[6] + (1 - (published_after_email_per_classification$published[12] / (published_after_email_per_classification$published[12] + published_after_email_per_classification$not_published[12]))) * published_after_email_per_classification$unsure[10]

# Check to see if they combine to 169 total included:
published_manual + published_auto + unpublished_manual + unpublished_auto

# Compute CI around estimates per group
result_total <- binom.confint(x = published_manual + published_auto, n = published_manual + unpublished_manual + published_auto + unpublished_auto, tol = 1e-8, method = "wilson")
result_manual <- binom.confint(x = published_manual, n = published_manual + unpublished_manual, tol = 1e-8, method = "wilson")
result_auto <- binom.confint(x = published_auto, n = published_auto + unpublished_auto, tol = 1e-8, method = "wilson")

```

Due to non-response to our emails there is remaining uncertainty about the number of registrations in our sample that were eventually published. The `r contacted_researchers_per_publication_status$sum[3] - responses_per_publication_status$sum[3]` non-responses related to registrations where we could not find a publication might contain some publications that we missed, and the `r contacted_researchers_per_publication_status$sum[4] - responses_per_publication_status$sum[4]` non-responses related to registrations where we remained uncertain similarly contain some published and some unpublished studies. Researchers might be more likely to reply when their registration is actually published, or non-response might be unaffected by the publication status of their registration. Under the latter assumption, we can use the observed percentages of published studies in both categories to predict that responses from researchers would have told us that `r unpublished_non_response_unpublished` out of `r total_assumed_unpublished_no_responses` of the registration in the unpublished classification and `r unpublished_non_response_uncertain` out of `r total_assumed_unpublished_uncertain_no_responses` of the registration in the uncertain classification are actually published. 

```{r table-estimated-nonpublication}
# Estimates of (non-)publication ------------------------------------------

estimated_nonpublication <- data.frame(Group = c("Manually Opened", "Automatically Opened", "Total"),
                 Total_Number_of_Registrations = c(total_open, total_closed, total_open + total_closed),
                 Percentage_Included = c(round(included_before$mean,2), ""),
                 Included_estimate = round(c(c(included_before$mean) * c(total_open, total_closed), sum(c(included_before$mean) * c(total_open, total_closed))),0),
                 Published = c(round(result_manual$mean,2), round(result_auto$mean,2), ""), 
                 Published_estimate = round(c(c(c(included_before$mean) * c(total_open, total_closed) *  c(result_manual$mean, result_auto$mean)), sum(c(c(included_before$mean) * c(total_open, total_closed) *  c(result_manual$mean, result_auto$mean)))),0),
                 Non_Published_estimate = round(c(c(included_before$mean) * c(total_open, total_closed), sum(c(included_before$mean) * c(total_open, total_closed))) - c(c(c(included_before$mean) * c(total_open, total_closed) *  c(result_manual$mean, result_auto$mean)), sum(c(c(included_before$mean) * c(total_open, total_closed) *  c(result_manual$mean, result_auto$mean)))),0)
                 )

result_population <- binom.confint(x = estimated_nonpublication[3,6], n = (estimated_nonpublication[3,6]+estimated_nonpublication[3,7]), tol = 1e-8, method = "wilson")

colnames(estimated_nonpublication) <- c("Group", "Registrations", "% Included", "Included", "% Published", "Published", "Not Published")
apa_table(estimated_nonpublication, digits = 0, caption = "Estimated number of published and unpublished studies registered on the OSF before November 2017.")

```

After incorporating responses from researchers, and extrapolating these responses to the registrations where we did not receive a response, we predict that out of a total of `r sum(included_after$sum_after)` included registrations, `r result_total$x` registered studies are published (`r round(100*result_total[4], 2)`%, 95% CI [`r round(100*result_total[5], 2)`; `r round(100*result_total[6], 2)`]). Our sample intentionally contained an almost equal number of manually opened and automatically opened registrations. As can be seen in Table \@ref(tab:table-estimated-nonpublication) manually opened registrations were much more likely to be published than automatically opened registrations, but also somewhat less frequent in the total population of registrations. Multiplying our publication estimates by the estimated number registrations on the OSF before November 2017 that can be classified as an actual research study we find that `r estimated_nonpublication[3,6]` registrations are published, and `r estimated_nonpublication[3,7]` remain unpublished. Therefore, in the total population of all registrations on the OSF from before November 2017 we estimated that `r round(100*result_population[4], 2)`%, of registrations are published as an article, preprint, thesis, or poster after at least four years.

```{r reasons_non_publish, include = FALSE}
# Reason not published ----------------------------------------------------

# It matters if this is done on all registrations we emailed for, or just the ones we ended up including. 

reasons_table <- registration_data_included %>% group_by(classification) %>% summarise(results = sum(reason_if_unpublished == 1 | reason_if_unpublished == 4 | reason_if_unpublished == 11), review = sum(reason_if_unpublished == 3 | reason_if_unpublished == 14), not_goal_to_publish = sum(reason_if_unpublished == 6 | reason_if_unpublished == 7 | reason_if_unpublished == 9), planning = sum(reason_if_unpublished == 2 | reason_if_unpublished == 5 | reason_if_unpublished == 8 | reason_if_unpublished == 10 | reason_if_unpublished == 13 | reason_if_unpublished == 15 | reason_if_unpublished == 16), study_design = sum(reason_if_unpublished == 12))
reasons_table[3,] <- reasons_table[1,] + reasons_table[2,]
reasons_table <- as.data.frame(reasons_table[,-1])
rownames(reasons_table) <- c("Manually Opened", "Automatically Opened", "Total")
colnames(reasons_table) <- c("Results", "Peer Review", "Not Goal to Publish", "Planning", "Study Design")

```

# Evaluating the Effect of Automatically Making Registries Public

We see that `r round(100*result_manual[4], 2)`%, 95% CI [`r round(100*result_manual[5], 2)`; `r round(100*result_manual[6], 2)`] of all registrations that based on our proxy were classified as manually opened by the researchers have been published. In the remaining `r 100-round(100*result_manual[4], 2)`% of these registrations researchers have voluntarily chosen to make the registration public, even though the study was not published. This shows that a platform that enables researchers to make their registration public will make studies that would otherwise remain in the file-drawer known to the research community. We estimate that `r round(100*result_auto[4], 2)`%, 95% CI [`r round(100*result_auto[5], 2)`; `r round(100*result_auto[6], 2)`] of automatically opened registrations is published. Consequently, `r 100-round(100*result_auto[4], 2)`% of these registrations would not be known to the research community were it not for the fact that the OSF automatically opens registrations after four years. In other words, automatically opening registrations increases our awareness of all studies that have been performed, above and beyond enabling researchers to create public registrations that they can voluntarily make public.

# Why do studies remained unpublished?

When registrations were classified as unpublished, or when we were uncertain about whether a registration was published, we reached out to the researchers to ask them whether their registration was published or not, and if not, why it remained unpublished. We also asked why they made the corresponding OSF project public or why they kept it private, and if the researchers were aware that the OSF would automatically make their registration public after four years. This study was approved by the ERB at Eindhoven University of Technology under proposal number 1674. 

There can be many reasons why a researcher does not publish a performed study. We classified the responses given by researchers into 5 categories (see Table \@ref(tab:table-reasons), for a more detailed overview, see the online supplement). Researchers indicated that their motivation to not publish was based on (1) the results of the study, such as null or unclear results and failed replications (`r round(100* reasons_table[3,1]/sum(reasons_table[3,]),0)`%), (2) the review process, such as that the paper was rejected, or that reviewers thought it would be better to leave this study out (`r round(100*reasons_table[3,2]/sum(reasons_table[3,-1]),0)`%), (3) the goal of the study, which was not publishing the results because the project was educational or intended to be shared with stakeholders (`r round(100* reasons_table[3,3]/sum(reasons_table[3,]),0)`%), (4) planning, such as a lack of time, or a new job on a different research topic or outside of academia (`r round(100* reasons_table[3,4]/sum(reasons_table[3,]),0)`%), and (5) problems with the study design (`r round(100* reasons_table[3,5]/sum(reasons_table[3,]),0)`%). 

According to the responses we received, issues related to ‘planning’ are the biggest cause of unpublished studies in our sample. Researchers leave academia and fail to complete a project before their contract ends, move on to a new position with a different research focus, or experience a lack of time to complete this specific project given other responsibilities. In some of these cases researchers did intend to come back to this project, and we observed continuing activity in projects registered more than four years ago, including publications that appeared in the literature.

```{r table-reasons, echo = FALSE}

apa_table(t(reasons_table), digits = 0, caption = "Summary of main reasons researchers self-reported to not publish registered studies.", note = "Only the first or main reason was coded whenever respondents gave multiple reasons.")

```

```{r reasons_open_project, include = FALSE}

reasons_open_project_table <- registration_data %>% summarise(open_science = sum(motivation_opening_project == 1 | motivation_opening_project == 6 | motivation_opening_project == 7), technical_issues = sum(motivation_opening_project == 2 | motivation_opening_project == 4), other = sum(motivation_opening_project == 3 | motivation_opening_project == 5))
reasons_open_project_table <- as.data.frame(reasons_open_project_table)
rownames(reasons_open_project_table) <- c("Reasons")
colnames(reasons_open_project_table) <- c("Open Science", "Technical Issues", "Other")

reasons_open_project_table[2]
sum(reasons_open_project_table)

reasons_closed_project_table <- registration_data %>% summarise(results = sum(motivation_not_opening_project == 1), publication_process = sum(motivation_not_opening_project == 2 | motivation_not_opening_project == 10), no_new_information = sum(motivation_not_opening_project == 4 | motivation_not_opening_project == 5 | motivation_not_opening_project == 7 | motivation_not_opening_project == 8 | motivation_not_opening_project == 9 | motivation_not_opening_project == 14), planning = sum(motivation_not_opening_project == 11 | motivation_not_opening_project == 6), technical_issues = sum(motivation_not_opening_project == 12 | motivation_not_opening_project == 13), forgotten = sum(motivation_not_opening_project == 3), other = sum(motivation_not_opening_project == 15 | motivation_not_opening_project == 16))
reasons_closed_project_table <- as.data.frame(reasons_closed_project_table)
rownames(reasons_closed_project_table) <- c("Reasons")
colnames(reasons_closed_project_table) <- c("Results", "Publication Process", "No New Information", "Planning", "Technical Issues", "Forgotten", "Other")
reasons_closed_project_table

sum(registration_data$four_years_known == 1)
sum(registration_data$four_years_known == 0)

```

When asked why researchers made their OSF project page open, most researchers (`r reasons_open_project_table[1]` out of `r sum(reasons_open_project_table)`) mentioned a motivation to practice open science. Out of `r sum(reasons_closed_project_table)` responses more diverse reasons were given for keeping the OSF project closed, such as that there was no relevant information stored in the project (`r reasons_closed_project_table[3]` times), researchers forgot to open it (`r reasons_closed_project_table[6]` times), were waiting until a project was published (`r reasons_closed_project_table[2]` times), or experienced technical issues (`r reasons_closed_project_table[5]` times). For more details, see the online supplement. Finally, we were interested in how many researchers who registered on the OSF before 2017 - many of which should be considered early adopters - were aware of the OSF policy to automatically open all registrations after four years. Although not all of the email respondents answered this question, `r sum(registration_data$four_years_known == 1)` of the researchers indicated they were aware of this policy, and `r sum(registration_data$four_years_known == 0)` were not aware of this policy.  

# Discussion

Our examination of `r sum(included_after$sum_after)` registrations led to an estimated publication rate of `r round(100*result_population[4], 2)`%, for all registrations on the Open Science framework that were created before November 18, 2017. Extrapolating from our sample, we estimate that of the  `r estimated_nonpublication[3,4]` registered research studies on the OSF that met our inclusion criteria, `r estimated_nonpublication[3,7]` remain unpublished. The main reason researchers give when asked why their studies remain unpublished are logistical issues related to a lack of time, or responsible researchers moving on to a new job. Our study provides an important datapoint to understand how many studies are performed, but never shared. The possibility that around `r 100-round(100*result_population[4], 2)`% of research studies registered on the OSF (excluding multi-lab studies and Registered Reports) are not shared should make scientists reflect on the efficiency of scientific research. Although not all unreported studies necessarily reflect research waste, and science will never be perfectly efficient, our results suggest scientists might need to seriously engage with the question how internal inefficiency can be reduced [@bernal_social_1939; @chalmers_avoidable_2009], wherever this waste is avoidable.

The fact that for `r round(100*reasons_table[3,2]/sum(reasons_table[3,]),0)`% of unpublished studies researchers indicated that the reason that the study remained unpublished is due to the results (e.g., null results or failed replications) is concerning, and considered unacceptable by the general public [@bottesini_what_2022; @pickett_questionable_2017]. It is possible that the  absence of these studies contributes to publication bias in the scientific literature. Researchers can prevent the results of a study from influencing the probability that their study will be published by submitting their registration as a Registered Report [@chambers_past_2022; @nosek_registered_2014]. Compared to the standard literature, where almost all studies support their hypotheses, Registered Reports are much more likely to report null results [@allen_open_2019; @scheel_excess_2021]. If researchers do not report failed replications it is impossible to correct false positive claims in the scientific literature, and if null-results remain unreported effect size estimates in meta-analyses will be inflated to an extent. With the rise of data repositories and preprints it has become feasible and easy to share the results of studies.

It is important to keep in mind that our estimate of the number of registrations on the OSF that are published after four years has 1) remaining uncertainty, and 2) is unlikely to generalize beyond the specific context of our study. First, even after contacting researchers, due to non-response some registrations will be classified incorrectly, and the true number of unpublished studies in our sample could differ slightly from the numbers we report. Second, the estimate is specific to the context of our study, which were all registrations on the Open Science Framework created before November 18, 2017. Users of the OSF in these years are early adopters interested in open science practices. It is possible that this group of researchers differs in how likely they are to share studies they have performed compared to researchers who did not register in this time-window, for example because researchers who adopt open science practices early more actively aim to publish null results, or reduce the size of their file-drawer. We studied the probability that a registered study was published, which might differ from the probability that a non-registered study is published. Registering a study requires work, and it is possible that researchers only register studies they feel have a higher probability of being submitted for publication. Because the practice of registering studies on the OSF up to 2017 was more widespread in experimental social psychology, it remains unknown how many studies remain unpublished in other disciplines. The probability that researchers have the time to share data from a registered study might change over time, and could be higher due to the COVID pandemic. In short, it is unclear whether our estimates generalize to different populations of scientists, different populations of studies, different disciplines, and whether this estimate remains stable over time. 

However, our goal was not to establish a generalizable estimate. The variation in the percentage of unpublished studies across scientific contexts is undoubtedly large. At the same time, the factors that cause researchers to not publish results (e.g., a lack of time, responsible researchers leaving academia, null results, rejections during peer review) play a role in many (if not all) scientific contexts. Our study demonstrates that the percentage of unpublished studies can be high, and deserves attention across scientific contexts where similar factors preventing the publication of studies play a role. In fields where most studies are submitted as a Registered Report, researchers work in larger teams where one team member takes over the work of a colleague who leaves academia, and all studies are designed to have a high probability to yield informative results, so there might be few unpublished studies. In fields where Registered Reports are rare, team science is less common, and less informative studies are designed, estimates of published results might be similar to those observed in our sample. Researchers who take efficient knowledge generation in their own scientific context seriously might want to empirically examine the question how many studies go unpublished to determine if there is room for improvement.

*Recommendations for Registries*. We found that registrations are not only used to register a study, but also to create a stable archive after a study has been completed. To be able to distinguish these uses of registries, it might be useful to create separate categories of registrations (e.g., distinguishing ‘prospective registration’ or ‘preregistration’ from ‘post registration’ or ‘data archiving’). A third registration option should be created for all registrations that are done for purely educational or practice purposes. The content of these registrations could remain the same, but some features (such as the automatically created DOI) could be omitted to save money. It would be useful for meta-scientific research if the API that allows researchers to access data from the registry could distinguish different types of registrations (e.g., pre- and post-registrations), and if the API made it possible to retrieve whether a registration was made public manually, or automatically after 4 years.  Lastly, the OSF currently displays registrations of separate components of the same project as unique registrations. It would be clearer if separate components belonging to the same registration would not be displayed as the same registrations. Alternatively, an extra field could be added that indicates that a registration is a sub-component of a certain larger project. 

*Recommendations for Registry Users*. Researchers should be aware that the OSF only makes the registration public, but not the main project page. This means that, for example, in the “OSF-Standard Pre-Data Collection Registration” the only information that becomes visible is the answer to the question “Has data collection begun for this project?”, the answer to the question “Have you looked at the data?”, and the text in the field “Other comments”. Some users wrote in this field “See the preregistration document specifying the logic, hypotheses, and analyses” but that files is not made accessible. If researchers want to use registrations to inform peers about a study they have performed, they will need to provide all relevant information in the “Other comments” field. It is recommended to follow a comprehensive preregistration template to increase the quality of the preregistration [@akker_effectiveness_2023] and to create machine-readable analysis code that describes when statistical hypotheses are corroborated or falsified [@lakens_improving_2021]. Finally, it would be useful if researchers include contact information such that peers can reach out to them with inquiries about the study that was performed. Adding an ORCID ID (for example to your OSF profile) and an email address (while realizing that researchers do not stay at the same institution indefinitely) is good practice.

# Conclusion

A substantial amount of data that researchers collect is never publicly shared. Every system will have some inefficiencies, and it is difficult to judge whether the percentage of unpublished data we observed in our sample is too high. What is perhaps most surprising is that researchers rarely talk about how many datasets they have collected that could have value for peers but that nevertheless linger in their filedrawer. Researchers might not feel like they can frankly discuss this topic due to the association of unpublished studies with publication bias. Although we found publication bias to be one reason to not share data, logistical issues were more commonly provided as a reason not to share data. More transparency about unpublished datasets would allow our field to learn and improve the way we work, and improve practices to increase the efficiency of psychological science. 

# References

<!-- # interlinepenalty prevents freak error when a hyperlink in reference breaks across a page -->
\begingroup
\interlinepenalty = 10000 


<div id="refs" custom-style="Bibliography"></div>
\endgroup
