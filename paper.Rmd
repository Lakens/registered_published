---
title             : "An Inception Cohort Study Quantifying How Many Registered Studies are Published."
shorttitle        : "QUANTIFYING PUBLISHED REGISTERED STUDIES"
author: 
  - name          : "Eline Ensinck"
    affiliation   : "1"
    corresponding : no    # Define only one corresponding author
    email         : "e.n.f.ensinck@student.tue.nl"  
  - name          : "Daniël Lakens"
    affiliation   : "1"
    corresponding : yes    # Define only one corresponding author
    address       : "Den Dolech 1, 5600MB Eindhoven, The Netherlands"
    email         : "D.Lakens@tue.nl"
affiliation:
  - id            : "1"
    institution   : "Eindhoven University of Technology"
authornote: |
  This work was funded by VIDI Grant 452-17-013 from the Netherlands Organisation for Scientific Research. The computationally reproducible version of this manuscript is available at https://github.com/Lakens/registered_published and additional materials on https://osf.io/8uqfb/. We would like the thanks Stephen Lindsay, Cristian Mesquida Caldentey, and Henry Wyneken for feedback on a draft. 
abstract: |
  We quantified how many studies registered on the Open Science Framework (OSF) up to November 2017 are performed but never shared. Examining a sample of 315 registrations, of which 169 were research studies, we found that 104 (62%) were published. We estimate that 5550 out of 9544 (58%) of registrations on the OSF are published within at least 4 years. Researchers use registries to make unpublished studies public, and the OSF policy to automatically make registrations public substantially increases the number of performed studies that become known to the scientific community. In responses to emails asking researchers why studies remained unpublished, logistical issues (e.g., lack of time, researchers changing jobs) was the most common reason, followed by null results and rejections during peer review. Our study shows that a substantial amount of data that researchers collect is never publicly shared. 
  
keywords          : "File Drawer; Registries; Research Waste; Scientific Publication"
wordcount         : "4501"
bibliography      : ["RegistriesPublication.bib"]
floatsintext      : yes
figurelist        : no
tablelist         : no
footnotelist      : no
linenumbers       : no
mask              : no
draft             : no
figsintext        : yes
class             : "jou, a4paper"
output            : papaja::apa6_pdf
link-citations    : true
editor_options: 
  chunk_output_type: console
---

```{r load packages and data, include = FALSE}
# Load packages ----

library(readxl)
library(dplyr)
library(papaja)
library(binom)
library(xfun)

# Load datafile ----

registration_data <- read_excel("registration_data.xlsx")

```

Not all studies that scientists perform end up in the scientific literature [@franco_publication_2014; @greenwald_consequences_1975; @sterling_publication_1959]. Studies remain unpublished for a variety of reasons, such as a lack of resources to analyse the results, a loss of interest, perceived methodological weaknesses, or because the results do not support predictions [@easterbrook_publication_1991; @lishner_sorting_2021]. When studies that have the potential to contribute to scientific knowledge go unpublished research resources are wasted [@buxton_avoiding_2021; @chalmers_avoidable_2009]. Although it is challenging to quantify how many studies remain unpublished the existence of the proverbial file drawer [@rosenthal_file_1979] has been demonstrated in disciplines such as psychology, sociology, economics, and medicine [@scheel_excess_2021; @franco_publication_2014; @gerber_publication_2010; @dickersin_factors_1992].

The most accurate estimates of the percentage of performed studies in the file drawer come from inception cohort studies where studies are – typically retroactively - followed from the moment they are started [@song_dissemination_2010]. For example, Dickersin and colleagues observed that from all studies approved by two institutional review boards serving the school of medicine and the school of health at the John Hopkins University 81% and 66% were published, respectively [@dickersin_factors_1992]. In the only inception cohort study in the social sciences we know of, Franco, Malhotra, and Simonovits [-@franco_publication_2014] found that out of 249 experiments 106 were published in a scientific journal or as a book chapter (45%), 29% of studies were written up but remained unpublished at the time of the study, and 20% (primarily non-significant results) were unpublished and not written up.

One way to make all studies that have been performed known to peers is to require researchers to (pre)register their study in a publicly accessible database [@dickersin_evolution_2012]. Registries often require researchers to update a registered study with the results. An example of such a study registry is ClinicalTrials.gov. The requirement to register certain types of studies does not exist in psychology. However, since 2013 the Open Science Framework (OSF) has provided a free study registry for any scientific domain [@spies_open_2013]. The OSF offers researchers the possibility to voluntarily register their study by specifying their hypotheses, methods, and statistical analysis plan. 

One interesting aspect of registrations on the OSF is the fact that every registration after June 8 2015  (if not withdrawn) will be made public after an embargo period of four years^[see https://web.archive.org/web/20230519110220/https://groups.google.com/g/openscienceframework/c/3ZaZYKV8WD4/m/JCN4OB31skQJ]. This policy means that the OSF provides a public record of all studies that were registered on the platform up to four years ago. It is therefore possible to use the database of registered studies for an inception cohort study that quantifies how many studies are performed, but never publicly shared.

# The current study

In this project we aimed to answer four main research questions. First, we were interested in estimating the number of registered studies on the OSF that were performed but remained unpublished after four years. Although registered studies are only a subset of all studies researchers perform, in the results section we estimate that our findings apply to 9,544 research studies registered on the OSF up to November 2017.

Second, we were interested in whether a registry with the policy to make registration public after an embargo facilitates identifying unpublished studies. By comparing the publication status of registrations manually made public by researchers with registrations automatically opened by the OSF after four years we can answer two questions. How often do researchers voluntarily share information about their study, even when the study remains unpublished? And how often do we learn about performed but unpublished studies because of the OSF policy to automatically make registrations public after four years?  Answers to these questions can provide insights into the benefits of a platform that makes registrations public after a certain time (e.g., OSF), compared to platforms that permanently keep registrations private (e.g., AsPredicted.org).

Our third question concerns the possibility to correctly identify whether a registered study is publicly shared, or not. For a registry to notify the scientific community about the presence of unpublished studies, the information in the registry needs to be complete enough to match the registration to a published study, or to conclude the registered study remains unpublished. As far as we know, this is the first investigation that retrieves registrations from the OSF to attempt to identify whether the associated study has been publicly shared or not. 

Finally, while it is useful to quantify how many registered studies remain unpublished after four years, it is equally important to understand why studies remain unpublished. The fourth aim of our study was to contact researchers of registrations of studies that were not published and ask why their study remains unpublished. 

```{r registry info, include = FALSE}
load("res_df_data.RData")
load("author_df.RData")

colnames(res_df) <-  c("reg_id", "reg_date_created", "reg_date_modified", "reg_date_registered", "reg_date_registered", "category", "description", "fork", "public", "reg_withdrawn", "reg_withdrawal_justification", "has_project", "parent_project", "project_id", "web_address", "project_created", "project_modified", "project_title", "project_public")

tot_res <- cbind(res_df, author_df)

# Number of parent projects
length(res_df$parent_project)
# 9721 projects closed and or withdrawn
sum(res_df$project_id == "NA")
# 8008 projects public
sum(res_df$project_public == TRUE)

# Total closed is total not public - total withdrawn = 9042
total_closed <- sum(res_df$project_id == "NA") - sum(res_df$reg_withdrawn == TRUE)
total_closed
# Total open 8008
total_open <- sum(res_df$project_public == TRUE)
total_open

```

# Method

*Data sources*: As registrations become public after four years, we downloaded all information that can be retrieved through the application programming interface (API) of the OSF for 17.729 registrations that were created before November 18, 2017 (four years before the start of this study). Registrations are part of at least one associated project page on the OSF. Sometimes researchers still work on projects four years after a registration, and we saw continued activity in some registrations in our sample. The final classification was based on whether the projects were public in April 2022. For `r sum(res_df$project_public == TRUE)` registrations (`r round(100 * sum(res_df$project_public == TRUE)/length(res_df$parent_project), 2)`%) the associated project was public, while for the remaining `r sum(res_df$project_id == "NA")`  registrations (`r round(100 * sum(res_df$project_id == "NA")/length(res_df$parent_project), 2)`%) the associated project was private. We randomly sampled registrations from both these groups. During data analysis we made sure that we did not include the same study twice (even though a single study can have multiple registrations, for example because an analysis subcomponent and a hypothesis subcomponent each get a separate ID in the OSF database). Our sampling strategy increased the probability that studies with multiple registrations were included in the study.


```{r inclusion, include = FALSE}

# I need to fix the issue of number of included before and after emails.
# We included more first, and emailed people, and then removed some data. 

# Table 2 with reasons for exclusions ----

excl_table <- registration_data %>% group_by(classification) %>% summarise_at(c("withdrawn", "no_research_study", "no_unique_study", "no_preregistration", "registered_report", "replication_project"), sum)
# Note that the sum in this table is higher than the total number of excluded registrations because an individual registration can be excluded for multiple reasons.

# Select only included studies ----

# select only the 171 included studies
registration_data_included <- registration_data[which(registration_data$no_research_study == 0 & registration_data$withdrawn == 0 & registration_data$no_unique_study == 0 & registration_data$no_preregistration == 0 & registration_data$registered_report == 0 & registration_data$replication_project == 0), ]

# Total registrations included in the study
total_included <- nrow(registration_data_included)

# Percentage included before and after emails for Table 1 ----

included_before <- registration_data %>% group_by(classification) %>% summarise(sum_before = sum(included_before_emails), binom::binom.confint(x = sum(included_before_emails), n = length(classification), tol = 1e-8, method = "wilson"))

included_after <- registration_data %>% group_by(classification) %>% summarise(sum_after = sum(included_after_emails), binom::binom.confint(x = sum(included_after_emails), n = length(classification), tol = 1e-8, method = "wilson"))

included_table <- cbind(paste(included_before$sum_before, " (", sprintf("%1.2f%%", 100*included_before$mean), ")", sep = ""), paste(included_after$sum_after, " (",sprintf("%1.2f%%", 100*included_after$mean), ")", sep = ""))
  
rownames(included_table) <- c(paste("Manually Opened (n = ", included_after$n[1], ")", sep = ""), paste("Automatically Opened (n = ", included_after$n[2], ")", sep = ""))
colnames(included_table) <- c("Included Before Emails", "Included After Emails")
included_table


# How often did we conclude the registration was published, not published, or uncertain? ----

table(registration_data_included$published_before_email)
# Based on an extensive search (see supplement) we classified 68 out of the 169 registrations as published, and 71 articles are unpublished. For 30 articles, we were uncertain about whether the registration was published, for example because we found a paper on the same topic, but this paper did not contain a link to the OSF registration, and there was very little information in the registration.

# Table 2 with reasons for exclusions ----

excl_table <- as.data.frame(registration_data %>% group_by(classification) %>% summarise_at(c("withdrawn", "no_research_study", "no_unique_study", "no_preregistration", "registered_report", "replication_project"), sum))[,2:7]
rownames(excl_table) <- c("Manually Opened", "Automatically Opened")
colnames(excl_table) <- c("Withdrawn", "No Research Study", "No Unique Study", "No Registration", "Registered Report", "Replication Project")
excl_table
# Note that the sum in this table is higher than the total number of excluded registrations because an individual registration can be excluded for multiple reasons.

```

*Classifying Registrations*: We aimed to examine at least 300 registrations (150 for which the registration was made public by the researchers, and 150 for which the registration was made public automatically after four years ). The OSF API does not provide information about how a project has been made public. To circumvent this limitation, we relied on a proxy indicator to classify projects as opened by the user, or opened automatically when the embargo was lifted. We classified OSF registrations as made public by the researcher when any of the associated OSF project pages was public, and classified the project as made public automatically by the OSF after four years when the associated project page was not made public. Our reasoning is that when only the registration is public, but the underlying project remains closed, the registration is most likely opened automatically, as there are not that many use-cases where a researcher would open the registration, but not the associated project. This is not a perfect proxy, as it is possible for users to make a registration public, but to keep the associated OSF project hidden (for example, because they mistakenly believe they made the entire project public when making the registration public). When the registration and underlying project are public, we assume the registration is opened by the researchers. This assumption is more likely to hold, as only users can make underlying project pages public. 

*Classifying Publication Status*: Studies can be available in different degrees, ranging from never being analysed (for example because the researcher who collected the data no longer works in science) to presented at a conference, to being published in a peer reviewed journal [@lishner_sorting_2021]. We considered a study ‘published’ if the study was communicated in any form that allows other researchers to learn about the results. In the remainder of this article the term ‘published’ is used when we classified studies as publicly shared because they appeared in the peer reviewed literature, but also when they were shared as a preprint, a PhD thesis in a stable repository, or a poster or conference paper that described the study in sufficient detail to include it in a meta-analysis (see the online supplementary material for more detail). We did not include bachelor or master theses because even though these can eventually be published (as this article demonstrates) they primarily have an educational goal and we do not consider the file drawer problem equally applicable to this category of studies. Our definition of ‘published’ is broader than in previous inception cohort studies (e.g., @dickersin_factors_1992; @franco_publication_2014). We believe the rise of preprints and stable repositories warrants a broader definition of what it means to publish the results of a study, especially because preprints provide a possible outlet for studies that a researcher might decide not to submit to a scientific journal. 

# Results 

*Inclusion criteria*: In total `r sum(included_after$n)` randomly chosen registrations from manually and automatically opened registrations were examined to see whether they met the selection criteria of our study. Of these, the associated project was public for `r included_after$n[1]` registrations and the associated project was closed for `r included_after$n[2]` registrations. Registrations needed to be actual research studies, as we are interested in estimating how many studies remain unpublished when researchers had the goal to publish. We therefore coded and excluded registrations that were (1) withdrawn (e.g., all content is removed because an error was made in the registration, but the meta-data continues to exist), (2) performed for practice purposes (e.g. student assignments or trial registrations), (3) used to generate a stable DOI for supplementary materials at the end of a project, and studies that are unlikely to end up in the file drawer such as (4) Registered Reports, [@allen_open_2019; @scheel_excess_2021; @chambers_past_2022], and (5) multi-lab projects (i.e., the Reproducibility Project: Psychology).

The number of included and excluded registrations in the sample is summarized in Table \@ref(tab:table-included). We originally included `r sum(included_before$sum_before)` registrations. After excluding `r sum(included_before$sum_before) - sum(included_after$sum_after)` registrations based on email responses for being student projects, performed without the goal of publishing the data, or part of a multi-lab project, our final sample size consists of `r sum(included_after$sum_after)` registrations. Of these, `r included_after$sum_after[1]` registrations were categorized as manually opened and `r included_after$sum_after[2]` registrations were categorized as automatically opened.  An overview of how often registrations were excluded for each of the 5 inclusion criteria is provided in Table \@ref(tab:table-excluded). 

```{r table-included, echo = FALSE}
apa_table(included_table, digits = 0, caption = "Studies included in the analysis before and after e-mailing researchers.")
```

```{r table-excluded, echo = FALSE}
apa_table(t(excl_table), digits = 0, caption = "Reasons studies were excluded from the analysis.", note = "The sum is greater than the total number of excluded registrations because an individual registration can be excluded for multiple reasons.")
```

*Identifying Publications Corresponding to Registrations*: We attempted to find a publication (e.g., a journal article, preprint, thesis, or scientific poster) associated with each registration included in the analysis. Ideally, it is possible to find a publication associated with a registration without the help of the researchers who performed the study. In practice, due to the often limited information included in a registration on the OSF, information provided by the researchers improved our ability to classify registrations as publicly shared or not. If a publication could be found that provided a direct link to the OSF registration we could be certain that we found the publication corresponding to the registration. In many cases, especially when registrations on the OSF contained little information, and publications did not link to a registration on the OSF, there was some uncertainty about whether a paper corresponded to a registration. Furthermore, if no publication could be found, there always remained the possibility that we failed to find it. Based on an extensive search (for details, see the online supplement) we classified `r table(registration_data_included$published_before_email)[2]` out of the `r sum(included_after$sum_after)` registrations as published, `r table(registration_data_included$published_before_email)[3]` registrations as unpublished, and for `r table(registration_data_included$published_before_email)[4]` registrations we were uncertain about whether the registration was published. We remained uncertain when, for example, there was very little information in the preregistration, and the information that was available seemed related to a paper without a link to an OSF registration.

```{r email_responses, include = FALSE}
# We contacted 109 researchers

contacted_researchers <- sum(registration_data$email_response != "-")

# 29 authors with registrations associated with public OSF projects and 80 authors with registrations associated with closed OSF projects

contacted_researchers_per_group <- registration_data %>% group_by(classification) %>% summarise(sum = sum(email_response != "-"))

# of which 24 and 53 replied to our email with additional information to update our classification.

replies_per_group <- registration_data %>% group_by(classification) %>% summarise(sum = sum(email_response == 1))
percentage_responded <- contacted_researchers_per_group[,2] / contacted_researchers

# Responses by initial publication classification before emails
contacted_researchers_per_publication_status <- registration_data %>% group_by(published_before_email) %>% summarise(sum = sum(email_response != "-"))
responses_per_publication_status <- registration_data %>% group_by(published_before_email) %>% summarise(sum = sum(email_response == 1))

```

For all registrations where we were uncertain about whether a publication corresponded to the registration, we attempted to retrieve contact information of researchers involved, and contacted the researchers. We also contacted researchers associated with all registrations classified after an extensive search as unpublished, to check if the registration was indeed unpublished, and if so, ask the researchers why the study remained unpublished. We contacted `r contacted_researchers` researchers, `r contacted_researchers_per_group$sum[1]` researchers with registrations associated with public OSF projects and `r contacted_researchers_per_group$sum[2]` researchers with registrations associated with closed OSF projects, of which `r replies_per_group$sum[1]` (`r round(100*replies_per_group$sum[1]/contacted_researchers_per_group$sum[1],2)`%) and `r replies_per_group$sum[2]` (`r round(100*replies_per_group$sum[2]/contacted_researchers_per_group$sum[2],2)`%) replied to our email with additional information to update our classification. We sent `r numbers_to_words(contacted_researchers_per_publication_status$sum[2])` email for a registration classified as published because we found a thesis in a repository to check if a paper had been published (and received `r numbers_to_words(responses_per_publication_status$sum[2])` response), for one registration we did not have to email the researcher because we could retrieve the required information from a paper in which they explicitly discussed their file-drawer [@van_elk_whats_2021], `r contacted_researchers_per_publication_status$sum[3]` emails for registrations we believed remained unpublished (and received `r responses_per_publication_status$sum[3]` responses), and `r contacted_researchers_per_publication_status$sum[4]` emails because we were uncertain if they were published or not (and received `r responses_per_publication_status$sum[4]` responses). Two additional researchers replied to our email, but did not give consent to use their replies in our data analysis, and we therefore did not change the classification of these two registrations. 

```{r match, include = FALSE}

match_per_group <- registration_data %>% group_by(classification) %>% summarise(no_match = sum(match == 0), match = sum(match == 1))
match_per_group

sum(registration_data$match == 0)
sum(registration_data$match == 1)

sum(registration_data_included$match != "-")

# How many did we correctly classify when we thought they were published (1), unpublished (2), and when we were uncertain (3)?
match_per_classification <- registration_data %>% group_by(published_before_email) %>% summarise(no_match = sum(match == 0), match = sum(match == 1))

match_per_classification_by_category <- registration_data %>% group_by(published_before_email, classification) %>% summarise(no_match = sum(match == 0), match = sum(match == 1))

# add total column to compute percentages
match_per_classification_by_category$total <- match_per_classification_by_category$no_match + match_per_classification_by_category$match
match_per_classification_by_category$no_match_percentage <- match_per_classification_by_category$no_match/match_per_classification_by_category$total
match_per_classification_by_category$match_percentage <- match_per_classification_by_category$match/match_per_classification_by_category$total


```

After updating our classification based on email responses, the publication rate in group 1 increased by 6% (meaning that we failed to identify some publications corresponding to registrations) while the publication rate in group 2 decreased by 1%. Sometimes a publication we found in the literature did not belong to the registration. If the study associated with the registration was published in another source than the one we identified, the classification remained the same, but we still counted this registration as a mismatch because we failed to identify the correct article without the help of the researchers. In total, `r sum(registration_data$match == 1)` of the `r sum(registration_data$match != "-")` responses match with publications we found. For the `r sum(match_per_classification[3,2:3])` registrations we classified as unpublished, responses to the follow-up emails indicated that `r match_per_classification[3,3]` (`r round(100*match_per_classification[3,3]/sum(match_per_classification[3,2:3]),2)`%) were unpublished, while for `r match_per_classification[3,2]` (`r round(100*match_per_classification[3,2]/sum(match_per_classification[3,2:3]),2)`%) we failed to identify the published paper based on the information in the registration. Most of the mismatches occurred when we were uncertain about whether a paper we found included the registered study because there was no link to the registration on the OSF and only some terminology in the registration and article overlapped. For the `r sum(match_per_classification[4,2:3])` registrations where we were uncertain if the study was published, `r match_per_classification[4,3]` (`r round(100*(match_per_classification[4,3]/sum(match_per_classification[4,2:3])), 2)`%) of registrations matched a paper we found. Of the `r match_per_classification[4,2]` (`r round(100*match_per_classification[3,2]/sum(match_per_classification[3,2:3]),2)`%) cases in this group that were a mismatch, 2 were indeed published but we failed to find the correct paper, and the remaining 12 were actually not published. In one special case the paper we found was indeed a match, but we decided to change the final classification to unpublished as the registration belonged to two studies and only one of the registered studies was published. Altogether, these results indicate that without contacting the researchers it is often difficult to determine whether a registration is published based exclusively on the information researchers provide in their registration on the OSF. 

```{r published, include = FALSE}

# How many are published after email when we thought before emailing they were published (1), unpublished (2), and when we were uncertain (3)? Divide this per classification (manually opened and automatically opened) and whether we got an email response. 
published_after_email_per_classification <- registration_data_included %>% group_by(published_before_email, email_response, classification) %>% summarise(published = sum(published_after_email == 1), not_published = sum(published_after_email == 2), unsure = sum(published_after_email == 3))
published_after_email_per_classification

# How many did we think unpublished before emails?
total_assumed_unpublished <- sum(published_after_email_per_classification$published[5:8], published_after_email_per_classification$not_published[5:8])

# Of those, how many did we get a reply for?
total_assumed_unpublished_responses <- sum(published_after_email_per_classification$published[7:8], published_after_email_per_classification$not_published[7:8])

# Of those, how many did we not get a reply for?
total_assumed_unpublished_no_responses <- sum(published_after_email_per_classification$published[5:6], published_after_email_per_classification$not_published[5:6])

# For the 45 we got responses about, 14 were actually published, and 31 were not.
sum(published_after_email_per_classification$published[7:8])
sum(published_after_email_per_classification$not_published[7:8])

# Of the 14 published, 6 were from automatically opened registrations, out of 29 total closed projects, so 6/29 automatically opened projects we thought were unpublished were published. For the manually opened projects, 8/16 were published.
published_after_email_per_classification$published[7]
published_after_email_per_classification$published[7] + published_after_email_per_classification$not_published[7]
published_after_email_per_classification$published[8]
published_after_email_per_classification$published[8] + published_after_email_per_classification$not_published[8]

# We did not get responses for 26 projects, of which 22 were automatically opened, and 4 were manually opened, for which the following might hold: 
# 8/16 of the 4 manually opened registrations could be published.
# 6/29 of the 22 automatically opened registrations could be published
total_assumed_unpublished_no_responses
published_after_email_per_classification$not_published[5]
published_after_email_per_classification$not_published[6]

published_after_email_per_classification$published[7] / (published_after_email_per_classification$published[7] + published_after_email_per_classification$not_published[7]) * published_after_email_per_classification$not_published[5]

published_after_email_per_classification$published[8] / (published_after_email_per_classification$published[8] + published_after_email_per_classification$not_published[8]) * published_after_email_per_classification$not_published[6]

# So how many of the assumed unpublished are actually published?
published_non_response_unpublished <- ((published_after_email_per_classification$published[7] / (published_after_email_per_classification$published[7] + published_after_email_per_classification$not_published[7]) * published_after_email_per_classification$not_published[5]) + (published_after_email_per_classification$published[8] / (published_after_email_per_classification$published[8] + published_after_email_per_classification$not_published[8]) * published_after_email_per_classification$not_published[6]))

unpublished_non_response_unpublished <- total_assumed_unpublished_no_responses - ((published_after_email_per_classification$published[7] / (published_after_email_per_classification$published[7] + published_after_email_per_classification$not_published[7]) * published_after_email_per_classification$not_published[5]) + (published_after_email_per_classification$published[8] / (published_after_email_per_classification$published[8] + published_after_email_per_classification$not_published[8]) * published_after_email_per_classification$not_published[6]))

# Now for uncertain projects

# How many were we uncertain about before emails? 30
total_assumed_unpublished_uncertain <- sum(published_after_email_per_classification$published[9:12], published_after_email_per_classification$not_published[9:12], published_after_email_per_classification$unsure[9:12])

# Of those, how many did we get a reply for?
total_assumed_unpublished_uncertain_responses <-  sum(published_after_email_per_classification$published[11:12], published_after_email_per_classification$not_published[11:12], published_after_email_per_classification$unsure[11:12])

# Of those, how many did we not get a reply for?
total_assumed_unpublished_uncertain_no_responses <-  sum(published_after_email_per_classification$published[9:10], published_after_email_per_classification$not_published[9:10], published_after_email_per_classification$unsure[9:10])

# For the 24 we got responses about, 12 were actually published, and 12 were not.
sum(published_after_email_per_classification$published[11:12])
sum(published_after_email_per_classification$not_published[11:12])

#Of the 14 published, 6 were from automatically opened registrations, out of 29 total closed projects, so 6/29 automatically opened projects we thought were unpublished were published. For the manually opened projects, 8/16 were published.
published_after_email_per_classification$published[11]
published_after_email_per_classification$published[11] + published_after_email_per_classification$not_published[11]
published_after_email_per_classification$published[12]
published_after_email_per_classification$published[12] + published_after_email_per_classification$not_published[12]

# We did not get a response for 6 projects we were uncertain about, of which 1 was manually opened, and 5 were automatically opened. For the 24 registrations we were uncertain about that we did received a response for, 12 are published, and 12 are not. For the manually opened projects, 3 out of 5 were published, and for the automatically opened projects 9 out of 10 were published. We did not get responses for 6 projects, of which 5 were automatically opened, and 1 was manually opened, for which the following might hold: 

total_assumed_unpublished_uncertain_no_responses
published_after_email_per_classification$unsure[9]
published_after_email_per_classification$unsure[10]

published_after_email_per_classification$published[11] / (published_after_email_per_classification$published[11] + published_after_email_per_classification$not_published[11]) * published_after_email_per_classification$unsure[9]

published_after_email_per_classification$published[12] / (published_after_email_per_classification$published[12] + published_after_email_per_classification$not_published[12]) * published_after_email_per_classification$unsure[10]

# So how many of the uncertain registrations are actually published?
published_non_response_uncertain <- (published_after_email_per_classification$published[11] / (published_after_email_per_classification$published[11] + published_after_email_per_classification$not_published[11]) * published_after_email_per_classification$unsure[9]) + (published_after_email_per_classification$published[12] / (published_after_email_per_classification$published[12] + published_after_email_per_classification$not_published[12]) * published_after_email_per_classification$unsure[10])

unpublished_non_response_uncertain <- total_assumed_unpublished_uncertain_no_responses - ((published_after_email_per_classification$published[11] / (published_after_email_per_classification$published[11] + published_after_email_per_classification$not_published[11]) * published_after_email_per_classification$unsure[9]) + (published_after_email_per_classification$published[12] / (published_after_email_per_classification$published[12] + published_after_email_per_classification$not_published[12]) * published_after_email_per_classification$unsure[10]))

# For manually opened registrations, altogether the % of published studies is:

published_manual <- published_after_email_per_classification$published[2] + published_after_email_per_classification$published[4] + published_after_email_per_classification$published[7] + published_after_email_per_classification$published[11] + published_after_email_per_classification$published[7] / (published_after_email_per_classification$published[7] + published_after_email_per_classification$not_published[7]) * published_after_email_per_classification$not_published[5] + published_after_email_per_classification$published[11] / (published_after_email_per_classification$published[11] + published_after_email_per_classification$not_published[11]) * published_after_email_per_classification$unsure[9]

# For automatically opened registrations (classification 2), altogether the % of published studies is: 
# 6 we classified as published, 6 published we thought unpublished but confirmed in email response, +9 confirmed unpublished we were uncertain about, + 5 * 9/19 for the 5 unsure without response, and those we thought unpublished predicted to be published. 
published_auto <- published_after_email_per_classification$published[3] + published_after_email_per_classification$published[8] + published_after_email_per_classification$published[12] + published_after_email_per_classification$published[12] / (published_after_email_per_classification$published[12] + published_after_email_per_classification$not_published[12]) * published_after_email_per_classification$unsure[10] + published_after_email_per_classification$published[8] / (published_after_email_per_classification$published[8] + published_after_email_per_classification$not_published[8]) * published_after_email_per_classification$not_published[6]

# And the unpublished studies for manually opened registrations is:
unpublished_manual <- published_after_email_per_classification$not_published[7] + published_after_email_per_classification$not_published[11] + (1 - (published_after_email_per_classification$published[7] / (published_after_email_per_classification$published[7] + published_after_email_per_classification$not_published[7]))) * published_after_email_per_classification$not_published[5] + (1 - (published_after_email_per_classification$published[11] / (published_after_email_per_classification$published[11] + published_after_email_per_classification$not_published[11]))) * published_after_email_per_classification$unsure[9]

# And the unpublished automatically opened studies are:
unpublished_auto <- published_after_email_per_classification$not_published[8] + published_after_email_per_classification$not_published[12] + (1 - (published_after_email_per_classification$published[8] / (published_after_email_per_classification$published[8] + published_after_email_per_classification$not_published[8]))) * published_after_email_per_classification$not_published[6] + (1 - (published_after_email_per_classification$published[12] / (published_after_email_per_classification$published[12] + published_after_email_per_classification$not_published[12]))) * published_after_email_per_classification$unsure[10]

# Check to see if they combine to 169 total included:
published_manual + published_auto + unpublished_manual + unpublished_auto

# Compute CI around estimates per group
result_total <- binom.confint(x = published_manual + published_auto, n = published_manual + unpublished_manual + published_auto + unpublished_auto, tol = 1e-8, method = "wilson")
result_manual <- binom.confint(x = published_manual, n = published_manual + unpublished_manual, tol = 1e-8, method = "wilson")
result_auto <- binom.confint(x = published_auto, n = published_auto + unpublished_auto, tol = 1e-8, method = "wilson")

```

Due to non-response to our emails there is remaining uncertainty about the number of registrations in our sample that were eventually published. The `r contacted_researchers_per_publication_status$sum[3] - responses_per_publication_status$sum[3]` non-responses related to registrations where we could not find a publication might contain some publications that we missed, and the `r contacted_researchers_per_publication_status$sum[4] - responses_per_publication_status$sum[4]` non-responses related to registrations where we remained uncertain similarly contain some published and some unpublished studies. Researchers might be more likely to reply when their registration is actually published, or non-response might be unaffected by the publication status of their registration. Under the latter assumption, we can use the observed percentages of published studies in both categories to predict that responses from researchers would have told us that `r unpublished_non_response_unpublished` out of `r total_assumed_unpublished_no_responses` of the registration in the unpublished classification and `r unpublished_non_response_uncertain` out of `r total_assumed_unpublished_uncertain_no_responses` of the registration in the uncertain classification are actually published. 

```{r table-estimated-nonpublication}
# Estimates of (non-)publication ------------------------------------------

estimated_nonpublication <- data.frame(Group = c("Manually Opened", "Automatically Opened", "Total"),
                 Total_Number_of_Registrations = c(total_open, total_closed, total_open + total_closed),
                 Percentage_Included = c(round(included_before$mean,2), ""),
                 Included_estimate = round(c(c(included_before$mean) * c(total_open, total_closed), sum(c(included_before$mean) * c(total_open, total_closed))),0),
                 Published = c(round(result_manual$mean,2), round(result_auto$mean,2), ""), 
                 Published_estimate = round(c(c(c(included_before$mean) * c(total_open, total_closed) *  c(result_manual$mean, result_auto$mean)), sum(c(c(included_before$mean) * c(total_open, total_closed) *  c(result_manual$mean, result_auto$mean)))),0),
                 Non_Published_estimate = round(c(c(included_before$mean) * c(total_open, total_closed), sum(c(included_before$mean) * c(total_open, total_closed))) - c(c(c(included_before$mean) * c(total_open, total_closed) *  c(result_manual$mean, result_auto$mean)), sum(c(c(included_before$mean) * c(total_open, total_closed) *  c(result_manual$mean, result_auto$mean)))),0)
                 )

result_population <- binom.confint(x = estimated_nonpublication[3,6], n = (estimated_nonpublication[3,6]+estimated_nonpublication[3,7]), tol = 1e-8, method = "wilson")

colnames(estimated_nonpublication) <- c("Group", "Registrations", "% Included", "Included", "% Published", "Published", "Not Published")
apa_table(estimated_nonpublication, digits = 0, caption = "Estimated number of published and unpublished studies registered on the OSF before November 2017.")

```

After incorporating responses from researchers, and extrapolating these responses to the registrations where we did not receive a response, we predict that out of a total of `r sum(included_after$sum_after)` included registrations, `r result_total$x` registered studies are published (`r round(100*result_total[4], 2)`%, 95% CI [`r round(100*result_total[5], 2)`; `r round(100*result_total[6], 2)`]). Our sample intentionally contained an almost equal number of manually opened and automatically opened registrations. As can be seen in Table \@ref(tab:table-estimated-nonpublication) manually opened registrations were much more likely to be published than automatically opened registrations, but also somewhat less frequent in the total population of registrations. Multiplying our publication estimates by the estimated number registrations on the OSF before November 2017 that can be classified as an actual research study we find that `r estimated_nonpublication[3,6]` registrations are published, and `r estimated_nonpublication[3,7]` remain unpublished. Therefore, in the total population of all registrations on the OSF from before November 2017 we estimated that `r round(100*result_population[4], 2)`%, of registrations are published as an article, preprint, thesis, or poster after at least four years.

```{r reasons_non_publish, include = FALSE}
# Reason not published ----------------------------------------------------

# It matters if this is done on all registrations we emailed for, or just the ones we ended up including. 

reasons_table <- registration_data_included %>% group_by(classification) %>% summarise(planning = sum(reason_if_unpublished == 2 | reason_if_unpublished == 5 | reason_if_unpublished == 8 | reason_if_unpublished == 10 | reason_if_unpublished == 13 | reason_if_unpublished == 15 | reason_if_unpublished == 16), results = sum(reason_if_unpublished == 1 | reason_if_unpublished == 4 | reason_if_unpublished == 11), review = sum(reason_if_unpublished == 3 | reason_if_unpublished == 14), study_design = sum(reason_if_unpublished == 12), not_goal_to_publish = sum(reason_if_unpublished == 6 | reason_if_unpublished == 7 | reason_if_unpublished == 9))
reasons_table[3,] <- reasons_table[1,] + reasons_table[2,]
reasons_table <- as.data.frame(reasons_table[,-1])
rownames(reasons_table) <- c("Manually Opened", "Automatically Opened", "Total")
colnames(reasons_table) <- c("Logistical Issues", "Results", "Peer Review", "Study Design", "Not Goal to Publish")

```

# Evaluating the Effect of Automatically Making Registries Public

We see that `r round(100*result_manual[4], 2)`%, 95% CI [`r round(100*result_manual[5], 2)`; `r round(100*result_manual[6], 2)`] of all registrations that based on our proxy were classified as manually opened by the researchers have been published. In the remaining `r 100-round(100*result_manual[4], 2)`% of these registrations researchers have voluntarily chosen to make the registration public, even though the study was not published. This shows that a platform that enables researchers to make their registration public will make studies that would otherwise remain in the file-drawer known to the research community. We estimate that `r round(100*result_auto[4], 2)`%, 95% CI [`r round(100*result_auto[5], 2)`; `r round(100*result_auto[6], 2)`] of automatically opened registrations is published. Consequently, `r 100-round(100*result_auto[4], 2)`% of these registrations would not be known to the research community were it not for the fact that the OSF automatically opens registrations after four years. In other words, automatically opening registrations increases our awareness of all studies that have been performed, above and beyond enabling researchers to create public registrations that they can voluntarily make public.

# Why do studies remained unpublished?

When registrations were classified as unpublished, or when we were uncertain about whether a registration was published, we reached out to the researchers to ask them whether their registration was published or not, and if not, why it remained unpublished. We also asked why they made the corresponding OSF project public or why they kept it private, and if the researchers were aware that the OSF would automatically make their registration public after four years. This study was approved by the ERB at Eindhoven University of Technology under proposal number 1674. 

There can be many reasons why a researcher does not publish a performed study. We classified the responses given by researchers into 5 categories (see Table \@ref(tab:table-reasons), for a more detailed overview, see the online supplement). Researchers indicated that their motivation to not publish was based on (1) logistical issues, such as a lack of time, or a new job on a different research topic or outside of academia (`r round(100*reasons_table[3,1]/sum(reasons_table[3,]),0)`%), (2) the results of the study, such as null or unclear results and failed replications (`r round(100* reasons_table[3,2]/sum(reasons_table[3,]),0)`%), (3) the review process, such as that the paper was rejected, or that reviewers thought it would be better to leave this study out (`r round(100*reasons_table[3,3]/sum(reasons_table[3,]),0)`%), (4) problems with the study design, such as not reaching the intended sample size, or a statistical artifact (`r round(100* reasons_table[3,4]/sum(reasons_table[3,]),0)`%), and (5) the goal of the study, which was not publishing the results because the project was educational or intended to be shared with stakeholders (`r round(100* reasons_table[3,5]/sum(reasons_table[3,]),0)`%). 

According to the responses we received, logistical issues are the biggest cause of unpublished studies in our sample. Researchers leave academia and fail to complete a project before their contract ends, move on to a new position with a different research focus, or experience a lack of time to complete this specific project given other responsibilities. In some of these cases researchers did intend to come back to this project, and we observed continuing activity in projects registered more than four years ago, including publications that appeared in the literature.

```{r table-reasons, echo = FALSE}

apa_table(t(reasons_table), digits = 0, caption = "Summary of main reasons researchers self-reported to not publish registered studies.", note = "Only the first or main reason was coded whenever respondents gave multiple reasons.")

```

```{r reasons_open_project, include = FALSE}

reasons_open_project_table <- registration_data %>% summarise(open_science = sum(motivation_opening_project == 1 | motivation_opening_project == 6 | motivation_opening_project == 7), technical_issues = sum(motivation_opening_project == 2 | motivation_opening_project == 4), other = sum(motivation_opening_project == 3 | motivation_opening_project == 5))
reasons_open_project_table <- as.data.frame(reasons_open_project_table)
rownames(reasons_open_project_table) <- c("Reasons")
colnames(reasons_open_project_table) <- c("Open Science", "Technical Issues", "Other")

reasons_open_project_table[2]
sum(reasons_open_project_table)

reasons_closed_project_table <- registration_data %>% summarise(results = sum(motivation_not_opening_project == 1), publication_process = sum(motivation_not_opening_project == 2 | motivation_not_opening_project == 10), no_new_information = sum(motivation_not_opening_project == 4 | motivation_not_opening_project == 5 | motivation_not_opening_project == 7 | motivation_not_opening_project == 8 | motivation_not_opening_project == 9 | motivation_not_opening_project == 14), planning = sum(motivation_not_opening_project == 11 | motivation_not_opening_project == 6), technical_issues = sum(motivation_not_opening_project == 12 | motivation_not_opening_project == 13), forgotten = sum(motivation_not_opening_project == 3), other = sum(motivation_not_opening_project == 15 | motivation_not_opening_project == 16))
reasons_closed_project_table <- as.data.frame(reasons_closed_project_table)
rownames(reasons_closed_project_table) <- c("Reasons")
colnames(reasons_closed_project_table) <- c("Results", "Publication Process", "No New Information", "Logistical Issues", "Technical Issues", "Forgotten", "Other")
reasons_closed_project_table

sum(registration_data$four_years_known == 1)
sum(registration_data$four_years_known == 0)

```

When asked why researchers made their OSF project page open, most researchers (`r reasons_open_project_table[1]` out of `r sum(reasons_open_project_table)`) mentioned a motivation to practice open science. Out of `r sum(reasons_closed_project_table)` responses more diverse reasons were given for keeping the OSF project closed, such as that there was no relevant information stored in the project (`r reasons_closed_project_table[3]` times), researchers forgot to open it (`r reasons_closed_project_table[6]` times), were waiting until a project was published (`r reasons_closed_project_table[2]` times), or experienced technical issues (`r reasons_closed_project_table[5]` times). For more details, see the online supplement. Finally, we were interested in how many researchers who registered on the OSF before 2017 - many of which should be considered early adopters - were aware of the OSF policy to automatically open all registrations after four years. Although not all of the email respondents answered this question, `r sum(registration_data$four_years_known == 1)` of the researchers indicated they were aware of this policy, and `r sum(registration_data$four_years_known == 0)` were not aware of this policy.  

# Discussion

Our examination of `r sum(included_after$sum_after)` registrations led to an estimated publication rate of `r round(100*result_population[4], 2)`%, for all registrations on the Open Science framework that were created before November 18, 2017. Extrapolating from our sample, we estimate that of the  `r estimated_nonpublication[3,4]` registered research studies on the OSF that met our inclusion criteria, `r estimated_nonpublication[3,7]` remain unpublished. Our study provides an important datapoint to understand how many studies are performed, but never shared. The possibility that around `r 100-round(100*result_population[4], 2)`% of research studies registered on the OSF (excluding multi-lab studies and Registered Reports) are not shared should make scientists reflect on the efficiency of scientific research. Although not all unreported studies necessarily reflect research waste, and science will never be perfectly efficient, our results suggest scientists might need to seriously engage with the question how internal inefficiency can be reduced [@bernal_social_1939; @chalmers_avoidable_2009], wherever this waste is avoidable.

The main reason researchers give when asked why their studies remain unpublished are logistical issues related to a lack of time, or responsible researchers moving on to a new job. Better time-management, designing projects from the outset in a way that they can be completed by other team members, or making unfinished projects available to potential collaborators could mitigate these issues. The fact that for `r round(100*reasons_table[3,2]/sum(reasons_table[3,]),0)`% of unpublished studies researchers indicated that the reason that the study remained unpublished is due to the results (e.g., null results or failed replications) is concerning, and considered unacceptable by the general public [@bottesini_what_2022; @pickett_questionable_2017]. It is possible that the  absence of these studies contributes to publication bias in the scientific literature. Researchers can prevent the results of a study from influencing the probability that their study will be published by submitting their registration as a Registered Report [@chambers_past_2022; @nosek_registered_2014]. If researchers do not report failed replications it is impossible to correct false positive claims in the scientific literature [@schmidt_shall_2009], and if null-results remain unreported effect size estimates in meta-analyses will be inflated [@egger_bias_1997].

Our estimate has remaining uncertainty as some registrations for which authors did not respond to our email might be classified incorrectly. Furthermore, the estimate is specific to the context of our study, which were all registrations on the OSF created before November 18, 2017. Users of the OSF in these years are early adopters interested in open science practices, and the likelihood they share a study might differ from researchers who do not use the OSF. Our estimate might not generalize to non-registered studies. The estimate might also change over time (e.g., more studies might have remained unpublished during the COVID pandemic). In short, the exact percentage of studies that remain unpublished will be different in other contexts. 

At the same time, the factors that cause researchers not to publish results (e.g., a lack of time, responsible researchers leaving academia, null results, rejections during peer review) play a role in many (if not all) scientific contexts. In fields where most studies are submitted as a Registered Report, researchers work in larger teams where one team member takes over the work of a colleague who leaves academia, and all studies are designed to have a high probability to yield informative results, there might be less reason to be concerned about the number of unpublished studies. Our study demonstrates that the percentage of unpublished studies can be high. The question how many studies remain unpublished deserves attention across scientific contexts where similar factors preventing the publication of studies play a role.  Empirically examining the size of the file drawer can determine if there is room for improvement.

*Recommendations for Registries*. We found that the OSF is not only used to (pre)register a study, but also to create a stable archive after a study has been completed. To be able to distinguish these uses of registries, it might be useful to create separate categories of registrations (e.g., distinguishing ‘preregistration’ from ‘data archiving’). Allowing users to distinguish educational projects from research projects might also be useful. It would be useful for meta-scientific research if the API that allows researchers to access data from the registry made it possible to retrieve whether a registration was made public manually or automatically by the OSF.  Lastly, the OSF currently displays registrations of separate components of the same project as unique registrations. It would be clearer if one could identify when registrations of components are part of the registration of the overarching project. 

*Recommendations for Registry Users*. Researchers should be aware that the OSF only makes the registration public, but not the main project page. This means that, for example, in the “OSF-Standard Pre-Data Collection Registration” the only information that becomes visible is the answer to the question “Has data collection begun for this project?”, the answer to the question “Have you looked at the data?”, and the text in the field “Other comments”. Some users wrote in this field “See the preregistration document specifying the logic, hypotheses, and analyses” but that files is not made accessible. If researchers want to use registrations to inform peers about a study they have performed, they will need to provide all relevant information in the “Other comments” field. It is recommended to follow a comprehensive preregistration template to increase the quality of the preregistration [@akker_effectiveness_2023] and to create machine-readable analysis code that describes when statistical hypotheses are corroborated or falsified [@lakens_improving_2021]. Finally, it would be useful if researchers include contact information such that peers can reach out to them with inquiries about the study that was performed. Adding an ORCID ID (for example to your OSF profile) and an email address (while realizing that researchers do not stay at the same institution indefinitely) is good practice.

# Conclusion

A substantial amount of data that researchers collect is never publicly shared. Every system will have some inefficiencies, and researchers will differ in their views about when a file drawer is too large [@dickersin_factors_1992]. What is perhaps most surprising is that researchers rarely talk about how many studies they have collected that could have value for peers, and yet linger in their file drawer. Researchers might feel uneasy to honestly discuss this topic because unpublished studies are strongly associated with bias. More transparency about unpublished studies would allow our field to improve the way we work and increase the efficiency of psychological science.

# References

<!-- # interlinepenalty prevents freak error when a hyperlink in reference breaks across a page -->
\begingroup
\interlinepenalty = 10000 


<div id="refs" custom-style="Bibliography"></div>
\endgroup
