---
title             : "Supplement to: An Inception Cohort Study Quantifying How Many Registered Studies are Published."
shorttitle        : "SUPPLEMENT"
author: 
  - name          : "Eline N. F. Ensinck"
    affiliation   : "1"
    corresponding : no    # Define only one corresponding author
    email         : "e.n.f.ensinck@student.tue.nl"  
  - name          : "Daniël Lakens"
    affiliation   : "1"
    corresponding : yes    # Define only one corresponding author
    address       : "Den Dolech 1, 5600MB Eindhoven, The Netherlands"
    email         : "D.Lakens@tue.nl"
affiliation:
  - id            : "1"
    institution   : "Eindhoven University of Technology"
authornote: |
  This work was funded by VIDI Grant 452-17-013 from the Netherlands Organisation for Scientific Research. The computationally reproducible version of this manuscript is available at https://github.com/Lakens/registered_published and additional materials on https://osf.io/8uqfb/.
abstract: |
  This supplement provides additional details on the classification of registrations as research studies or not, the classification of studies as published or not, the calculations to extrapolate from our sample to the population of registrations on the OSF before November 2017, additional analyses about how extensive registrations were, and a more fine-grained overview of reasons researchers provided not to publish their study. 
  
floatsintext      : yes
figurelist        : no
tablelist         : no
footnotelist      : no
linenumbers       : no
mask              : no
draft             : no
figsintext        : yes
class             : "jou, a4paper"
output            : papaja::apa6_pdf
link-citations    : true
editor_options: 
  chunk_output_type: console
---

```{r load packages and data, include = FALSE}
# Load packages ----

library(readxl)
library(dplyr)
library(papaja)
library(binom)

# Load datafile ----

registration_data <- read_excel("registration_data.xlsx")

registration_data_included <- registration_data[which(registration_data$no_research_study == 0 & registration_data$withdrawn == 0 & registration_data$no_unique_study == 0 & registration_data$no_preregistration == 0 & registration_data$registered_report == 0 & registration_data$replication_project == 0), ]

```

# Details on Classifying Registrations as a Research Study

It can be challenging to determine whether a registration was performed with the goal to publish a study. Sometimes, the description of the registration or project page explicitly revealed that a registration was not an actual research study (for instance, when the group number of the course was shared), or it explicitly linked to a final publication. When it was not clear from the description on the OSF if the registration concerned an actual research study, we searched for information about the researcher(s) on the OSF. If the person who registered the project had various activity points and projects on the OSF, we assumed this person was an active researcher, and not a student. If someone had used the OSF at most a few times since 2017, and did not provide any personal information, we tried to determine if the user was a researcher by searching for scientific publications or membership of research groups (e.g. profiles on institutional websites). If such information could be found, the registration was coded as an actual research study. If such information could not be found, the registration was marked as “no research study” (e.g., it was assumed to be a student assignment). In the case of multiple researchers, it is possible that only a subgroup of the researchers could be found. If only one of the researchers seemed to be a real researcher, we assumed that the registration was part of a student project, where the researcher supervised the other users of the OSF. However, if more than one researcher seemed to be an actual researcher, the registration was marked as an actual research study. These decisions necessarily introduce some arbitrariness in whether a study was intended to be published or not, and therefore we explored whether researchers intended to publish a study when we emailed researchers to follow up on unpublished registrations (see below). As we did not email researchers when we felt confident the registration was not a research study, and as we did not receive replies from all researchers we emailed, the final classification has remaining uncertainty (see the discussion).  

# Details on Classifying Studies as Published or Not

```{r publicationtype}
# Reason not published ----------------------------------------------------

# It matters if this is done on all registrations we emailed for, or just the ones we ended up including. 

publicationtype <- registration_data_included %>% group_by(classification) %>% summarise(journal = sum(journal_article_after_email == 1), preprint = sum(grey_literature_after_email == 1), thesis = sum(grey_literature_after_email == 2), poster = sum(grey_literature_after_email == 3), report = sum(grey_literature_after_email == 4), conference = sum(grey_literature_after_email == 5))
publicationtype[3,] <- publicationtype[1,] + publicationtype[2,]
publicationtype <- as.data.frame(publicationtype[,-1])
rownames(publicationtype) <- c("Manually Opened", "Automatically Opened", "Total")
colnames(publicationtype) <- c("Journal Article", "Preprint", "PhD Thesis", "Poster", "Report", "Conference Proceeding")

apa_table(t(publicationtype), digits = 0, caption = "Type of document that matched the registration for all registrations that were publicly shared")

```

For each registration, we first searched for the combination of the title of the registration and the names of the researchers in Google Scholar and examined the search results. We considered the registration published if it appeared as a journal article, preprint, PhD thesis in a repository, an online report, or conference proceedings. The frequency of each publication type is provided in Table \@ref(tab:publicationtype). When a publication with a similar title was identified, the publication was downloaded and we determined if the publication matched the topic of the registration, was published after the registration date on the OSF, and the OSF users on the registration at least partly overlapped with the researchers of the publication. We also searched the publication for a link to an OSF project page that was associated with the registration. If the publication indeed seemed to be about the same topic and was published after the registration date on the OSF, the study was marked as publicly shared. 

Because titles of the registrations and papers often do not match we continued our search based on the names of the OSF users. For each user associated with a registration we retrieved a list of their publications and scanned through their publications after the registration date. If a document similar to the topic of the registration or in collaboration with some of the other OSF users associated with the registration was found, we again aimed to determine if the publication matched the registration. Whenever we failed to find a publication that seemed to be related to a registration the registration was marked as “not publicly shared” in any of the document types outlined above.  

# Details on Extrapolating to the Population

```{r published, include = FALSE}

# How many are published after email when we thought before emailing they were published (1), unpublished (2), and when we were uncertain (3)? Divide this per classification (manually opened and automatically opened) and whether we got an email response. 
published_after_email_per_classification <- registration_data_included %>% group_by(published_before_email, email_response, classification) %>% summarise(published = sum(published_after_email == 1), not_published = sum(published_after_email == 2), unsure = sum(published_after_email == 3))
published_after_email_per_classification

# How many did we think unpublished before emails?
total_assumed_unpublished <- sum(published_after_email_per_classification$published[5:8], published_after_email_per_classification$not_published[5:8])

# Of those, how many did we get a reply for?
total_assumed_unpublished_responses <- sum(published_after_email_per_classification$published[7:8], published_after_email_per_classification$not_published[7:8])

# Of those, how many did we not get a reply for?
total_assumed_unpublished_no_responses <- sum(published_after_email_per_classification$published[5:6], published_after_email_per_classification$not_published[5:6])

# For the 45 we got responses about, 14 were actually published, and 31 were not.
sum(published_after_email_per_classification$published[7:8])
sum(published_after_email_per_classification$not_published[7:8])

# Of the 14 published, 6 were from automatically opened registrations, out of 29 total closed projects, so 6/29 automatically opened projects we thought were unpublished were published. For the manually opened projects, 8/16 were published.
published_after_email_per_classification$published[7]
published_after_email_per_classification$published[7] + published_after_email_per_classification$not_published[7]
published_after_email_per_classification$published[8]
published_after_email_per_classification$published[8] + published_after_email_per_classification$not_published[8]

# We did not get responses for 26 projects, of which 22 were automatically opened, and 4 were manually opened, for which the following might hold: 
# 8/16 of the 4 manually opened registrations could be published.
# 6/29 of the 22 automatically opened registrations could be published
total_assumed_unpublished_no_responses
published_after_email_per_classification$not_published[5]
published_after_email_per_classification$not_published[6]

published_after_email_per_classification$published[7] / (published_after_email_per_classification$published[7] + published_after_email_per_classification$not_published[7]) * published_after_email_per_classification$not_published[5]

published_after_email_per_classification$published[8] / (published_after_email_per_classification$published[8] + published_after_email_per_classification$not_published[8]) * published_after_email_per_classification$not_published[6]

# So how many of the assumed unpublished are actually published?
published_non_response_unpublished <- ((published_after_email_per_classification$published[7] / (published_after_email_per_classification$published[7] + published_after_email_per_classification$not_published[7]) * published_after_email_per_classification$not_published[5]) + (published_after_email_per_classification$published[8] / (published_after_email_per_classification$published[8] + published_after_email_per_classification$not_published[8]) * published_after_email_per_classification$not_published[6]))

unpublished_non_response_unpublished <- total_assumed_unpublished_no_responses - ((published_after_email_per_classification$published[7] / (published_after_email_per_classification$published[7] + published_after_email_per_classification$not_published[7]) * published_after_email_per_classification$not_published[5]) + (published_after_email_per_classification$published[8] / (published_after_email_per_classification$published[8] + published_after_email_per_classification$not_published[8]) * published_after_email_per_classification$not_published[6]))

# Now for uncertain projects

# How many were we uncertain about before emails? 30
total_assumed_unpublished_uncertain <- sum(published_after_email_per_classification$published[9:12], published_after_email_per_classification$not_published[9:12], published_after_email_per_classification$unsure[9:12])

# Of those, how many did we get a reply for?
total_assumed_unpublished_uncertain_responses <-  sum(published_after_email_per_classification$published[11:12], published_after_email_per_classification$not_published[11:12], published_after_email_per_classification$unsure[11:12])

# Of those, how many did we not get a reply for?
total_assumed_unpublished_uncertain_no_responses <-  sum(published_after_email_per_classification$published[9:10], published_after_email_per_classification$not_published[9:10], published_after_email_per_classification$unsure[9:10])

# For the 24 we got responses about, 12 were actually published, and 12 were not.
sum(published_after_email_per_classification$published[11:12])
sum(published_after_email_per_classification$not_published[11:12])

# We did not get a response for 6 projects we were uncertain about, of which 1 was manually opened, and 5 were automatically opened. For the 24 registrations we were uncertain about that we did received a response for, 12 are published, and 12 are not. For the manually opened projects, 3 out of 5 were published, and for the automatically opened projects 9 out of 19 were published. We did not get responses for 6 projects, of which 5 were automatically opened, and 1 was manually opened, for which the following might hold: 

published_after_email_per_classification$published[11]
published_after_email_per_classification$published[11] + published_after_email_per_classification$not_published[11]
published_after_email_per_classification$published[12]
published_after_email_per_classification$published[12] + published_after_email_per_classification$not_published[12]

# We did not get a response for 6 projects we were uncertain about, of which 1 was manually opened, and 5 were automatically opened. For the 24 registrations we were uncertain about that we did received a response for, 12 are published, and 12 are not. For the manually opened projects, 3 out of 5 were published, and for the automatically opened projects 9 out of 10 were published. We did not get responses for 6 projects, of which 5 were automatically opened, and 1 was manually opened, for which the following might hold: 

total_assumed_unpublished_uncertain_no_responses
published_after_email_per_classification$unsure[9]
published_after_email_per_classification$unsure[10]

published_after_email_per_classification$published[11] / (published_after_email_per_classification$published[11] + published_after_email_per_classification$not_published[11]) * published_after_email_per_classification$unsure[9]

published_after_email_per_classification$published[12] / (published_after_email_per_classification$published[12] + published_after_email_per_classification$not_published[12]) * published_after_email_per_classification$unsure[10]

# So how many of the uncertain registrations we received no response for might actually published?
# 2.368421 out of 6, and 3.031579 are unpublished. 

published_non_response_uncertain <- (published_after_email_per_classification$published[11] / (published_after_email_per_classification$published[11] + published_after_email_per_classification$not_published[11]) * published_after_email_per_classification$unsure[9]) + (published_after_email_per_classification$published[12] / (published_after_email_per_classification$published[12] + published_after_email_per_classification$not_published[12]) * published_after_email_per_classification$unsure[10])

unpublished_non_response_uncertain <- total_assumed_unpublished_uncertain_no_responses - ((published_after_email_per_classification$published[11] / (published_after_email_per_classification$published[11] + published_after_email_per_classification$not_published[11]) * published_after_email_per_classification$unsure[9]) + (published_after_email_per_classification$published[12] / (published_after_email_per_classification$published[12] + published_after_email_per_classification$not_published[12]) * published_after_email_per_classification$unsure[10]))

# For manually opened registrations, altogether the % of published studies is:

published_manual <- published_after_email_per_classification$published[2] + published_after_email_per_classification$published[4] + published_after_email_per_classification$published[7] + published_after_email_per_classification$published[11] + published_after_email_per_classification$published[7] / (published_after_email_per_classification$published[7] + published_after_email_per_classification$not_published[7]) * published_after_email_per_classification$not_published[5] + published_after_email_per_classification$published[11] / (published_after_email_per_classification$published[11] + published_after_email_per_classification$not_published[11]) * published_after_email_per_classification$unsure[9]

# For automatically opened registrations (classification 2), altogether the % of published studies is: 
# 6 we classified as published, 6 published we thought unpublished but confirmed in email response, +9 confirmed unpublished we were uncertain about, + 5 * 9/19 for the 5 unsure without response, and those we thought unpublished predicted to be published. 
published_auto <- published_after_email_per_classification$published[3] + published_after_email_per_classification$published[8] + published_after_email_per_classification$published[12] + published_after_email_per_classification$published[12] / (published_after_email_per_classification$published[12] + published_after_email_per_classification$not_published[12]) * published_after_email_per_classification$unsure[10] + published_after_email_per_classification$published[8] / (published_after_email_per_classification$published[8] + published_after_email_per_classification$not_published[8]) * published_after_email_per_classification$not_published[6]

# And the unpublished studies for manually opened registrations is:
unpublished_manual <- published_after_email_per_classification$not_published[7] + published_after_email_per_classification$not_published[11] + (1 - (published_after_email_per_classification$published[7] / (published_after_email_per_classification$published[7] + published_after_email_per_classification$not_published[7]))) * published_after_email_per_classification$not_published[5] + (1 - (published_after_email_per_classification$published[11] / (published_after_email_per_classification$published[11] + published_after_email_per_classification$not_published[11]))) * published_after_email_per_classification$unsure[9]

# And the unpublished automatically opened studies are:
unpublished_auto <- published_after_email_per_classification$not_published[8] + published_after_email_per_classification$not_published[12] + (1 - (published_after_email_per_classification$published[8] / (published_after_email_per_classification$published[8] + published_after_email_per_classification$not_published[8]))) * published_after_email_per_classification$not_published[6] + (1 - (published_after_email_per_classification$published[12] / (published_after_email_per_classification$published[12] + published_after_email_per_classification$not_published[12]))) * published_after_email_per_classification$unsure[10]

# Check to see if they combine to 169 total included:
published_manual + published_auto + unpublished_manual + unpublished_auto

# Compute CI around estimates per group
result_total <- binom.confint(x = published_manual + published_auto, n = published_manual + unpublished_manual + published_auto + unpublished_auto, tol = 1e-8, method = "wilson")
result_manual <- binom.confint(x = published_manual, n = published_manual + unpublished_manual, tol = 1e-8, method = "wilson")
result_auto <- binom.confint(x = published_auto, n = published_auto + unpublished_auto, tol = 1e-8, method = "wilson")

```

Before emailing authors, we believed `r total_assumed_unpublished` articles remained unpublished. We received responses for `r total_assumed_unpublished_responses` registrations after emailing researchers, and we did not get a response for `r total_assumed_unpublished_no_responses` registrations. Based on these responses we learned that out that of all registrations we thought were unpublished `r sum(published_after_email_per_classification$published[7:8])` were actually published, and `r sum(published_after_email_per_classification$not_published[7:8])` remained unpublished. We will compute the probability that a paper is published, even though we believed it was unpublished both for registrations classified as automatically opened (`r published_after_email_per_classification$published[8] + published_after_email_per_classification$not_published[8]` out of `r total_assumed_unpublished_responses`), and manually opened (`r published_after_email_per_classification$published[7] + published_after_email_per_classification$not_published[7]` out of `r total_assumed_unpublished_responses`). 

Of the `r published_after_email_per_classification$published[7] + published_after_email_per_classification$published[8]` registrations researchers informed us were actually published, `r published_after_email_per_classification$published[8]` were from registrations classified as automatically opened (out of `r published_after_email_per_classification$published[8] + published_after_email_per_classification$not_published[8]` automatically opened registrations that we received a response for), so `r published_after_email_per_classification$published[8]`/`r published_after_email_per_classification$published[8] + published_after_email_per_classification$not_published[8]` registrations classified as automatically opened that we thought were unpublished were actually published. For the registrations classified as manually opened, `r published_after_email_per_classification$published[7]` out of `r published_after_email_per_classification$published[7] + published_after_email_per_classification$not_published[7]` were published. 

We can use these two percentages to extrapolate to the `r total_assumed_unpublished_no_responses` registrations we did not get a response to our email for. Of these `r total_assumed_unpublished_no_responses`, `r published_after_email_per_classification$not_published[6]` were classified as automatically opened, and `r published_after_email_per_classification$not_published[5]` were classified as manually opened, for which the following might hold. 8/16 of the `r published_after_email_per_classification$not_published[5]` manually opened registrations could be published (which equals `r published_after_email_per_classification$published[7] / (published_after_email_per_classification$published[7] + published_after_email_per_classification$not_published[7]) * published_after_email_per_classification$not_published[5]` published registrations), and `r published_after_email_per_classification$published[8]`/`r published_after_email_per_classification$published[8] + published_after_email_per_classification$not_published[8]` of the `r published_after_email_per_classification$not_published[6]` automatically opened registrations could be published (which equals `r published_after_email_per_classification$published[8]/ (published_after_email_per_classification$published[8] + published_after_email_per_classification$not_published[8])` * `r published_after_email_per_classification$not_published[6]` = `r 
published_after_email_per_classification$published[8] / (published_after_email_per_classification$published[8] + published_after_email_per_classification$not_published[8]) * published_after_email_per_classification$not_published[6]` registrations). So an additional `r published_non_response_unpublished` registrations should be considered published, and `r unpublished_non_response_unpublished` remain unpublished. 

We can repeat these extrapolations for the registrations we were uncertain about. We were uncertain about `r total_assumed_unpublished_uncertain` registrations (often due to a lack of information in the registration). Of those we got a reply for `r total_assumed_unpublished_uncertain_responses`. For the `r total_assumed_unpublished_uncertain_responses` registrations we were uncertain about that we did received a response for, `r sum(published_after_email_per_classification$published[11:12])` are published, and `r sum(published_after_email_per_classification$not_published[11:12])` are not. For the manually opened projects, `r published_after_email_per_classification$published[11]` out of `r published_after_email_per_classification$published[11] + published_after_email_per_classification$not_published[11]` were published, and for the automatically opened projects `r published_after_email_per_classification$published[12]` out of `r published_after_email_per_classification$published[12] + published_after_email_per_classification$not_published[12]` were published. Of the `r sum(published_after_email_per_classification$unsure[9:10])` registrations we did not received a reply for, `r published_after_email_per_classification$unsure[9:10][1]` was manually opened, and `r published_after_email_per_classification$unsure[9:10][2]` were automatically opened. So on average `r published_after_email_per_classification$published[11]`/`r (published_after_email_per_classification$published[11] + published_after_email_per_classification$not_published[11])` * `r published_after_email_per_classification$unsure[9]` = `r published_after_email_per_classification$published[11] / (published_after_email_per_classification$published[11] + published_after_email_per_classification$not_published[11]) * published_after_email_per_classification$unsure[9]` registrations classified as manually opened will be published, and `r published_after_email_per_classification$published[12]`/`r (published_after_email_per_classification$published[12] + published_after_email_per_classification$not_published[12])` * `r published_after_email_per_classification$unsure[10]` = `r published_after_email_per_classification$published[12] / (published_after_email_per_classification$published[12] + published_after_email_per_classification$not_published[12]) * published_after_email_per_classification$unsure[10]` registrations classified as automatically opened will be published, for a total of `r published_non_response_uncertain`.

Altogether, for automatically opened registrations the percentage of published studies is `r published_after_email_per_classification$published[3]` we classified as published, `r published_after_email_per_classification$published[8]` registrations we thought were unpublished, but through email responses learned were published, `r published_after_email_per_classification$published[12]` we were originally uncertain about but were confirmed unpublished in an email response, `r published_after_email_per_classification$published[12] / (published_after_email_per_classification$published[12] + published_after_email_per_classification$not_published[12]) * published_after_email_per_classification$unsure[10]` for the 5 we initially were unsure about but were predicted to be published had researchers replied to our emails, and the `r published_after_email_per_classification$published[8] / (published_after_email_per_classification$published[8] + published_after_email_per_classification$not_published[8]) * published_after_email_per_classification$not_published[6]` we initially thought were unpublished but were predicted to be published had researchers replied to our emails, for a total of `r published_auto` published automatically opened articles out of a total of `r (published_auto + unpublished_auto)` (`r 100*published_auto/(published_auto + unpublished_auto)`%).

For automatically opened registrations the percentage of published studies is `r published_after_email_per_classification$published[2] + published_after_email_per_classification$published[4]` we classified as published, `r published_after_email_per_classification$published[7]` registrations we thought were unpublished, but through email responses learned were published, `r published_after_email_per_classification$published[11]` we were originally uncertain about but were confirmed unpublished in an email response, `r published_after_email_per_classification$published[11] / (published_after_email_per_classification$published[11] + published_after_email_per_classification$not_published[11]) * published_after_email_per_classification$unsure[9]` we initially were unsure about but were predicted to be published had researchers replied to our emails, and the `r published_after_email_per_classification$published[7] / (published_after_email_per_classification$published[7] + published_after_email_per_classification$not_published[7]) * published_after_email_per_classification$not_published[5]` we initially thought were unpublished but were predicted to be published had researchers replied to our emails, for a total of `r published_manual` published manually opened articles out of a total of `r (published_manual + unpublished_manual)` (`r 100*published_manual/(published_manual + unpublished_manual)`%). 

These two estimated percentages of the number of published studies in the registrations classified as manually opened and automatically opened are used in Table 3 in the main document to extrapolate from our sample to the number of published articles in the entire population of registrations we retrieved from the OSF up to November 2017. 

# Extensiveness of Information in Registrations

```{r table-quality, echo = FALSE}

quality_reg <- as.data.frame.matrix(table(registration_data_included$classification, registration_data_included$quality_of_registration))
rownames(quality_reg) <- c("Manually Opened", "Automatically Opened")
colnames(quality_reg) <- c("Empty",	"Minimal", "Short", "Extensive")

apa_table(quality_reg, digits = 0, caption = "Classification of the extensiveness of registrations.")

```

We observed quite some variation in the amount of detail in the registrations that we examined. We therefore decided to code the extensiveness of the registrations based on the information written in the registration (but not in any associated files, such as uploaded text documents). We labeled all registrations that were empty as 1 (which means it was not clear what the study was about based on the registration) and all registrations that had a minimal description as 2 (this means either a detailed title or small remark about the content of the study was included). If there was a somewhat more detailed but short description about the study, the registration was coded as 3 and finally, all registrations that included a detailed description were coded as 4. In this way, we could compare the quality of the registrations in both groups and check for possible differences that could lead to different publication rates. `r round(100*quality_reg[1,1]/sum(quality_reg[1,]),0)`% of all registrations in the manually opened registrations and `r round(100*quality_reg[2,1]/sum(quality_reg[2,]),0)`% of all registrations in the automatically opened registration were empty, so that it was not clear what the goal or topic of the study was purely based on the registration. At least a minimal description was given for `r round(100*quality_reg[1,2]/sum(quality_reg[1,]),0)`% of the registrations in the manually opened registrations and `r round(100*quality_reg[2,2]/sum(quality_reg[2,]),0)`% in the automatically opened registration, meaning that although very brief, there were some details about the content of the study (e.g., a descriptive title). A more detailed short description was provided in `r round(100*quality_reg[1,3]/sum(quality_reg[1,]),0)`% of the cases in the manually opened registrations and `r round(100*quality_reg[2,3]/sum(quality_reg[2,]),0)`% of the cases in the automatically opened registration. Finally, most registrations seem to fall in the last group of extensive registrations (`r round(100*quality_reg[1,4]/sum(quality_reg[1,]),0)`% and `r round(100*quality_reg[2,4]/sum(quality_reg[2,]),0)`% in the manually opened registrations and the automatically opened registration, respectively). These results are summarized in Table \@ref(tab:table-quality). The fact that similar percentages are found in manually opened registrations and the automatically opened registration shows that the level of detail in the registrations does not differ substantially between the two groups. Therefore, the difference in probability that we found a registration that was classified as manually opened, compared to classifications that were automatically opened, is not explained by a difference in the level of detail of the registrations.

# Fine-Grained Categorization of Reasons for Non-Publication

In the main text we have classified the reasons that researchers provided to not publish their registered study in five categories. Below in Table \@ref(tab:reasons-non-publish) we provide a more fine-grained classification consisting of 15 categories. 

```{r reasons-non-publish}
# Reason not published ----------------------------------------------------

reasons_table <- registration_data %>% group_by(classification) %>% summarise(a = sum(reason_if_unpublished == 1), b = sum(reason_if_unpublished == 2), c = sum(reason_if_unpublished == 3), d = sum(reason_if_unpublished == 4), e = sum(reason_if_unpublished == 5), f = sum(reason_if_unpublished == 6), h = sum(reason_if_unpublished == 7), i = sum(reason_if_unpublished == 8), j = sum(reason_if_unpublished == 9), k = sum(reason_if_unpublished == 10), l = sum(reason_if_unpublished == 11), m = sum(reason_if_unpublished == 12), n = sum(reason_if_unpublished == 13), o = sum(reason_if_unpublished == 14), p = sum(reason_if_unpublished == 15), q = sum(reason_if_unpublished == 16))
reasons_table[3,] <- reasons_table[1,] + reasons_table[2,]
reasons_table <- as.data.frame(reasons_table[,-1])
rownames(reasons_table) <- c("Manually Opened", "Automatically Opened", "Total")
colnames(reasons_table) <- c("No significant results", "Left academia", "Paper rejected", "Failed replication", "Unfinished study", "Educational project", "Pilot", "Switched job / research directions", "Shared with funder (not the goal to publish)", "Lack of time / busy with other projects", "Conflicting / unclear results", "Flaws in procedure", "Still in progress", "Reviewers requested it", "Extra study (no priority)", "Health issues")

apa_table(t(reasons_table), digits = 0, caption = "Summary of main reasons researchers self-reported to not publish registered studies.", note = "Only the first or main reason was coded whenever respondents gave multiple reasons.")

```

```{r reasons_open_project, include = FALSE}

reasons_open_project_table <- registration_data %>% summarise(open_science = sum(motivation_opening_project == 1 | motivation_opening_project == 6 | motivation_opening_project == 7), technical_issues = sum(motivation_opening_project == 2 | motivation_opening_project == 4), other = sum(motivation_opening_project == 3 | motivation_opening_project == 5))
reasons_open_project_table <- as.data.frame(reasons_open_project_table)
rownames(reasons_open_project_table) <- c("Reasons")
colnames(reasons_open_project_table) <- c("Open Science", "Technical Issues", "Other")

reasons_closed_project_table <- registration_data %>% summarise(results = sum(motivation_not_opening_project == 1), publication_process = sum(motivation_not_opening_project == 2 | motivation_not_opening_project == 10), no_new_information = sum(motivation_not_opening_project == 4 | motivation_not_opening_project == 5 | motivation_not_opening_project == 7 | motivation_not_opening_project == 8 | motivation_not_opening_project == 9 | motivation_not_opening_project == 14), planning = sum(motivation_not_opening_project == 11 | motivation_not_opening_project == 6), technical_issues = sum(motivation_not_opening_project == 12 | motivation_not_opening_project == 13), forgotten = sum(motivation_not_opening_project == 3), other = sum(motivation_not_opening_project == 15 | motivation_not_opening_project == 16))
reasons_closed_project_table <- as.data.frame(reasons_closed_project_table)
rownames(reasons_closed_project_table) <- c("Reasons")
colnames(reasons_closed_project_table) <- c("Results", "Publication Process", "No New Information", "Planning", "Technical Issues", "Forgotten", "Other")
reasons_closed_project_table


```

```{r reasonopen} 
apa_table(reasons_open_project_table, digits = 0, caption = "Summary of main reasons researchers self-reported to open their OSF project page.")
```

```{r reasonclosed} 
apa_table(reasons_closed_project_table, digits = 0, caption = "Summary of main reasons researchers self-reported to not open their OSF project page.")
```

# Motivations for opening or not opening OSF project pages

We were also interested in the motivation to either make the associated OSF project public or keep it private. While most researchers (79%) mentioned a motivation to practice open science as the primary reason for opening an OSF project (see Table \@ref(tab:reasonopen)), more diverse reasons were given for keeping the OSF project closed (see Table \@ref(tab:reasonclosed)). Many researchers (36%) mentioned that the associated project does not contain any new or relevant information, so peers would not benefit if the project was opened. The second most common reason not to open the project (26%) was that researchers simply forgot. Others made the moment to open the project dependent on the publication of the study (14%), and technical issues (e.g., a lack of familiarity or misunderstanding about the OSF) were reasons to both open a project and to keep it private (in 8% and 10% of the cases, respectively). 

# Researchers

Some researchers appeared in our database multiple times. This is an indication of the use of the OSF by early adopters in the years up to 2017. After excluding multiple registrations for the same research project, one researcher appeared three times, and three other researchers appeared two times. Although each registration can be published or not, there is the possibility of some dependencies in the data set because the probability that these authors publish a paper might be correlated across registrations. 

